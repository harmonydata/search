1:"$Sreact.fragment"
2:I[82104,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
3:I[10683,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"AuthProvider"]
4:I[63612,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"SearchProvider"]
5:I[68998,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
6:I[98904,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
7:I[15244,[],""]
8:I[43866,[],""]
9:I[14046,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"ToastContainer"]
b:I[86213,[],"OutletBoundary"]
d:I[86213,[],"MetadataBoundary"]
f:I[86213,[],"ViewportBoundary"]
11:I[34835,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/2c4d913f25bfc6bf.css","style"]
:HL["/search/_next/static/css/4921cfd18b262f8c.css","style"]
0:{"P":null,"b":"8r-g2-FTTcZL6JFFobnJN","p":"/search","c":["","items","021af5163d790d3b2878e2ea0e348f47"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","021af5163d790d3b2878e2ea0e348f47","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/2c4d913f25bfc6bf.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":["$","meta",null,{"name":"emotion-insertion-point","content":""}]}],["$","body",null,{"className":"__className_62a302","children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":[["$","$L5",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L6",null,{}],["$","$L5",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$L9",null,{"position":"bottom-right"}]]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","021af5163d790d3b2878e2ea0e348f47","d"],["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$La",[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/4921cfd18b262f8c.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$Lb",null,{"children":"$Lc"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","T8sj_M0_FMQAqUDtDR-f3",{"children":[["$","$Ld",null,{"children":"$Le"}],["$","$Lf",null,{"children":"$L10"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
10:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:["$","div",null,{"children":[["$","script",null,{"dangerouslySetInnerHTML":{"__html":"window.location.replace('/items/eeg-evidence-for-spatial-selectivity-in-feature-based-preparation-for-visual-search-2023-2024');"}}],["$","p",null,{"children":["Redirecting to"," ",["$","a",null,{"href":"/items/eeg-evidence-for-spatial-selectivity-in-feature-based-preparation-for-visual-search-2023-2024","children":["/items/","eeg-evidence-for-spatial-selectivity-in-feature-based-preparation-for-visual-search-2023-2024"]}],"..."]}]]}]
12:T12b1,In many visual search tasks, the detection of target objects in visual search requires feature-selective attentional guidance and space-based attentional selection. Feature-based attention is often assumed to operate in a spatially global fashion across the entire visual field, but there is also evidence that it can be restricted to task-relevant locations under some conditions. Here, we investigated whether such spatial filtering processes are already evident when representations of target-defining features (attentional templates) are activated during the preparation for an upcoming search episode. We measured N2pc components (an electrophysiological index of attentional allocation) in response to a rapid series of lateral task-irrelevant but template-matching colour probes that appeared while participants prepared for an upcoming search task with colour-defined targets. Critically, search targets would either always appear in the same lateral regions of visual space as the probes, or at different locations (near fixation or in lateral areas that never contained probes), thus rendering the probed locations either task-relevant or irrelevant. N2pc components triggered by target-colour probes during the preparation period emerged later and were attenuated when probes were presented at irrelevant locations. This demonstrates that the effects of preparatory feature-based attentional templates can be modulated by spatial expectations. However, this type of spatial filtering during search preparation only attenuates but not completely eliminates feature-based attentional modulations.Our perception of the outside world, and the way that we interact with external objects and events, is not just determined by incoming sensory information, but also by our expectations and intentions. We are not merely passive recipients of perceptual signals - very often, we are already prepared for what to expect and for what will be relevant in a given situation. Being prepared allows us to deal with our environment more effectively, by focussing our attention on what is important, and filtering out other information that can be safely ignored. It is obvious that preparation is one of the most important aspects of human cognition - it shapes our conscious experience and guides our interactions with the world. However, we still know very little about how we prepare for upcoming tasks. The reason for this is that the activation of preparatory states is an internal mental phenomenon that usually takes place in the absence of any directly observable behaviour. Such states are therefore difficult to assess with the conventional performance-based measures of experimental psychology.
 In this project, we will measure preparatory states directly, while they occur, by recording brain activity (EEG) from observers when they prepare for upcoming visual search tasks. In these tasks, they have to search for a specific known target object among multiple irrelevant objects (distractors). They can prepare for search by activating a mental representation of this target object, which will then help to guide their attention to the target when it appears. We have recently developed new methods to measure such preparatory "images in the mind" directly, at the moment when they are activated, and to track these activation states in real time. We can therefore now directly observe when preparation starts and how it changes across time. We can also determine the content of such preparatory states. For example, when we prepare to search for our mobile phone on our cluttered desk, do we activate a mental image of the whole object, or just a specific attribute of this object, such as its colour or shape? Can we control the content of what we prepare for, and deliberately employ different preparation strategies in different contexts? For example, do we prepare more effectively for task goals that are motivationally relevant, because they are associated with a higher reward? How quickly can we change preparatory states affected when task goals suddenly change? Importantly, we will also investigate links between preparation and failures of selective attention. We often fail to find what we are looking for, or attention can be distracted by irrelevant objects and events. By comparing preparatory states measured on an occasion where subsequent attentional selection operates efficiently and on an occasion where it does not, we can find out how fluctuations in preparedness produce different behavioural outcomes. 
 The question how we prepare for currently relevant objects and events is important for psychological theories of selective attention, and our research will therefore have important theoretical and conceptual implications for attention research.13:T12b1,In many visual search tasks, the detection of target objects in visual search requires feature-selective attentional guidance and space-based attentional selection. Feature-based attention is often assumed to operate in a spatially global fashion across the entire visual field, but there is also evidence that it can be restricted to task-relevant locations under some conditions. Here, we investigated whether such spatial filtering processes are already evident when representations of target-defining features (attentional templates) are activated during the preparation for an upcoming search episode. We measured N2pc components (an electrophysiological index of attentional allocation) in response to a rapid series of lateral task-irrelevant but template-matching colour probes that appeared while participants prepared for an upcoming search task with colour-defined targets. Critically, search targets would either always appear in the same lateral regions of visual space as the probes, or at different locations (near fixation or in lateral areas that never contained probes), thus rendering the probed locations either task-relevant or irrelevant. N2pc components triggered by target-colour probes during the preparation period emerged later and were attenuated when probes were presented at irrelevant locations. This demonstrates that the effects of preparatory feature-based attentional templates can be modulated by spatial expectations. However, this type of spatial filtering during search preparation only attenuates but not completely eliminates feature-based attentional modulations.Our perception of the outside world, and the way that we interact with external objects and events, is not just determined by incoming sensory information, but also by our expectations and intentions. We are not merely passive recipients of perceptual signals - very often, we are already prepared for what to expect and for what will be relevant in a given situation. Being prepared allows us to deal with our environment more effectively, by focussing our attention on what is important, and filtering out other information that can be safely ignored. It is obvious that preparation is one of the most important aspects of human cognition - it shapes our conscious experience and guides our interactions with the world. However, we still know very little about how we prepare for upcoming tasks. The reason for this is that the activation of preparatory states is an internal mental phenomenon that usually takes place in the absence of any directly observable behaviour. Such states are therefore difficult to assess with the conventional performance-based measures of experimental psychology.
 In this project, we will measure preparatory states directly, while they occur, by recording brain activity (EEG) from observers when they prepare for upcoming visual search tasks. In these tasks, they have to search for a specific known target object among multiple irrelevant objects (distractors). They can prepare for search by activating a mental representation of this target object, which will then help to guide their attention to the target when it appears. We have recently developed new methods to measure such preparatory "images in the mind" directly, at the moment when they are activated, and to track these activation states in real time. We can therefore now directly observe when preparation starts and how it changes across time. We can also determine the content of such preparatory states. For example, when we prepare to search for our mobile phone on our cluttered desk, do we activate a mental image of the whole object, or just a specific attribute of this object, such as its colour or shape? Can we control the content of what we prepare for, and deliberately employ different preparation strategies in different contexts? For example, do we prepare more effectively for task goals that are motivationally relevant, because they are associated with a higher reward? How quickly can we change preparatory states affected when task goals suddenly change? Importantly, we will also investigate links between preparation and failures of selective attention. We often fail to find what we are looking for, or attention can be distracted by irrelevant objects and events. By comparing preparatory states measured on an occasion where subsequent attentional selection operates efficiently and on an occasion where it does not, we can find out how fluctuations in preparedness produce different behavioural outcomes. 
 The question how we prepare for currently relevant objects and events is important for psychological theories of selective attention, and our research will therefore have important theoretical and conceptual implications for attention research.14:T12b1,In many visual search tasks, the detection of target objects in visual search requires feature-selective attentional guidance and space-based attentional selection. Feature-based attention is often assumed to operate in a spatially global fashion across the entire visual field, but there is also evidence that it can be restricted to task-relevant locations under some conditions. Here, we investigated whether such spatial filtering processes are already evident when representations of target-defining features (attentional templates) are activated during the preparation for an upcoming search episode. We measured N2pc components (an electrophysiological index of attentional allocation) in response to a rapid series of lateral task-irrelevant but template-matching colour probes that appeared while participants prepared for an upcoming search task with colour-defined targets. Critically, search targets would either always appear in the same lateral regions of visual space as the probes, or at different locations (near fixation or in lateral areas that never contained probes), thus rendering the probed locations either task-relevant or irrelevant. N2pc components triggered by target-colour probes during the preparation period emerged later and were attenuated when probes were presented at irrelevant locations. This demonstrates that the effects of preparatory feature-based attentional templates can be modulated by spatial expectations. However, this type of spatial filtering during search preparation only attenuates but not completely eliminates feature-based attentional modulations.Our perception of the outside world, and the way that we interact with external objects and events, is not just determined by incoming sensory information, but also by our expectations and intentions. We are not merely passive recipients of perceptual signals - very often, we are already prepared for what to expect and for what will be relevant in a given situation. Being prepared allows us to deal with our environment more effectively, by focussing our attention on what is important, and filtering out other information that can be safely ignored. It is obvious that preparation is one of the most important aspects of human cognition - it shapes our conscious experience and guides our interactions with the world. However, we still know very little about how we prepare for upcoming tasks. The reason for this is that the activation of preparatory states is an internal mental phenomenon that usually takes place in the absence of any directly observable behaviour. Such states are therefore difficult to assess with the conventional performance-based measures of experimental psychology.
 In this project, we will measure preparatory states directly, while they occur, by recording brain activity (EEG) from observers when they prepare for upcoming visual search tasks. In these tasks, they have to search for a specific known target object among multiple irrelevant objects (distractors). They can prepare for search by activating a mental representation of this target object, which will then help to guide their attention to the target when it appears. We have recently developed new methods to measure such preparatory "images in the mind" directly, at the moment when they are activated, and to track these activation states in real time. We can therefore now directly observe when preparation starts and how it changes across time. We can also determine the content of such preparatory states. For example, when we prepare to search for our mobile phone on our cluttered desk, do we activate a mental image of the whole object, or just a specific attribute of this object, such as its colour or shape? Can we control the content of what we prepare for, and deliberately employ different preparation strategies in different contexts? For example, do we prepare more effectively for task goals that are motivationally relevant, because they are associated with a higher reward? How quickly can we change preparatory states affected when task goals suddenly change? Importantly, we will also investigate links between preparation and failures of selective attention. We often fail to find what we are looking for, or attention can be distracted by irrelevant objects and events. By comparing preparatory states measured on an occasion where subsequent attentional selection operates efficiently and on an occasion where it does not, we can find out how fluctuations in preparedness produce different behavioural outcomes. 
 The question how we prepare for currently relevant objects and events is important for psychological theories of selective attention, and our research will therefore have important theoretical and conceptual implications for attention research.e:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"EEG Evidence for Spatial Selectivity in Feature-Based Preparation for Visual Search, 2023-2024"}],["$","meta","2",{"name":"description","content":"$12"}],["$","meta","3",{"property":"og:title","content":"EEG Evidence for Spatial Selectivity in Feature-Based Preparation for Visual Search, 2023-2024"}],["$","meta","4",{"property":"og:description","content":"$13"}],["$","meta","5",{"property":"og:url","content":"https://discoverynext.vercel.app/items/021af5163d790d3b2878e2ea0e348f47"}],["$","meta","6",{"property":"og:site_name","content":"Academic Resource Discovery"}],["$","meta","7",{"property":"og:locale","content":"en_US"}],["$","meta","8",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","meta","9",{"property":"og:image:width","content":"1200"}],["$","meta","10",{"property":"og:image:height","content":"630"}],["$","meta","11",{"property":"og:image:alt","content":"EEG Evidence for Spatial Selectivity in Feature-Based Preparation for Visual Search, 2023-2024"}],["$","meta","12",{"property":"og:type","content":"website"}],["$","meta","13",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","14",{"name":"twitter:title","content":"EEG Evidence for Spatial Selectivity in Feature-Based Preparation for Visual Search, 2023-2024"}],["$","meta","15",{"name":"twitter:description","content":"$14"}],["$","meta","16",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","link","17",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
c:null
