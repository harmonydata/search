<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/search/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/search/_next/static/css/2c4d913f25bfc6bf.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/search/_next/static/chunks/webpack-904c4041abd776f2.js"/><script src="/search/_next/static/chunks/4bd1b696-220750848fc52813.js" async=""></script><script src="/search/_next/static/chunks/1517-45045142ab33e6f1.js" async=""></script><script src="/search/_next/static/chunks/main-app-c0fb4dfbd302de72.js" async=""></script><script src="/search/_next/static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js" async=""></script><script src="/search/_next/static/chunks/f71d1b72-799ff7a6833dc50c.js" async=""></script><script src="/search/_next/static/chunks/6586-1013c110456598c2.js" async=""></script><script src="/search/_next/static/chunks/4889-f0599128dd4090a0.js" async=""></script><script src="/search/_next/static/chunks/9141-d17bf49085d8e296.js" async=""></script><script src="/search/_next/static/chunks/2926-f97573e569b0b5d8.js" async=""></script><script src="/search/_next/static/chunks/8173-30737ce2fc776efb.js" async=""></script><script src="/search/_next/static/chunks/9756-90c6220c809c4148.js" async=""></script><script src="/search/_next/static/chunks/3163-d1a03f172499fcd8.js" async=""></script><script src="/search/_next/static/chunks/app/layout-802ca43371b3eb9d.js" async=""></script><link rel="preload" href="/search/_next/static/css/4921cfd18b262f8c.css" as="style"/><meta name="next-size-adjust" content=""/><meta name="emotion-insertion-point" content=""/><title>Functional magnetic resonance imaging data of brain area activity when recognising facial expressions</title><meta name="description" content="Data resulting from an experiment which used brain scanning or functional magnetic resonance imaging (fMRI) to investigate the brain areas active when recognising facial expressions and to learn about how they are connected and how they communicate with each other.

The dataset consists of volumetric 3D scans of brains, necessarily stored in a special, purpose-made file format. The dataset also contains information necessary for analysing the data, i.e. stimuli and their onsets times. The dataset lastly contains participant ratings of the stimuli collected in a behavioural testing session, following scanning.

Our analyses of these data are reported in papers:
(1) Furl N, Henson RN, Friston KJ, Calder AJ. 2013. Top-down control of visual responses to fear by the amygdala. J Neurosci 33:17435-43.
(2) Furl N, Henson RN, Friston KJ, Calder AJ. 2013. Network Interactions Explain Sensitivity to Dynamic Faces in the Superior Temporal Sulcus. Cereb Cortex. 2015 Sep; 25(9): 2876–2882. 

Although a person&#x27;s facial identity is immutable, faces are dynamic and undergo complex movements which signal critical social cues (viewpoint, eye gaze, speech movements, expressions of emotion and pain).  These movements can confuse automated systems, yet humans recognise moving faces robustly.

Our objective is to discover the stimulus information, neural representations and computational mechanisms that the human brain uses when recognising social categories from moving faces. We will use human brain imaging to put an existing theory to the test. This theory proposes that recognition of changeable attributes (eg, expression) and facial identity are each recognised separately by two different brain pathways, each in a different part of the temporal lobe of the brain.

The evidence we provide might indeed support and fill in many gaps in this theory. Nevertheless, we expect instead to instantiate a new alternative theory. By this new theory, some brain areas can recognise both identities and expressions, using unified representations, with one of the two pathways specialised for representing movement. Thus, the successful completion of our project will provide a new theoretical framework sufficient to motivate improved automated visual systems and advance new directions of research on human social perception."/><meta property="og:title" content="Functional magnetic resonance imaging data of brain area activity when recognising facial expressions"/><meta property="og:description" content="Data resulting from an experiment which used brain scanning or functional magnetic resonance imaging (fMRI) to investigate the brain areas active when recognising facial expressions and to learn about how they are connected and how they communicate with each other.

The dataset consists of volumetric 3D scans of brains, necessarily stored in a special, purpose-made file format. The dataset also contains information necessary for analysing the data, i.e. stimuli and their onsets times. The dataset lastly contains participant ratings of the stimuli collected in a behavioural testing session, following scanning.

Our analyses of these data are reported in papers:
(1) Furl N, Henson RN, Friston KJ, Calder AJ. 2013. Top-down control of visual responses to fear by the amygdala. J Neurosci 33:17435-43.
(2) Furl N, Henson RN, Friston KJ, Calder AJ. 2013. Network Interactions Explain Sensitivity to Dynamic Faces in the Superior Temporal Sulcus. Cereb Cortex. 2015 Sep; 25(9): 2876–2882. 

Although a person&#x27;s facial identity is immutable, faces are dynamic and undergo complex movements which signal critical social cues (viewpoint, eye gaze, speech movements, expressions of emotion and pain).  These movements can confuse automated systems, yet humans recognise moving faces robustly.

Our objective is to discover the stimulus information, neural representations and computational mechanisms that the human brain uses when recognising social categories from moving faces. We will use human brain imaging to put an existing theory to the test. This theory proposes that recognition of changeable attributes (eg, expression) and facial identity are each recognised separately by two different brain pathways, each in a different part of the temporal lobe of the brain.

The evidence we provide might indeed support and fill in many gaps in this theory. Nevertheless, we expect instead to instantiate a new alternative theory. By this new theory, some brain areas can recognise both identities and expressions, using unified representations, with one of the two pathways specialised for representing movement. Thus, the successful completion of our project will provide a new theoretical framework sufficient to motivate improved automated visual systems and advance new directions of research on human social perception."/><meta property="og:url" content="https://discoverynext.vercel.app/items/7fc40fef2a39565d46b180db4f568a31"/><meta property="og:site_name" content="Academic Resource Discovery"/><meta property="og:locale" content="en_US"/><meta property="og:image" content="https://harmonydata.ac.uk/search/harmony.png"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image:alt" content="Functional magnetic resonance imaging data of brain area activity when recognising facial expressions"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Functional magnetic resonance imaging data of brain area activity when recognising facial expressions"/><meta name="twitter:description" content="Data resulting from an experiment which used brain scanning or functional magnetic resonance imaging (fMRI) to investigate the brain areas active when recognising facial expressions and to learn about how they are connected and how they communicate with each other.

The dataset consists of volumetric 3D scans of brains, necessarily stored in a special, purpose-made file format. The dataset also contains information necessary for analysing the data, i.e. stimuli and their onsets times. The dataset lastly contains participant ratings of the stimuli collected in a behavioural testing session, following scanning.

Our analyses of these data are reported in papers:
(1) Furl N, Henson RN, Friston KJ, Calder AJ. 2013. Top-down control of visual responses to fear by the amygdala. J Neurosci 33:17435-43.
(2) Furl N, Henson RN, Friston KJ, Calder AJ. 2013. Network Interactions Explain Sensitivity to Dynamic Faces in the Superior Temporal Sulcus. Cereb Cortex. 2015 Sep; 25(9): 2876–2882. 

Although a person&#x27;s facial identity is immutable, faces are dynamic and undergo complex movements which signal critical social cues (viewpoint, eye gaze, speech movements, expressions of emotion and pain).  These movements can confuse automated systems, yet humans recognise moving faces robustly.

Our objective is to discover the stimulus information, neural representations and computational mechanisms that the human brain uses when recognising social categories from moving faces. We will use human brain imaging to put an existing theory to the test. This theory proposes that recognition of changeable attributes (eg, expression) and facial identity are each recognised separately by two different brain pathways, each in a different part of the temporal lobe of the brain.

The evidence we provide might indeed support and fill in many gaps in this theory. Nevertheless, we expect instead to instantiate a new alternative theory. By this new theory, some brain areas can recognise both identities and expressions, using unified representations, with one of the two pathways specialised for representing movement. Thus, the successful completion of our project will provide a new theoretical framework sufficient to motivate improved automated visual systems and advance new directions of research on human social perception."/><meta name="twitter:image" content="https://harmonydata.ac.uk/search/harmony.png"/><link rel="icon" href="/search/favicon.ico" type="image/x-icon" sizes="16x16"/><script src="/search/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script><style data-emotion="mui-global o39zl1">html{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;box-sizing:border-box;-webkit-text-size-adjust:100%;}*,*::before,*::after{box-sizing:inherit;}strong,b{font-weight:700;}body{margin:0;color:#1A1A1A;font-size:0.875rem;line-height:1.5;font-family:'Roboto','Roboto Fallback';font-weight:400;background-color:#FFFFFF;}@media (min-width:600px){body{font-size:1rem;}}@media print{body{background-color:#fff;}}body::backdrop{background-color:#FFFFFF;}</style></head><body class="__className_62a302"><script src="/search/_next/static/chunks/webpack-904c4041abd776f2.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[82104,[\"2992\",\"static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js\",\"9895\",\"static/chunks/f71d1b72-799ff7a6833dc50c.js\",\"6586\",\"static/chunks/6586-1013c110456598c2.js\",\"4889\",\"static/chunks/4889-f0599128dd4090a0.js\",\"9141\",\"static/chunks/9141-d17bf49085d8e296.js\",\"2926\",\"static/chunks/2926-f97573e569b0b5d8.js\",\"8173\",\"static/chunks/8173-30737ce2fc776efb.js\",\"9756\",\"static/chunks/9756-90c6220c809c4148.js\",\"3163\",\"static/chunks/3163-d1a03f172499fcd8.js\",\"7177\",\"static/chunks/app/layout-802ca43371b3eb9d.js\"],\"default\"]\n3:I[10683,[\"2992\",\"static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js\",\"9895\",\"static/chunks/f71d1b72-799ff7a6833dc50c.js\",\"6586\",\"static/chunks/6586-1013c110456598c2.js\",\"4889\",\"static/chunks/4889-f0599128dd4090a0.js\",\"9141\",\"static/chunks/9141-d17bf49085d8e296.js\",\"2926\",\"static/chunks/2926-f97573e569b0b5d8.js\",\"8173\",\"static/chunks/8173-30737ce2fc776efb.js\",\"9756\",\"static/chunks/9756-90c6220c809c4148.js\",\"3163\",\"static/chunks/3163-d1a03f172499fcd8.js\",\"7177\",\"static/chunks/app/layout-802ca43371b3eb9d.js\"],\"AuthProvider\"]\n4:I[63612,[\"2992\",\"static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js\",\"9895\",\"static/chunks/f71d1b72-799ff7a6833dc50c.js\",\"6586\",\"static/chunks/6586-1013c110456598c2.js\",\"4889\",\"static/chunks/4889-f0599128dd4090a0.js\",\"9141\",\"static/chunks/9141-d17bf49085d8e296.js\",\"2926\",\"static/chunks/2926-f97573e569b0b5d8.js\",\"8173\",\"static/chunks/8173-30737ce2fc776efb.js\",\"9756\",\"static/chunks/9756-90c6220c809c4148.js\",\"3163\",\"static/chunks/3163-d1a03f172499fcd8.js\",\"7177\",\"static/chunks/app/layout-802ca43371b3eb9d.js\"],\"SearchProvider\"]\n5:I[68998,[\"2992\",\"static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js\",\"9895\",\"static/chunks/f71d1b72-799ff7a6833dc50c.js\",\"6586\",\"static/chunks/6586-1013c110456598c2.js\",\"4889\",\"static/chunks/4889-f0599128dd4090a0.js\",\"9141\",\"static/chunks/9141-d17bf49085d8e296.js\",\"2926\",\"static/chunks/2926-f97573e569b0b5d8.js\",\"8173\",\"static/chunks/8173-30737ce2fc776efb.js\",\"9756\",\"static/chunks/9756-90c6220c809c4148.js\",\"3163\",\"static/chunks/3163-d1a03f172499fcd8.js\",\"7177\",\"stati"])</script><script>self.__next_f.push([1,"c/chunks/app/layout-802ca43371b3eb9d.js\"],\"default\"]\n6:I[98904,[\"2992\",\"static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js\",\"9895\",\"static/chunks/f71d1b72-799ff7a6833dc50c.js\",\"6586\",\"static/chunks/6586-1013c110456598c2.js\",\"4889\",\"static/chunks/4889-f0599128dd4090a0.js\",\"9141\",\"static/chunks/9141-d17bf49085d8e296.js\",\"2926\",\"static/chunks/2926-f97573e569b0b5d8.js\",\"8173\",\"static/chunks/8173-30737ce2fc776efb.js\",\"9756\",\"static/chunks/9756-90c6220c809c4148.js\",\"3163\",\"static/chunks/3163-d1a03f172499fcd8.js\",\"7177\",\"static/chunks/app/layout-802ca43371b3eb9d.js\"],\"default\"]\n7:I[15244,[],\"\"]\n8:I[43866,[],\"\"]\n9:I[14046,[\"2992\",\"static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js\",\"9895\",\"static/chunks/f71d1b72-799ff7a6833dc50c.js\",\"6586\",\"static/chunks/6586-1013c110456598c2.js\",\"4889\",\"static/chunks/4889-f0599128dd4090a0.js\",\"9141\",\"static/chunks/9141-d17bf49085d8e296.js\",\"2926\",\"static/chunks/2926-f97573e569b0b5d8.js\",\"8173\",\"static/chunks/8173-30737ce2fc776efb.js\",\"9756\",\"static/chunks/9756-90c6220c809c4148.js\",\"3163\",\"static/chunks/3163-d1a03f172499fcd8.js\",\"7177\",\"static/chunks/app/layout-802ca43371b3eb9d.js\"],\"ToastContainer\"]\nb:I[86213,[],\"OutletBoundary\"]\nd:I[86213,[],\"MetadataBoundary\"]\nf:I[86213,[],\"ViewportBoundary\"]\n11:I[34835,[],\"\"]\n:HL[\"/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/search/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/search/_next/static/css/2c4d913f25bfc6bf.css\",\"style\"]\n:HL[\"/search/_next/static/css/4921cfd18b262f8c.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"8r-g2-FTTcZL6JFFobnJN\",\"p\":\"/search\",\"c\":[\"\",\"items\",\"7fc40fef2a39565d46b180db4f568a31\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"items\",{\"children\":[[\"slug\",\"7fc40fef2a39565d46b180db4f568a31\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/search/_next/static/css/2c4d913f25bfc6bf.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[\"$\",\"meta\",null,{\"name\":\"emotion-insertion-point\",\"content\":\"\"}]}],[\"$\",\"body\",null,{\"className\":\"__className_62a302\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"children\":[\"$\",\"$L4\",null,{\"children\":[[\"$\",\"$L5\",null,{\"sx\":{\"display\":\"flex\",\"flexDirection\":{\"xs\":\"column\",\"md\":\"row\"}},\"children\":[[\"$\",\"$L6\",null,{}],[\"$\",\"$L5\",null,{\"component\":\"main\",\"sx\":{\"flexGrow\":1,\"ml\":{\"xs\":0,\"md\":\"72px\"},\"mt\":{\"xs\":\"64px\",\"md\":0},\"minHeight\":{\"xs\":\"calc(100vh - 64px)\",\"md\":\"100vh\"},\"width\":{\"xs\":\"100%\",\"md\":\"calc(100% - 72px)\"}},\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}],[\"$\",\"$L9\",null,{\"position\":\"bottom-right\"}]]}]}]}]}]]}]]}],{\"children\":[\"items\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"items\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"7fc40fef2a39565d46b180db4f568a31\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"items\",\"children\",\"$0:f:0:1:2:children:2:children:0\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$La\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/search/_next/static/css/4921cfd18b262f8c.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"VbyIYAWD9bEsd2dd9KK0R\",{\"children\":[[\"$\",\"$Ld\",null,{\"children\":\"$Le\"}],[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"div\",null,{\"children\":[[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"window.location.replace('/items/functional-magnetic-resonance-imaging-data-of-brain-area-activity-when-recognising-facial-expressions');\"}}],[\"$\",\"p\",null,{\"children\":[\"Redirecting to\",\" \",[\"$\",\"a\",null,{\"href\":\"/items/functional-magnetic-resonance-imaging-data-of-brain-area-activity-when-recognising-facial-expressions\",\"children\":[\"/items/\",\"functional-magnetic-resonance-imaging-data-of-brain-area-activity-when-recognising-facial-expressions\"]}],\"...\"]}]]}]\n"])</script><script>self.__next_f.push([1,"12:T919,"])</script><script>self.__next_f.push([1,"Data resulting from an experiment which used brain scanning or functional magnetic resonance imaging (fMRI) to investigate the brain areas active when recognising facial expressions and to learn about how they are connected and how they communicate with each other.\n\nThe dataset consists of volumetric 3D scans of brains, necessarily stored in a special, purpose-made file format. The dataset also contains information necessary for analysing the data, i.e. stimuli and their onsets times. The dataset lastly contains participant ratings of the stimuli collected in a behavioural testing session, following scanning.\n\nOur analyses of these data are reported in papers:\n(1) Furl N, Henson RN, Friston KJ, Calder AJ. 2013. Top-down control of visual responses to fear by the amygdala. J Neurosci 33:17435-43.\n(2) Furl N, Henson RN, Friston KJ, Calder AJ. 2013. Network Interactions Explain Sensitivity to Dynamic Faces in the Superior Temporal Sulcus. Cereb Cortex. 2015 Sep; 25(9): 2876–2882. \n\nAlthough a person's facial identity is immutable, faces are dynamic and undergo complex movements which signal critical social cues (viewpoint, eye gaze, speech movements, expressions of emotion and pain).  These movements can confuse automated systems, yet humans recognise moving faces robustly.\n\nOur objective is to discover the stimulus information, neural representations and computational mechanisms that the human brain uses when recognising social categories from moving faces. We will use human brain imaging to put an existing theory to the test. This theory proposes that recognition of changeable attributes (eg, expression) and facial identity are each recognised separately by two different brain pathways, each in a different part of the temporal lobe of the brain.\n\nThe evidence we provide might indeed support and fill in many gaps in this theory. Nevertheless, we expect instead to instantiate a new alternative theory. By this new theory, some brain areas can recognise both identities and expressions, using unified representations, with one of the two pathways specialised for representing movement. Thus, the successful completion of our project will provide a new theoretical framework sufficient to motivate improved automated visual systems and advance new directions of research on human social perception."])</script><script>self.__next_f.push([1,"13:T919,"])</script><script>self.__next_f.push([1,"Data resulting from an experiment which used brain scanning or functional magnetic resonance imaging (fMRI) to investigate the brain areas active when recognising facial expressions and to learn about how they are connected and how they communicate with each other.\n\nThe dataset consists of volumetric 3D scans of brains, necessarily stored in a special, purpose-made file format. The dataset also contains information necessary for analysing the data, i.e. stimuli and their onsets times. The dataset lastly contains participant ratings of the stimuli collected in a behavioural testing session, following scanning.\n\nOur analyses of these data are reported in papers:\n(1) Furl N, Henson RN, Friston KJ, Calder AJ. 2013. Top-down control of visual responses to fear by the amygdala. J Neurosci 33:17435-43.\n(2) Furl N, Henson RN, Friston KJ, Calder AJ. 2013. Network Interactions Explain Sensitivity to Dynamic Faces in the Superior Temporal Sulcus. Cereb Cortex. 2015 Sep; 25(9): 2876–2882. \n\nAlthough a person's facial identity is immutable, faces are dynamic and undergo complex movements which signal critical social cues (viewpoint, eye gaze, speech movements, expressions of emotion and pain).  These movements can confuse automated systems, yet humans recognise moving faces robustly.\n\nOur objective is to discover the stimulus information, neural representations and computational mechanisms that the human brain uses when recognising social categories from moving faces. We will use human brain imaging to put an existing theory to the test. This theory proposes that recognition of changeable attributes (eg, expression) and facial identity are each recognised separately by two different brain pathways, each in a different part of the temporal lobe of the brain.\n\nThe evidence we provide might indeed support and fill in many gaps in this theory. Nevertheless, we expect instead to instantiate a new alternative theory. By this new theory, some brain areas can recognise both identities and expressions, using unified representations, with one of the two pathways specialised for representing movement. Thus, the successful completion of our project will provide a new theoretical framework sufficient to motivate improved automated visual systems and advance new directions of research on human social perception."])</script><script>self.__next_f.push([1,"14:T919,"])</script><script>self.__next_f.push([1,"Data resulting from an experiment which used brain scanning or functional magnetic resonance imaging (fMRI) to investigate the brain areas active when recognising facial expressions and to learn about how they are connected and how they communicate with each other.\n\nThe dataset consists of volumetric 3D scans of brains, necessarily stored in a special, purpose-made file format. The dataset also contains information necessary for analysing the data, i.e. stimuli and their onsets times. The dataset lastly contains participant ratings of the stimuli collected in a behavioural testing session, following scanning.\n\nOur analyses of these data are reported in papers:\n(1) Furl N, Henson RN, Friston KJ, Calder AJ. 2013. Top-down control of visual responses to fear by the amygdala. J Neurosci 33:17435-43.\n(2) Furl N, Henson RN, Friston KJ, Calder AJ. 2013. Network Interactions Explain Sensitivity to Dynamic Faces in the Superior Temporal Sulcus. Cereb Cortex. 2015 Sep; 25(9): 2876–2882. \n\nAlthough a person's facial identity is immutable, faces are dynamic and undergo complex movements which signal critical social cues (viewpoint, eye gaze, speech movements, expressions of emotion and pain).  These movements can confuse automated systems, yet humans recognise moving faces robustly.\n\nOur objective is to discover the stimulus information, neural representations and computational mechanisms that the human brain uses when recognising social categories from moving faces. We will use human brain imaging to put an existing theory to the test. This theory proposes that recognition of changeable attributes (eg, expression) and facial identity are each recognised separately by two different brain pathways, each in a different part of the temporal lobe of the brain.\n\nThe evidence we provide might indeed support and fill in many gaps in this theory. Nevertheless, we expect instead to instantiate a new alternative theory. By this new theory, some brain areas can recognise both identities and expressions, using unified representations, with one of the two pathways specialised for representing movement. Thus, the successful completion of our project will provide a new theoretical framework sufficient to motivate improved automated visual systems and advance new directions of research on human social perception."])</script><script>self.__next_f.push([1,"e:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Functional magnetic resonance imaging data of brain area activity when recognising facial expressions\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"$12\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:title\",\"content\":\"Functional magnetic resonance imaging data of brain area activity when recognising facial expressions\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:description\",\"content\":\"$13\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:url\",\"content\":\"https://discoverynext.vercel.app/items/7fc40fef2a39565d46b180db4f568a31\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:site_name\",\"content\":\"Academic Resource Discovery\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:image\",\"content\":\"https://harmonydata.ac.uk/search/harmony.png\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:image:height\",\"content\":\"630\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:image:alt\",\"content\":\"Functional magnetic resonance imaging data of brain area activity when recognising facial expressions\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:title\",\"content\":\"Functional magnetic resonance imaging data of brain area activity when recognising facial expressions\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:description\",\"content\":\"$14\"}],[\"$\",\"meta\",\"16\",{\"name\":\"twitter:image\",\"content\":\"https://harmonydata.ac.uk/search/harmony.png\"}],[\"$\",\"link\",\"17\",{\"rel\":\"icon\",\"href\":\"/search/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}]]\nc:null\n"])</script></body></html>