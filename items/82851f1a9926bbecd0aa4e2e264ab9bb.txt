1:"$Sreact.fragment"
2:I[82104,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
3:I[10683,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"AuthProvider"]
4:I[63612,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"SearchProvider"]
5:I[68998,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
6:I[98904,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
7:I[15244,[],""]
8:I[43866,[],""]
9:I[14046,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"ToastContainer"]
b:I[86213,[],"OutletBoundary"]
d:I[86213,[],"MetadataBoundary"]
f:I[86213,[],"ViewportBoundary"]
11:I[34835,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/2c4d913f25bfc6bf.css","style"]
:HL["/search/_next/static/css/4921cfd18b262f8c.css","style"]
0:{"P":null,"b":"8r-g2-FTTcZL6JFFobnJN","p":"/search","c":["","items","82851f1a9926bbecd0aa4e2e264ab9bb"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","82851f1a9926bbecd0aa4e2e264ab9bb","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/2c4d913f25bfc6bf.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":["$","meta",null,{"name":"emotion-insertion-point","content":""}]}],["$","body",null,{"className":"__className_62a302","children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":[["$","$L5",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L6",null,{}],["$","$L5",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$L9",null,{"position":"bottom-right"}]]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","82851f1a9926bbecd0aa4e2e264ab9bb","d"],["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$La",[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/4921cfd18b262f8c.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$Lb",null,{"children":"$Lc"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","8I5kg4P0iL5hVae4bIart",{"children":[["$","$Ld",null,{"children":"$Le"}],["$","$Lf",null,{"children":"$L10"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
10:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:["$","div",null,{"children":[["$","script",null,{"dangerouslySetInnerHTML":{"__html":"window.location.replace('/items/tracing-the-template-investigating-the-representation-of-perceptual-relevance-experimental-data');"}}],["$","p",null,{"children":["Redirecting to"," ",["$","a",null,{"href":"/items/tracing-the-template-investigating-the-representation-of-perceptual-relevance-experimental-data","children":["/items/","tracing-the-template-investigating-the-representation-of-perceptual-relevance-experimental-data"]}],"..."]}]]}]
12:Te3b,This data collection contains the behavioural and EEG data from the main experiments conducted in this research project.
The main goals of this project were as follows: human visual perception is strongly affected by current expectations and intentions. What is perceived is determined by what is attended, and what is attended is determined by "images in the mind" that guide attention in line with active goals and preferences. This project uses new experimental procedures and new methodological techniques (including temporally precise measures of electrical brain activity) to investigate how many things we can attend to at any time, and to study the adverse consequences of having to simultaneously attend to multiple objects in perception, visual working memory, and action. Are there systematic differences between individuals in their ability to attend to more than one thing at a time? We will also develop new methods to obtain precise measures of the speed of voluntary visual attention shifts: If attention is engaged at a particular location, how fast can it be moved to a new potentially relevant object? Our results suggest that the top-down guidance of attention is faster and more flexible than usually assumed, and we will test whether and under which circumstances this is the case. Our results will have important consequences for current theoretical models of how attention operates.             
The question how "images in the mind" control conscious experience and voluntary action is central to theories of selective attention. Finding new answers to this question will have important general theoretical and conceptual implications for attention research. But our research is also important from an applied perspective. A defining feature of life in our technologically advanced society is the attentional competition between multiple sources of information, which result in permanent demands on attentional object selection and choice. New insights into how attentional templates guide what individuals perceive and how they choose to act therefore has obvious practical implications for areas as diverse as education, workplace design, and economic decision making.  Adaptive perception requires the prioritization of relevant over irrelevant information. When we are looking for a specific book of which we only remember the color of its cover, we can limit our search to mainly that color. The mental representation of what we are looking for is called the attentional template (also target template, search template, attentional set; e.g., Folk et al., 1992). An attentional template is a flexible representation reflecting current selection preferences, as derived from continuously changing task demands and prior selection history. Even though attentional templates are essential for shaping and controlling perception and action in everyday life, surprisingly little is known about their nature. For example, when you look for your car keys, do you look for their shape, their color, or both? In case of the latter, are shape and color integrated in a single representation, or are they independently represented? Can you look for your wallet at the same time, without affecting your "key" template? Furthermore, it is often assumed that visual attention is guided by visual templates, but it is perfectly possible that non-visual types of representation (e.g., semantic codes) are also involved. Finally, the nature of the template may change fundamentally in the course of learning, as a result of selection history. The aim of this collaborative project is to answer some of these fundamental questions.13:Te3b,This data collection contains the behavioural and EEG data from the main experiments conducted in this research project.
The main goals of this project were as follows: human visual perception is strongly affected by current expectations and intentions. What is perceived is determined by what is attended, and what is attended is determined by "images in the mind" that guide attention in line with active goals and preferences. This project uses new experimental procedures and new methodological techniques (including temporally precise measures of electrical brain activity) to investigate how many things we can attend to at any time, and to study the adverse consequences of having to simultaneously attend to multiple objects in perception, visual working memory, and action. Are there systematic differences between individuals in their ability to attend to more than one thing at a time? We will also develop new methods to obtain precise measures of the speed of voluntary visual attention shifts: If attention is engaged at a particular location, how fast can it be moved to a new potentially relevant object? Our results suggest that the top-down guidance of attention is faster and more flexible than usually assumed, and we will test whether and under which circumstances this is the case. Our results will have important consequences for current theoretical models of how attention operates.             
The question how "images in the mind" control conscious experience and voluntary action is central to theories of selective attention. Finding new answers to this question will have important general theoretical and conceptual implications for attention research. But our research is also important from an applied perspective. A defining feature of life in our technologically advanced society is the attentional competition between multiple sources of information, which result in permanent demands on attentional object selection and choice. New insights into how attentional templates guide what individuals perceive and how they choose to act therefore has obvious practical implications for areas as diverse as education, workplace design, and economic decision making.  Adaptive perception requires the prioritization of relevant over irrelevant information. When we are looking for a specific book of which we only remember the color of its cover, we can limit our search to mainly that color. The mental representation of what we are looking for is called the attentional template (also target template, search template, attentional set; e.g., Folk et al., 1992). An attentional template is a flexible representation reflecting current selection preferences, as derived from continuously changing task demands and prior selection history. Even though attentional templates are essential for shaping and controlling perception and action in everyday life, surprisingly little is known about their nature. For example, when you look for your car keys, do you look for their shape, their color, or both? In case of the latter, are shape and color integrated in a single representation, or are they independently represented? Can you look for your wallet at the same time, without affecting your "key" template? Furthermore, it is often assumed that visual attention is guided by visual templates, but it is perfectly possible that non-visual types of representation (e.g., semantic codes) are also involved. Finally, the nature of the template may change fundamentally in the course of learning, as a result of selection history. The aim of this collaborative project is to answer some of these fundamental questions.14:Te3b,This data collection contains the behavioural and EEG data from the main experiments conducted in this research project.
The main goals of this project were as follows: human visual perception is strongly affected by current expectations and intentions. What is perceived is determined by what is attended, and what is attended is determined by "images in the mind" that guide attention in line with active goals and preferences. This project uses new experimental procedures and new methodological techniques (including temporally precise measures of electrical brain activity) to investigate how many things we can attend to at any time, and to study the adverse consequences of having to simultaneously attend to multiple objects in perception, visual working memory, and action. Are there systematic differences between individuals in their ability to attend to more than one thing at a time? We will also develop new methods to obtain precise measures of the speed of voluntary visual attention shifts: If attention is engaged at a particular location, how fast can it be moved to a new potentially relevant object? Our results suggest that the top-down guidance of attention is faster and more flexible than usually assumed, and we will test whether and under which circumstances this is the case. Our results will have important consequences for current theoretical models of how attention operates.             
The question how "images in the mind" control conscious experience and voluntary action is central to theories of selective attention. Finding new answers to this question will have important general theoretical and conceptual implications for attention research. But our research is also important from an applied perspective. A defining feature of life in our technologically advanced society is the attentional competition between multiple sources of information, which result in permanent demands on attentional object selection and choice. New insights into how attentional templates guide what individuals perceive and how they choose to act therefore has obvious practical implications for areas as diverse as education, workplace design, and economic decision making.  Adaptive perception requires the prioritization of relevant over irrelevant information. When we are looking for a specific book of which we only remember the color of its cover, we can limit our search to mainly that color. The mental representation of what we are looking for is called the attentional template (also target template, search template, attentional set; e.g., Folk et al., 1992). An attentional template is a flexible representation reflecting current selection preferences, as derived from continuously changing task demands and prior selection history. Even though attentional templates are essential for shaping and controlling perception and action in everyday life, surprisingly little is known about their nature. For example, when you look for your car keys, do you look for their shape, their color, or both? In case of the latter, are shape and color integrated in a single representation, or are they independently represented? Can you look for your wallet at the same time, without affecting your "key" template? Furthermore, it is often assumed that visual attention is guided by visual templates, but it is perfectly possible that non-visual types of representation (e.g., semantic codes) are also involved. Finally, the nature of the template may change fundamentally in the course of learning, as a result of selection history. The aim of this collaborative project is to answer some of these fundamental questions.e:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Tracing the template - Investigating the representation of perceptual relevance: Experimental data"}],["$","meta","2",{"name":"description","content":"$12"}],["$","meta","3",{"property":"og:title","content":"Tracing the template - Investigating the representation of perceptual relevance: Experimental data"}],["$","meta","4",{"property":"og:description","content":"$13"}],["$","meta","5",{"property":"og:url","content":"https://discoverynext.vercel.app/items/82851f1a9926bbecd0aa4e2e264ab9bb"}],["$","meta","6",{"property":"og:site_name","content":"Academic Resource Discovery"}],["$","meta","7",{"property":"og:locale","content":"en_US"}],["$","meta","8",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","meta","9",{"property":"og:image:width","content":"1200"}],["$","meta","10",{"property":"og:image:height","content":"630"}],["$","meta","11",{"property":"og:image:alt","content":"Tracing the template - Investigating the representation of perceptual relevance: Experimental data"}],["$","meta","12",{"property":"og:type","content":"website"}],["$","meta","13",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","14",{"name":"twitter:title","content":"Tracing the template - Investigating the representation of perceptual relevance: Experimental data"}],["$","meta","15",{"name":"twitter:description","content":"$14"}],["$","meta","16",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","link","17",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
c:null
