1:"$Sreact.fragment"
2:I[82104,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
3:I[10683,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"AuthProvider"]
4:I[63612,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"SearchProvider"]
5:I[68998,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
6:I[98904,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
7:I[15244,[],""]
8:I[43866,[],""]
9:I[14046,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"ToastContainer"]
b:I[86213,[],"OutletBoundary"]
d:I[86213,[],"MetadataBoundary"]
f:I[86213,[],"ViewportBoundary"]
11:I[34835,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/2c4d913f25bfc6bf.css","style"]
:HL["/search/_next/static/css/4921cfd18b262f8c.css","style"]
0:{"P":null,"b":"8r-g2-FTTcZL6JFFobnJN","p":"/search","c":["","items","behavioural-and-eye-tracking-experimental-data"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","behavioural-and-eye-tracking-experimental-data","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/2c4d913f25bfc6bf.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":["$","meta",null,{"name":"emotion-insertion-point","content":""}]}],["$","body",null,{"className":"__className_62a302","children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":[["$","$L5",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L6",null,{}],["$","$L5",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$L9",null,{"position":"bottom-right"}]]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","behavioural-and-eye-tracking-experimental-data","d"],["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$La",[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/4921cfd18b262f8c.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$Lb",null,{"children":"$Lc"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","FWb7x0kvrN-OViSXXmsyX",{"children":[["$","$Ld",null,{"children":"$Le"}],["$","$Lf",null,{"children":"$L10"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
10:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
13:I[5749,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","2154","static/chunks/834cb1aa-fe75579b2a50baac.js","3524","static/chunks/2170a4aa-66be1631595ccab0.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","1057","static/chunks/1057-d97430463abd6821.js","2282","static/chunks/2282-26bc5318a4471ee9.js","9234","static/chunks/9234-fce85e807baa599f.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","5733","static/chunks/5733-d0ad15157d7394e5.js","8173","static/chunks/8173-30737ce2fc776efb.js","613","static/chunks/613-3467f3d6fe7e6e6a.js","9756","static/chunks/9756-90c6220c809c4148.js","8738","static/chunks/8738-58586275b0d791e8.js","2649","static/chunks/2649-8d5f655ba5d1c168.js","1857","static/chunks/1857-99747bd4076c313b.js","2288","static/chunks/2288-ffb609d77f258e27.js","6387","static/chunks/app/items/%5Bslug%5D/page-38377216d08118bc.js"],"default"]
12:T1500,{"@context":"https://schema.org/","@type":"Dataset","name":"Behavioural and eye-tracking, Experimental data","description":"Data is of three types (1) Working memory performance behavioural data, (2) eye movement data, (3) Questionnaire data. This project investigates how threatening versus non-threatening expressions of emotion deferentially modulate the precision and durability of face identity-location bindings in visuospatial working memory. \nExperiments 1-4 comprise the main data and results of a visuo-spatial WM task in which participants (aged between 18-40 years) were asked to remember the identity and location of between 1 to 4 faces presented on a computer screen. After a maintenance period, a test face was presented in the centre of the screen and participants had to relocate this face to where it was. Faces during the encoding period conveyed emotion whereas the test face was neutral. In 4 experiments (Experiments 1, 2, 3a/b/c, 4), we manipulated different parameters such as: the number of study faces (Experiment 1),the duration of the maintenance period (Experiment 2), the type and the number of emotions present at encoding (Experiments 3 and 4). An additional experiment (Experiment 0) was conducted early on to assess the influence of\ncompeting emotions at encoding on purely visual WM.It is fundamental to normal, human social interaction that we are able to read other people's faces and infer from their expression how they are feeling and what their behavioural intention towards us may be. This is particularly important when a person exhibits a facial expression such as anger, which signals hostility or aggression and threatens our physical and emotional welfare. However, facial expressions are fleeting, lasting only between 0.5 and 4 seconds, so it is essential that we clearly and accurately remember who expressed an emotion and where that person is so that we can respond in an appropriate way.\n\nTo accurately remember who was where at a particular moment in time, we need to preserve this information in a temporary and short-lived memory store called visuo-spatial working memory. Working memory is an essential component of human life. It is used during every goal-related task, behaviour, and thought, and it enables us to keep track of unfolding events from second to second. Without working memory our daily lives would be chaotic and unmanageable, and it is important to understand how this special kind of memory supports social information processing.\n\nIn the current project I will develop a novel experimental task which investigates how threatening (angry) versus non-threatening (happy, sad, fearful) expressions of emotion influence how precisely we can recall the location of faces using visuo-spatial working memory.\n\nResearch has shown that angry faces rapidly attract and hold attention better than non-threatening expressions of emotion. There is also evidence that angry expressions improve the accuracy with which we use working memory to recognise a person. However, we currently have a very limited understanding of how facial expressions influence our ability to remember where that person was. Participants will be shown a number of angry and non-threatening faces, and they have to store these faces and their locations in working memory. These 'study' faces will then disappear for a second or so, after which one of them will reappear in a new location but with a neutral expression (now a 'test' face). The disappearance of the initial emotional expression to a neutral pose in the test face thus mirrors the fleeting nature of facial expressions in real life. Using a touch-screen, participants will use their finger to reposition the test face to its original location. The precision with which this response is made will be measured in terms of the distance between the original and recalled locations. Eye movements will also be recorded using an eye-tracking device, in order to examine how much each face is looked at. These eye movement patterns will help determine whether the amount of attention paid to a face and its location relates to how precisely it is subsequently repositioned. \n\nThe presence of an angry expression is predicted to enhance recall precision for who was where. It is also expected that angry faces will require less attention than non-threatening faces in order to remember their location accurately, due to the presence of threat stimulating the brain to process this information more rapidly and efficiently. In some experiments the amount of time participants are required to hold the faces in mind immediately after their disappearance (called the maintenance phase) will be increased. If memory for angry individuals is stronger and more durable over time, it is expected that recall of who was where will remain more precise for angry than non-threatening faces at extended maintenance intervals.","url":"https://discoverynext.vercel.app/items/behavioural-and-eye-tracking-experimental-data","keywords":["MEMORY"],"identifier":["http://dx.doi.org/10.5255/UKDA-SN-852758"],"includedInDataCatalog":[{"@type":"DataCatalog","name":"UK Data Service","url":"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=852758"}],"sponsor":[{"@type":"Organization","name":"Economic and Social Research Council"}],"temporalCoverage":"2015-03-02/2017-03-31"}14:T12b2,Data is of three types (1) Working memory performance behavioural data, (2) eye movement data, (3) Questionnaire data. This project investigates how threatening versus non-threatening expressions of emotion deferentially modulate the precision and durability of face identity-location bindings in visuospatial working memory. 
Experiments 1-4 comprise the main data and results of a visuo-spatial WM task in which participants (aged between 18-40 years) were asked to remember the identity and location of between 1 to 4 faces presented on a computer screen. After a maintenance period, a test face was presented in the centre of the screen and participants had to relocate this face to where it was. Faces during the encoding period conveyed emotion whereas the test face was neutral. In 4 experiments (Experiments 1, 2, 3a/b/c, 4), we manipulated different parameters such as: the number of study faces (Experiment 1),the duration of the maintenance period (Experiment 2), the type and the number of emotions present at encoding (Experiments 3 and 4). An additional experiment (Experiment 0) was conducted early on to assess the influence of
competing emotions at encoding on purely visual WM.It is fundamental to normal, human social interaction that we are able to read other people's faces and infer from their expression how they are feeling and what their behavioural intention towards us may be. This is particularly important when a person exhibits a facial expression such as anger, which signals hostility or aggression and threatens our physical and emotional welfare. However, facial expressions are fleeting, lasting only between 0.5 and 4 seconds, so it is essential that we clearly and accurately remember who expressed an emotion and where that person is so that we can respond in an appropriate way.

To accurately remember who was where at a particular moment in time, we need to preserve this information in a temporary and short-lived memory store called visuo-spatial working memory. Working memory is an essential component of human life. It is used during every goal-related task, behaviour, and thought, and it enables us to keep track of unfolding events from second to second. Without working memory our daily lives would be chaotic and unmanageable, and it is important to understand how this special kind of memory supports social information processing.

In the current project I will develop a novel experimental task which investigates how threatening (angry) versus non-threatening (happy, sad, fearful) expressions of emotion influence how precisely we can recall the location of faces using visuo-spatial working memory.

Research has shown that angry faces rapidly attract and hold attention better than non-threatening expressions of emotion. There is also evidence that angry expressions improve the accuracy with which we use working memory to recognise a person. However, we currently have a very limited understanding of how facial expressions influence our ability to remember where that person was. Participants will be shown a number of angry and non-threatening faces, and they have to store these faces and their locations in working memory. These 'study' faces will then disappear for a second or so, after which one of them will reappear in a new location but with a neutral expression (now a 'test' face). The disappearance of the initial emotional expression to a neutral pose in the test face thus mirrors the fleeting nature of facial expressions in real life. Using a touch-screen, participants will use their finger to reposition the test face to its original location. The precision with which this response is made will be measured in terms of the distance between the original and recalled locations. Eye movements will also be recorded using an eye-tracking device, in order to examine how much each face is looked at. These eye movement patterns will help determine whether the amount of attention paid to a face and its location relates to how precisely it is subsequently repositioned. 

The presence of an angry expression is predicted to enhance recall precision for who was where. It is also expected that angry faces will require less attention than non-threatening faces in order to remember their location accurately, due to the presence of threat stimulating the brain to process this information more rapidly and efficiently. In some experiments the amount of time participants are required to hold the faces in mind immediately after their disappearance (called the maintenance phase) will be increased. If memory for angry individuals is stronger and more durable over time, it is expected that recall of who was where will remain more precise for angry than non-threatening faces at extended maintenance intervals.a:[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$12"}}],["$","$L13",null,{"dataset":{"title":"Behavioural and eye-tracking, Experimental data","description":"$14","image":"$undefined","publisher":"$undefined","funders":"$undefined","geographicCoverage":"GB","temporalCoverage":"2015-03-02/2017-03-31","ageCoverage":"$undefined","studyDesign":[],"resourceType":"dataset","topics":["MEMORY"],"instruments":[],"dataCatalogs":[{"name":"UK Data Service","url":"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=852758","logo":"$undefined"}],"matchedVariables":[],"allVariables":[],"additionalLinks":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=852758","https://reshare.ukdataservice.ac.uk/852758","http://dx.doi.org/10.5255/UKDA-SN-852758","http://dx.doi.org/10.5255/UKDA-SN-852758"],"child_datasets":[],"aiSummary":null}}]]
15:T12b2,Data is of three types (1) Working memory performance behavioural data, (2) eye movement data, (3) Questionnaire data. This project investigates how threatening versus non-threatening expressions of emotion deferentially modulate the precision and durability of face identity-location bindings in visuospatial working memory. 
Experiments 1-4 comprise the main data and results of a visuo-spatial WM task in which participants (aged between 18-40 years) were asked to remember the identity and location of between 1 to 4 faces presented on a computer screen. After a maintenance period, a test face was presented in the centre of the screen and participants had to relocate this face to where it was. Faces during the encoding period conveyed emotion whereas the test face was neutral. In 4 experiments (Experiments 1, 2, 3a/b/c, 4), we manipulated different parameters such as: the number of study faces (Experiment 1),the duration of the maintenance period (Experiment 2), the type and the number of emotions present at encoding (Experiments 3 and 4). An additional experiment (Experiment 0) was conducted early on to assess the influence of
competing emotions at encoding on purely visual WM.It is fundamental to normal, human social interaction that we are able to read other people's faces and infer from their expression how they are feeling and what their behavioural intention towards us may be. This is particularly important when a person exhibits a facial expression such as anger, which signals hostility or aggression and threatens our physical and emotional welfare. However, facial expressions are fleeting, lasting only between 0.5 and 4 seconds, so it is essential that we clearly and accurately remember who expressed an emotion and where that person is so that we can respond in an appropriate way.

To accurately remember who was where at a particular moment in time, we need to preserve this information in a temporary and short-lived memory store called visuo-spatial working memory. Working memory is an essential component of human life. It is used during every goal-related task, behaviour, and thought, and it enables us to keep track of unfolding events from second to second. Without working memory our daily lives would be chaotic and unmanageable, and it is important to understand how this special kind of memory supports social information processing.

In the current project I will develop a novel experimental task which investigates how threatening (angry) versus non-threatening (happy, sad, fearful) expressions of emotion influence how precisely we can recall the location of faces using visuo-spatial working memory.

Research has shown that angry faces rapidly attract and hold attention better than non-threatening expressions of emotion. There is also evidence that angry expressions improve the accuracy with which we use working memory to recognise a person. However, we currently have a very limited understanding of how facial expressions influence our ability to remember where that person was. Participants will be shown a number of angry and non-threatening faces, and they have to store these faces and their locations in working memory. These 'study' faces will then disappear for a second or so, after which one of them will reappear in a new location but with a neutral expression (now a 'test' face). The disappearance of the initial emotional expression to a neutral pose in the test face thus mirrors the fleeting nature of facial expressions in real life. Using a touch-screen, participants will use their finger to reposition the test face to its original location. The precision with which this response is made will be measured in terms of the distance between the original and recalled locations. Eye movements will also be recorded using an eye-tracking device, in order to examine how much each face is looked at. These eye movement patterns will help determine whether the amount of attention paid to a face and its location relates to how precisely it is subsequently repositioned. 

The presence of an angry expression is predicted to enhance recall precision for who was where. It is also expected that angry faces will require less attention than non-threatening faces in order to remember their location accurately, due to the presence of threat stimulating the brain to process this information more rapidly and efficiently. In some experiments the amount of time participants are required to hold the faces in mind immediately after their disappearance (called the maintenance phase) will be increased. If memory for angry individuals is stronger and more durable over time, it is expected that recall of who was where will remain more precise for angry than non-threatening faces at extended maintenance intervals.16:T12b2,Data is of three types (1) Working memory performance behavioural data, (2) eye movement data, (3) Questionnaire data. This project investigates how threatening versus non-threatening expressions of emotion deferentially modulate the precision and durability of face identity-location bindings in visuospatial working memory. 
Experiments 1-4 comprise the main data and results of a visuo-spatial WM task in which participants (aged between 18-40 years) were asked to remember the identity and location of between 1 to 4 faces presented on a computer screen. After a maintenance period, a test face was presented in the centre of the screen and participants had to relocate this face to where it was. Faces during the encoding period conveyed emotion whereas the test face was neutral. In 4 experiments (Experiments 1, 2, 3a/b/c, 4), we manipulated different parameters such as: the number of study faces (Experiment 1),the duration of the maintenance period (Experiment 2), the type and the number of emotions present at encoding (Experiments 3 and 4). An additional experiment (Experiment 0) was conducted early on to assess the influence of
competing emotions at encoding on purely visual WM.It is fundamental to normal, human social interaction that we are able to read other people's faces and infer from their expression how they are feeling and what their behavioural intention towards us may be. This is particularly important when a person exhibits a facial expression such as anger, which signals hostility or aggression and threatens our physical and emotional welfare. However, facial expressions are fleeting, lasting only between 0.5 and 4 seconds, so it is essential that we clearly and accurately remember who expressed an emotion and where that person is so that we can respond in an appropriate way.

To accurately remember who was where at a particular moment in time, we need to preserve this information in a temporary and short-lived memory store called visuo-spatial working memory. Working memory is an essential component of human life. It is used during every goal-related task, behaviour, and thought, and it enables us to keep track of unfolding events from second to second. Without working memory our daily lives would be chaotic and unmanageable, and it is important to understand how this special kind of memory supports social information processing.

In the current project I will develop a novel experimental task which investigates how threatening (angry) versus non-threatening (happy, sad, fearful) expressions of emotion influence how precisely we can recall the location of faces using visuo-spatial working memory.

Research has shown that angry faces rapidly attract and hold attention better than non-threatening expressions of emotion. There is also evidence that angry expressions improve the accuracy with which we use working memory to recognise a person. However, we currently have a very limited understanding of how facial expressions influence our ability to remember where that person was. Participants will be shown a number of angry and non-threatening faces, and they have to store these faces and their locations in working memory. These 'study' faces will then disappear for a second or so, after which one of them will reappear in a new location but with a neutral expression (now a 'test' face). The disappearance of the initial emotional expression to a neutral pose in the test face thus mirrors the fleeting nature of facial expressions in real life. Using a touch-screen, participants will use their finger to reposition the test face to its original location. The precision with which this response is made will be measured in terms of the distance between the original and recalled locations. Eye movements will also be recorded using an eye-tracking device, in order to examine how much each face is looked at. These eye movement patterns will help determine whether the amount of attention paid to a face and its location relates to how precisely it is subsequently repositioned. 

The presence of an angry expression is predicted to enhance recall precision for who was where. It is also expected that angry faces will require less attention than non-threatening faces in order to remember their location accurately, due to the presence of threat stimulating the brain to process this information more rapidly and efficiently. In some experiments the amount of time participants are required to hold the faces in mind immediately after their disappearance (called the maintenance phase) will be increased. If memory for angry individuals is stronger and more durable over time, it is expected that recall of who was where will remain more precise for angry than non-threatening faces at extended maintenance intervals.17:T12b2,Data is of three types (1) Working memory performance behavioural data, (2) eye movement data, (3) Questionnaire data. This project investigates how threatening versus non-threatening expressions of emotion deferentially modulate the precision and durability of face identity-location bindings in visuospatial working memory. 
Experiments 1-4 comprise the main data and results of a visuo-spatial WM task in which participants (aged between 18-40 years) were asked to remember the identity and location of between 1 to 4 faces presented on a computer screen. After a maintenance period, a test face was presented in the centre of the screen and participants had to relocate this face to where it was. Faces during the encoding period conveyed emotion whereas the test face was neutral. In 4 experiments (Experiments 1, 2, 3a/b/c, 4), we manipulated different parameters such as: the number of study faces (Experiment 1),the duration of the maintenance period (Experiment 2), the type and the number of emotions present at encoding (Experiments 3 and 4). An additional experiment (Experiment 0) was conducted early on to assess the influence of
competing emotions at encoding on purely visual WM.It is fundamental to normal, human social interaction that we are able to read other people's faces and infer from their expression how they are feeling and what their behavioural intention towards us may be. This is particularly important when a person exhibits a facial expression such as anger, which signals hostility or aggression and threatens our physical and emotional welfare. However, facial expressions are fleeting, lasting only between 0.5 and 4 seconds, so it is essential that we clearly and accurately remember who expressed an emotion and where that person is so that we can respond in an appropriate way.

To accurately remember who was where at a particular moment in time, we need to preserve this information in a temporary and short-lived memory store called visuo-spatial working memory. Working memory is an essential component of human life. It is used during every goal-related task, behaviour, and thought, and it enables us to keep track of unfolding events from second to second. Without working memory our daily lives would be chaotic and unmanageable, and it is important to understand how this special kind of memory supports social information processing.

In the current project I will develop a novel experimental task which investigates how threatening (angry) versus non-threatening (happy, sad, fearful) expressions of emotion influence how precisely we can recall the location of faces using visuo-spatial working memory.

Research has shown that angry faces rapidly attract and hold attention better than non-threatening expressions of emotion. There is also evidence that angry expressions improve the accuracy with which we use working memory to recognise a person. However, we currently have a very limited understanding of how facial expressions influence our ability to remember where that person was. Participants will be shown a number of angry and non-threatening faces, and they have to store these faces and their locations in working memory. These 'study' faces will then disappear for a second or so, after which one of them will reappear in a new location but with a neutral expression (now a 'test' face). The disappearance of the initial emotional expression to a neutral pose in the test face thus mirrors the fleeting nature of facial expressions in real life. Using a touch-screen, participants will use their finger to reposition the test face to its original location. The precision with which this response is made will be measured in terms of the distance between the original and recalled locations. Eye movements will also be recorded using an eye-tracking device, in order to examine how much each face is looked at. These eye movement patterns will help determine whether the amount of attention paid to a face and its location relates to how precisely it is subsequently repositioned. 

The presence of an angry expression is predicted to enhance recall precision for who was where. It is also expected that angry faces will require less attention than non-threatening faces in order to remember their location accurately, due to the presence of threat stimulating the brain to process this information more rapidly and efficiently. In some experiments the amount of time participants are required to hold the faces in mind immediately after their disappearance (called the maintenance phase) will be increased. If memory for angry individuals is stronger and more durable over time, it is expected that recall of who was where will remain more precise for angry than non-threatening faces at extended maintenance intervals.e:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Behavioural and eye-tracking, Experimental data"}],["$","meta","2",{"name":"description","content":"$15"}],["$","meta","3",{"property":"og:title","content":"Behavioural and eye-tracking, Experimental data"}],["$","meta","4",{"property":"og:description","content":"$16"}],["$","meta","5",{"property":"og:url","content":"https://discoverynext.vercel.app/items/behavioural-and-eye-tracking-experimental-data"}],["$","meta","6",{"property":"og:site_name","content":"Academic Resource Discovery"}],["$","meta","7",{"property":"og:locale","content":"en_US"}],["$","meta","8",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","meta","9",{"property":"og:image:width","content":"1200"}],["$","meta","10",{"property":"og:image:height","content":"630"}],["$","meta","11",{"property":"og:image:alt","content":"Behavioural and eye-tracking, Experimental data"}],["$","meta","12",{"property":"og:type","content":"website"}],["$","meta","13",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","14",{"name":"twitter:title","content":"Behavioural and eye-tracking, Experimental data"}],["$","meta","15",{"name":"twitter:description","content":"$17"}],["$","meta","16",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","link","17",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
c:null
