1:"$Sreact.fragment"
2:I[82104,["380","static/chunks/380-d8fc8750446cdfbd.js","679","static/chunks/679-405abec3824763ec.js","565","static/chunks/565-54db0a7d2e1aba40.js","125","static/chunks/125-7d30c00b6eeefbe1.js","69","static/chunks/69-c55f2c213564cb8d.js","991","static/chunks/991-2ad0b5c736ce73b9.js","177","static/chunks/app/layout-c69c260b268ff7f5.js"],"default"]
3:I[68998,["380","static/chunks/380-d8fc8750446cdfbd.js","679","static/chunks/679-405abec3824763ec.js","565","static/chunks/565-54db0a7d2e1aba40.js","125","static/chunks/125-7d30c00b6eeefbe1.js","69","static/chunks/69-c55f2c213564cb8d.js","991","static/chunks/991-2ad0b5c736ce73b9.js","177","static/chunks/app/layout-c69c260b268ff7f5.js"],"default"]
4:I[71109,["380","static/chunks/380-d8fc8750446cdfbd.js","679","static/chunks/679-405abec3824763ec.js","565","static/chunks/565-54db0a7d2e1aba40.js","125","static/chunks/125-7d30c00b6eeefbe1.js","69","static/chunks/69-c55f2c213564cb8d.js","991","static/chunks/991-2ad0b5c736ce73b9.js","177","static/chunks/app/layout-c69c260b268ff7f5.js"],"default"]
5:I[15244,[],""]
6:I[43866,[],""]
7:I[14046,["380","static/chunks/380-d8fc8750446cdfbd.js","679","static/chunks/679-405abec3824763ec.js","565","static/chunks/565-54db0a7d2e1aba40.js","125","static/chunks/125-7d30c00b6eeefbe1.js","69","static/chunks/69-c55f2c213564cb8d.js","991","static/chunks/991-2ad0b5c736ce73b9.js","177","static/chunks/app/layout-c69c260b268ff7f5.js"],"ToastContainer"]
9:I[86213,[],"OutletBoundary"]
b:I[86213,[],"MetadataBoundary"]
d:I[86213,[],"ViewportBoundary"]
f:I[34835,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/a38081d17169b30a.css","style"]
:HL["/search/_next/static/css/4921cfd18b262f8c.css","style"]
0:{"P":null,"b":"N1iE1x_dlq9NtpeegQrw4","p":"/search","c":["","items","capture-24-activity-tracker-dataset-for-human-activity-recognition"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","capture-24-activity-tracker-dataset-for-human-activity-recognition","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/a38081d17169b30a.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":["$","meta",null,{"name":"emotion-insertion-point","content":""}]}],["$","body",null,{"className":"__className_efdaca","children":["$","$L2",null,{"children":[["$","$L3",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L4",null,{}],["$","$L3",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$L7",null,{"position":"bottom-right"}]]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","items","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","capture-24-activity-tracker-dataset-for-human-activity-recognition","d"],["$","$1","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","items","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L8",[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/4921cfd18b262f8c.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$L9",null,{"children":"$La"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","FwvuzdnmNGITx_ZaazVAV",{"children":[["$","$Lb",null,{"children":"$Lc"}],["$","$Ld",null,{"children":"$Le"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$f","$undefined"],"s":false,"S":true}
e:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
11:I[97791,["524","static/chunks/2170a4aa-d8e2ba2c4e2f8c29.js","380","static/chunks/380-d8fc8750446cdfbd.js","679","static/chunks/679-405abec3824763ec.js","565","static/chunks/565-54db0a7d2e1aba40.js","282","static/chunks/282-5f4ad41caba1fe80.js","26","static/chunks/26-9885e297d63f64e7.js","152","static/chunks/152-7ea4954c1b436a81.js","125","static/chunks/125-7d30c00b6eeefbe1.js","234","static/chunks/234-71007108384397ef.js","476","static/chunks/476-849dafa05fdb9044.js","434","static/chunks/434-f0b7236e40c0acbe.js","69","static/chunks/69-c55f2c213564cb8d.js","664","static/chunks/664-91a23a11f84e12fc.js","649","static/chunks/649-4bd46080f3e111f1.js","857","static/chunks/857-a8a200ff5ac3d3d2.js","544","static/chunks/544-9c223bdd40831b34.js","387","static/chunks/app/items/%5Bslug%5D/page-873e644b3f7fef5b.js"],"default"]
10:Tac1,{"@context":"https://schema.org/","@type":"Dataset","name":"Capture-24: Activity tracker dataset for human activity recognition","description":"This dataset contains Axivity AX3 wrist-worn activity tracker data that were collected from 151 participants in 2014-2016 around the Oxfordshire area. Participants were asked to wear the device in daily living for a period of roughly 24 hours, amounting to a total of almost 4,000 hours. Vicon Autograph wearable cameras and Whitehall II sleep diaries were used to obtain the ground truth activities performed during the period (e.g. sitting watching TV, walking the dog, washing dishes, sleeping), resulting in more than 2,500 hours of labelled data. Accompanying code to analyse this data is available at https://github.com/activityMonitoring/capture24. The following papers describe the data collection protocol in full: i.) Gershuny J, Harms T, Doherty A, Thomas E, Milton K, Kelly P, Foster C (2020) Testing self-report time-use diaries against objective instruments in real time. Sociological Methodology doi: 10.1177/0081175019884591; ii.) Willetts M, Hollowell S, Aslett L, Holmes C, Doherty A. (2018) Statistical machine learning of sleep and physical activity phenotypes from sensor data in 96,220 UK Biobank participants. Scientific Reports. 8(1):7961. Regarding Data Protection, the Clinical Data Set will not include any direct subject identifiers. However, it is possible that the Data Set may contain certain information that could be used in combination with other information to identify a specific individual, such as a combination of activities specific to that individual (\"Personal Data\"). Accordingly, in the conduct of the Analysis, users will comply with all applicable laws and regulations relating to information privacy. Further, the user agrees to preserve the confidentiality of, and not attempt to identify, individuals in the Data Set.","url":"https://discoverynext.vercel.app/items/capture-24-activity-tracker-dataset-for-human-activity-recognition","keywords":["activity trackers","human activity recognition","wearable technology","fitness trackers","machine learning","accelerometers"],"identifier":["10.5287/bodleian:NGx0JOMP5"],"variableMeasured":[{"name":"PXXX.csv","description":"Timestamp, e.g. 2016-11-13 02:18:00.123456"},{"name":"metadata.csv","description":"Participant ID"},{"name":"annotation-label-dictionary.csv","description":"Text annotation describing performed activities"}],"includedInDataCatalog":[{"@type":"DataCatalog","name":"Health Data Research Innovation Gateway","url":"https://healthdatagateway.org/en/dataset/830"}],"publisher":[{"@type":"Organization","name":"Oxford Research Archive"}],"temporalCoverage":"2014-01-01/2016-12-31","size":"151"}12:T6e6,This dataset contains Axivity AX3 wrist-worn activity tracker data that were collected from 151 participants in 2014-2016 around the Oxfordshire area. Participants were asked to wear the device in daily living for a period of roughly 24 hours, amounting to a total of almost 4,000 hours. Vicon Autograph wearable cameras and Whitehall II sleep diaries were used to obtain the ground truth activities performed during the period (e.g. sitting watching TV, walking the dog, washing dishes, sleeping), resulting in more than 2,500 hours of labelled data. Accompanying code to analyse this data is available at https://github.com/activityMonitoring/capture24. The following papers describe the data collection protocol in full: i.) Gershuny J, Harms T, Doherty A, Thomas E, Milton K, Kelly P, Foster C (2020) Testing self-report time-use diaries against objective instruments in real time. Sociological Methodology doi: 10.1177/0081175019884591; ii.) Willetts M, Hollowell S, Aslett L, Holmes C, Doherty A. (2018) Statistical machine learning of sleep and physical activity phenotypes from sensor data in 96,220 UK Biobank participants. Scientific Reports. 8(1):7961. Regarding Data Protection, the Clinical Data Set will not include any direct subject identifiers. However, it is possible that the Data Set may contain certain information that could be used in combination with other information to identify a specific individual, such as a combination of activities specific to that individual ("Personal Data"). Accordingly, in the conduct of the Analysis, users will comply with all applicable laws and regulations relating to information privacy. Further, the user agrees to preserve the confidentiality of, and not attempt to identify, individuals in the Data Set.8:[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$10"}}],["$","$L11",null,{"dataset":{"title":"Capture-24: Activity tracker dataset for human activity recognition","description":"$12","image":"$undefined","publisher":{"name":"Oxford Research Archive","url":"$undefined","logo":"$undefined"},"funders":"$undefined","geographicCoverage":"GB","temporalCoverage":"2014-01-01/2016-12-31","ageCoverage":"$undefined","studyDesign":[],"resourceType":"dataset","topics":["activity trackers","human activity recognition","wearable technology","fitness trackers","machine learning","accelerometers"],"instruments":[],"dataCatalogs":[{"name":"Health Data Research Innovation Gateway","url":"https://healthdatagateway.org/en/dataset/830","logo":"$undefined"}],"matchedVariables":[{"name":"metadata.csv","description":"Participant ID","source":["hdruk"]},{"name":"PXXX.csv","description":"Timestamp, e.g. 2016-11-13 02:18:00.123456","source":["hdruk"]},{"name":"annotation-label-dictionary.csv","description":"Text annotation describing performed activities","source":["hdruk"]}],"allVariables":[{"name":"PXXX.csv","description":"Timestamp, e.g. 2016-11-13 02:18:00.123456"},{"name":"metadata.csv","description":"Participant ID"},{"name":"annotation-label-dictionary.csv","description":"Text annotation describing performed activities"}],"additionalLinks":["https://healthdatagateway.org/en/dataset/830","https://doi.org/10.5287/bodleian:NGx0JOMP5"],"child_datasets":[]}}]]
13:T6e6,This dataset contains Axivity AX3 wrist-worn activity tracker data that were collected from 151 participants in 2014-2016 around the Oxfordshire area. Participants were asked to wear the device in daily living for a period of roughly 24 hours, amounting to a total of almost 4,000 hours. Vicon Autograph wearable cameras and Whitehall II sleep diaries were used to obtain the ground truth activities performed during the period (e.g. sitting watching TV, walking the dog, washing dishes, sleeping), resulting in more than 2,500 hours of labelled data. Accompanying code to analyse this data is available at https://github.com/activityMonitoring/capture24. The following papers describe the data collection protocol in full: i.) Gershuny J, Harms T, Doherty A, Thomas E, Milton K, Kelly P, Foster C (2020) Testing self-report time-use diaries against objective instruments in real time. Sociological Methodology doi: 10.1177/0081175019884591; ii.) Willetts M, Hollowell S, Aslett L, Holmes C, Doherty A. (2018) Statistical machine learning of sleep and physical activity phenotypes from sensor data in 96,220 UK Biobank participants. Scientific Reports. 8(1):7961. Regarding Data Protection, the Clinical Data Set will not include any direct subject identifiers. However, it is possible that the Data Set may contain certain information that could be used in combination with other information to identify a specific individual, such as a combination of activities specific to that individual ("Personal Data"). Accordingly, in the conduct of the Analysis, users will comply with all applicable laws and regulations relating to information privacy. Further, the user agrees to preserve the confidentiality of, and not attempt to identify, individuals in the Data Set.14:T6e6,This dataset contains Axivity AX3 wrist-worn activity tracker data that were collected from 151 participants in 2014-2016 around the Oxfordshire area. Participants were asked to wear the device in daily living for a period of roughly 24 hours, amounting to a total of almost 4,000 hours. Vicon Autograph wearable cameras and Whitehall II sleep diaries were used to obtain the ground truth activities performed during the period (e.g. sitting watching TV, walking the dog, washing dishes, sleeping), resulting in more than 2,500 hours of labelled data. Accompanying code to analyse this data is available at https://github.com/activityMonitoring/capture24. The following papers describe the data collection protocol in full: i.) Gershuny J, Harms T, Doherty A, Thomas E, Milton K, Kelly P, Foster C (2020) Testing self-report time-use diaries against objective instruments in real time. Sociological Methodology doi: 10.1177/0081175019884591; ii.) Willetts M, Hollowell S, Aslett L, Holmes C, Doherty A. (2018) Statistical machine learning of sleep and physical activity phenotypes from sensor data in 96,220 UK Biobank participants. Scientific Reports. 8(1):7961. Regarding Data Protection, the Clinical Data Set will not include any direct subject identifiers. However, it is possible that the Data Set may contain certain information that could be used in combination with other information to identify a specific individual, such as a combination of activities specific to that individual ("Personal Data"). Accordingly, in the conduct of the Analysis, users will comply with all applicable laws and regulations relating to information privacy. Further, the user agrees to preserve the confidentiality of, and not attempt to identify, individuals in the Data Set.15:T6e6,This dataset contains Axivity AX3 wrist-worn activity tracker data that were collected from 151 participants in 2014-2016 around the Oxfordshire area. Participants were asked to wear the device in daily living for a period of roughly 24 hours, amounting to a total of almost 4,000 hours. Vicon Autograph wearable cameras and Whitehall II sleep diaries were used to obtain the ground truth activities performed during the period (e.g. sitting watching TV, walking the dog, washing dishes, sleeping), resulting in more than 2,500 hours of labelled data. Accompanying code to analyse this data is available at https://github.com/activityMonitoring/capture24. The following papers describe the data collection protocol in full: i.) Gershuny J, Harms T, Doherty A, Thomas E, Milton K, Kelly P, Foster C (2020) Testing self-report time-use diaries against objective instruments in real time. Sociological Methodology doi: 10.1177/0081175019884591; ii.) Willetts M, Hollowell S, Aslett L, Holmes C, Doherty A. (2018) Statistical machine learning of sleep and physical activity phenotypes from sensor data in 96,220 UK Biobank participants. Scientific Reports. 8(1):7961. Regarding Data Protection, the Clinical Data Set will not include any direct subject identifiers. However, it is possible that the Data Set may contain certain information that could be used in combination with other information to identify a specific individual, such as a combination of activities specific to that individual ("Personal Data"). Accordingly, in the conduct of the Analysis, users will comply with all applicable laws and regulations relating to information privacy. Further, the user agrees to preserve the confidentiality of, and not attempt to identify, individuals in the Data Set.c:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Capture-24: Activity tracker dataset for human activity recognition"}],["$","meta","2",{"name":"description","content":"$13"}],["$","meta","3",{"property":"og:title","content":"Capture-24: Activity tracker dataset for human activity recognition"}],["$","meta","4",{"property":"og:description","content":"$14"}],["$","meta","5",{"property":"og:url","content":"https://discoverynext.vercel.app/items/capture-24-activity-tracker-dataset-for-human-activity-recognition"}],["$","meta","6",{"property":"og:site_name","content":"Academic Resource Discovery"}],["$","meta","7",{"property":"og:locale","content":"en_US"}],["$","meta","8",{"property":"og:image","content":"http://localhost:3000/harmony.png"}],["$","meta","9",{"property":"og:image:width","content":"1200"}],["$","meta","10",{"property":"og:image:height","content":"630"}],["$","meta","11",{"property":"og:image:alt","content":"Capture-24: Activity tracker dataset for human activity recognition"}],["$","meta","12",{"property":"og:type","content":"website"}],["$","meta","13",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","14",{"name":"twitter:title","content":"Capture-24: Activity tracker dataset for human activity recognition"}],["$","meta","15",{"name":"twitter:description","content":"$15"}],["$","meta","16",{"name":"twitter:image","content":"http://localhost:3000/harmony.png"}],["$","link","17",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
a:null
