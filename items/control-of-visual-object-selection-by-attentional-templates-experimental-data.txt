1:"$Sreact.fragment"
2:I[82104,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","4398","static/js/4398.8644925b.js","7177","static/js/app/layout.0819bb7e.js"],"default"]
3:I[17146,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","4398","static/js/4398.8644925b.js","7177","static/js/app/layout.0819bb7e.js"],"AuthProvider"]
4:I[83705,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","4398","static/js/4398.8644925b.js","7177","static/js/app/layout.0819bb7e.js"],"FirebaseProvider"]
5:"$Sreact.suspense"
6:I[63612,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","4398","static/js/4398.8644925b.js","7177","static/js/app/layout.0819bb7e.js"],"SearchProvider"]
7:I[68998,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","4398","static/js/4398.8644925b.js","7177","static/js/app/layout.0819bb7e.js"],"default"]
8:I[98904,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","4398","static/js/4398.8644925b.js","7177","static/js/app/layout.0819bb7e.js"],"default"]
9:I[15244,[],""]
a:I[43866,[],""]
b:I[14046,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","4398","static/js/4398.8644925b.js","7177","static/js/app/layout.0819bb7e.js"],"ToastContainer"]
d:I[86213,[],"OutletBoundary"]
f:I[86213,[],"MetadataBoundary"]
11:I[86213,[],"ViewportBoundary"]
13:I[34835,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/0d5b820fee8240e5.css","style"]
0:{"P":null,"b":"1RcbC2baE79jiGpG5vLIx","p":"/search","c":["","items","control-of-visual-object-selection-by-attentional-templates-experimental-data"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","control-of-visual-object-selection-by-attentional-templates-experimental-data","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/0d5b820fee8240e5.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":[["$","meta",null,{"name":"emotion-insertion-point","content":""}],["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"preconnect","href":"https://www.cataloguementalhealth.ac.uk"}],["$","link",null,{"rel":"dns-prefetch","href":"https://harmonydata.ac.uk"}],["$","style",null,{"dangerouslySetInnerHTML":{"__html":"\n            /* Ensure immediate rendering with Roboto and fallbacks */\n            * { \n              font-family: \"Roboto\", -apple-system, BlinkMacSystemFont, \"Segoe UI\", \"Oxygen\", \"Ubuntu\", \"Cantarell\", \"Fira Sans\", \"Droid Sans\", \"Helvetica Neue\", sans-serif !important;\n              font-display: swap;\n              -webkit-font-smoothing: antialiased;\n              -moz-osx-font-smoothing: grayscale;\n            }\n            body { \n              visibility: visible !important; \n              opacity: 1 !important; \n              margin: 0; \n              padding: 0; \n            }\n          "}}]]}],["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":["$","$5",null,{"fallback":["$","div",null,{"children":"Loading..."}],"children":["$","$L6",null,{"children":[["$","$L7",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L8",null,{}],["$","$L7",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L9",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$Lb",null,{"position":"bottom-right"}]]}]}]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L9",null,{"parallelRouterKey":"children","segmentPath":["children","items","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","control-of-visual-object-selection-by-attentional-templates-experimental-data","d"],["$","$1","c",{"children":[null,["$","$L9",null,{"parallelRouterKey":"children","segmentPath":["children","items","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$Lc",null,["$","$Ld",null,{"children":"$Le"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","Jd_d0xgsWvz_boC5o64lQ",{"children":[["$","$Lf",null,{"children":"$L10"}],["$","$L11",null,{"children":"$L12"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$13","$undefined"],"s":false,"S":true}
14:I[53704,["6586","static/js/6586.2e946dbf.js","8378","static/js/8378.a1bea36e.js","2282","static/js/2282.e20001b9.js","9809","static/js/9809.f9049b43.js","6741","static/js/6741.ce01eadc.js","2649","static/js/2649.95608f08.js","4398","static/js/4398.8644925b.js","1857","static/js/1857.a01744c0.js","7626","static/js/7626.cc8608ac.js","6387","static/js/app/items/%5Bslug%5D/page.ff89d9aa.js"],""]
16:I[77626,["6586","static/js/6586.2e946dbf.js","8378","static/js/8378.a1bea36e.js","2282","static/js/2282.e20001b9.js","9809","static/js/9809.f9049b43.js","6741","static/js/6741.ce01eadc.js","2649","static/js/2649.95608f08.js","4398","static/js/4398.8644925b.js","1857","static/js/1857.a01744c0.js","7626","static/js/7626.cc8608ac.js","6387","static/js/app/items/%5Bslug%5D/page.ff89d9aa.js"],"default"]
15:T708,{"@context":"https://schema.org/","@type":"Dataset","name":"The control of visual object selection by attentional templates -Experimental data","description":"This data collection contains the behavioural and EEG data from the main experiments conducted in this research project.\nHuman visual perception is strongly affected by current expectations and intentions. What is perceived is determined by what is attended, and what is attended is determined by images in the mind that guide attention in line with active goals and preferences. This project uses new experimental procedures and methodological techniques (including temporally precise measures of electrical brain activity) to investigate how many things can be attended at any time, and to study the adverse consequences of having to simultaneously attend to multiple objects in perception, visual working memory, and action.\n\nAre there systematic differences between individuals in their ability to attend to more than one thing at a time? New methods will be developed to obtain precise measures of the speed of voluntary visual attention shifts: If attention is engaged at a particular location, how fast can it be moved to a new potentially relevant object? Initial results suggest that the top-down guidance of attention is faster and more flexible than usually assumed, and the project will test whether and under which circumstances this is the case. Experimental results will have important consequences for current theoretical models of how attention operates.","url":"https://harmonydata.ac.uk/search/items/control-of-visual-object-selection-by-attentional-templates-experimental-data","identifier":["http://dx.doi.org/10.5255/UKDA-SN-852464"],"keywords":["EXPERIMENTAL PSYCHOLOGY"],"temporalCoverage":"2013-12-01/2016-11-30"}17:T55b,This data collection contains the behavioural and EEG data from the main experiments conducted in this research project.
Human visual perception is strongly affected by current expectations and intentions. What is perceived is determined by what is attended, and what is attended is determined by images in the mind that guide attention in line with active goals and preferences. This project uses new experimental procedures and methodological techniques (including temporally precise measures of electrical brain activity) to investigate how many things can be attended at any time, and to study the adverse consequences of having to simultaneously attend to multiple objects in perception, visual working memory, and action.

Are there systematic differences between individuals in their ability to attend to more than one thing at a time? New methods will be developed to obtain precise measures of the speed of voluntary visual attention shifts: If attention is engaged at a particular location, how fast can it be moved to a new potentially relevant object? Initial results suggest that the top-down guidance of attention is faster and more flexible than usually assumed, and the project will test whether and under which circumstances this is the case. Experimental results will have important consequences for current theoretical models of how attention operates.c:["$","$5",null,{"fallback":["$","div",null,{"children":"Loading..."}],"children":[["$","$L14",null,{"strategy":"beforeInteractive","id":"structured-data","type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$15"}}],["$","$L16",null,{"study":{"dataset_schema":{"@context":"https://schema.org/","@type":"Dataset","name":"The control of visual object selection by attentional templates -Experimental data","description":"$17","url":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=852464","https://reshare.ukdataservice.ac.uk/852464"],"keywords":["EXPERIMENTAL PSYCHOLOGY"],"identifier":["http://dx.doi.org/10.5255/UKDA-SN-852464"],"includedInDataCatalog":[{"@type":"DataCatalog","name":"UK Data Service","url":"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=852464"}],"sponsor":[{"@type":"Organization","name":"ESRC"}],"temporalCoverage":"2013-12-01/2016-11-30"},"extra_data":{"harmony_id":"ukds/852464","dois":["http://dx.doi.org/10.5255/UKDA-SN-852464"],"country_codes":["GB"],"language_codes":["en"],"name":"The control of visual object selection by attentional templates -Experimental data","instruments":[],"sex":"male","geographic_coverage":"London","duration_years":3,"source":["ukds"],"data_access":"The Data Collection is available to any user without the requirement for registration for download/access.","start_year":2013,"resource_type":"dataset","genetic_data_collected":false,"urls":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=852464","https://reshare.ukdataservice.ac.uk/852464"],"ai_summary":null,"study_design":[],"end_year":2016,"slug":"control-of-visual-object-selection-by-attentional-templates-experimental-data","num_variables":null,"uuid":"511f7604d8bca3a17e2711825f6f0e85"},"distance":0,"score":0,"parent":{},"ancestors":[]}}]]}]
12:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
18:T55b,This data collection contains the behavioural and EEG data from the main experiments conducted in this research project.
Human visual perception is strongly affected by current expectations and intentions. What is perceived is determined by what is attended, and what is attended is determined by images in the mind that guide attention in line with active goals and preferences. This project uses new experimental procedures and methodological techniques (including temporally precise measures of electrical brain activity) to investigate how many things can be attended at any time, and to study the adverse consequences of having to simultaneously attend to multiple objects in perception, visual working memory, and action.

Are there systematic differences between individuals in their ability to attend to more than one thing at a time? New methods will be developed to obtain precise measures of the speed of voluntary visual attention shifts: If attention is engaged at a particular location, how fast can it be moved to a new potentially relevant object? Initial results suggest that the top-down guidance of attention is faster and more flexible than usually assumed, and the project will test whether and under which circumstances this is the case. Experimental results will have important consequences for current theoretical models of how attention operates.19:T55b,This data collection contains the behavioural and EEG data from the main experiments conducted in this research project.
Human visual perception is strongly affected by current expectations and intentions. What is perceived is determined by what is attended, and what is attended is determined by images in the mind that guide attention in line with active goals and preferences. This project uses new experimental procedures and methodological techniques (including temporally precise measures of electrical brain activity) to investigate how many things can be attended at any time, and to study the adverse consequences of having to simultaneously attend to multiple objects in perception, visual working memory, and action.

Are there systematic differences between individuals in their ability to attend to more than one thing at a time? New methods will be developed to obtain precise measures of the speed of voluntary visual attention shifts: If attention is engaged at a particular location, how fast can it be moved to a new potentially relevant object? Initial results suggest that the top-down guidance of attention is faster and more flexible than usually assumed, and the project will test whether and under which circumstances this is the case. Experimental results will have important consequences for current theoretical models of how attention operates.1a:T55b,This data collection contains the behavioural and EEG data from the main experiments conducted in this research project.
Human visual perception is strongly affected by current expectations and intentions. What is perceived is determined by what is attended, and what is attended is determined by images in the mind that guide attention in line with active goals and preferences. This project uses new experimental procedures and methodological techniques (including temporally precise measures of electrical brain activity) to investigate how many things can be attended at any time, and to study the adverse consequences of having to simultaneously attend to multiple objects in perception, visual working memory, and action.

Are there systematic differences between individuals in their ability to attend to more than one thing at a time? New methods will be developed to obtain precise measures of the speed of voluntary visual attention shifts: If attention is engaged at a particular location, how fast can it be moved to a new potentially relevant object? Initial results suggest that the top-down guidance of attention is faster and more flexible than usually assumed, and the project will test whether and under which circumstances this is the case. Experimental results will have important consequences for current theoretical models of how attention operates.10:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"The control of visual object selection by attentional templates -Experimental data"}],["$","meta","2",{"name":"description","content":"$18"}],["$","meta","3",{"property":"og:title","content":"The control of visual object selection by attentional templates -Experimental data"}],["$","meta","4",{"property":"og:description","content":"$19"}],["$","meta","5",{"property":"og:url","content":"https://harmonydata.ac.uk/search/items/control-of-visual-object-selection-by-attentional-templates-experimental-data"}],["$","meta","6",{"property":"og:site_name","content":"Academic Resource Discovery"}],["$","meta","7",{"property":"og:locale","content":"en_US"}],["$","meta","8",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","meta","9",{"property":"og:image:width","content":"1200"}],["$","meta","10",{"property":"og:image:height","content":"630"}],["$","meta","11",{"property":"og:image:alt","content":"The control of visual object selection by attentional templates -Experimental data"}],["$","meta","12",{"property":"og:type","content":"website"}],["$","meta","13",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","14",{"name":"twitter:title","content":"The control of visual object selection by attentional templates -Experimental data"}],["$","meta","15",{"name":"twitter:description","content":"$1a"}],["$","meta","16",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","link","17",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
e:null
