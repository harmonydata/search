1:"$Sreact.fragment"
2:I[82104,["6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","9756","static/js/9756.65c7d9ea.js","5246","static/js/5246.d88343e0.js","7177","static/js/app/layout.2233f7cc.js"],"default"]
3:I[17146,["6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","9756","static/js/9756.65c7d9ea.js","5246","static/js/5246.d88343e0.js","7177","static/js/app/layout.2233f7cc.js"],"AuthProvider"]
4:I[63612,["6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","9756","static/js/9756.65c7d9ea.js","5246","static/js/5246.d88343e0.js","7177","static/js/app/layout.2233f7cc.js"],"SearchProvider"]
5:I[68998,["6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","9756","static/js/9756.65c7d9ea.js","5246","static/js/5246.d88343e0.js","7177","static/js/app/layout.2233f7cc.js"],"default"]
6:I[98904,["6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","9756","static/js/9756.65c7d9ea.js","5246","static/js/5246.d88343e0.js","7177","static/js/app/layout.2233f7cc.js"],"default"]
7:I[15244,[],""]
8:I[43866,[],""]
9:I[14046,["6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","9756","static/js/9756.65c7d9ea.js","5246","static/js/5246.d88343e0.js","7177","static/js/app/layout.2233f7cc.js"],"ToastContainer"]
b:I[86213,[],"OutletBoundary"]
d:I[86213,[],"MetadataBoundary"]
f:I[86213,[],"ViewportBoundary"]
11:I[34835,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/a38392bd344718e4.css","style"]
:HL["/search/_next/static/css/4921cfd18b262f8c.css","style"]
0:{"P":null,"b":"0Pw2B2A-ctu_hDkpgRDyX","p":"/search","c":["","items","elderlucid-london-ucl-older-adults-clear-speech-in-interaction-database"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","elderlucid-london-ucl-older-adults-clear-speech-in-interaction-database","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/a38392bd344718e4.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":["$","meta",null,{"name":"emotion-insertion-point","content":""}]}],["$","body",null,{"className":"__className_55b4bf","children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":[["$","$L5",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L6",null,{}],["$","$L5",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$L9",null,{"position":"bottom-right"}]]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","elderlucid-london-ucl-older-adults-clear-speech-in-interaction-database","d"],["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$La",[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/4921cfd18b262f8c.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$Lb",null,{"children":"$Lc"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","y-iMgpbx6zZGFCjxXm3Aq",{"children":[["$","$Ld",null,{"children":"$Le"}],["$","$Lf",null,{"children":"$L10"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
12:I[53704,["3524","static/js/2170a4aa.3678665e.js","6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","1057","static/js/1057.fef3cc4c.js","2282","static/js/2282.e20001b9.js","9234","static/js/9234.7cf96505.js","2926","static/js/2926.76e4f620.js","7511","static/js/7511.a52b23cb.js","8173","static/js/8173.582c8c90.js","613","static/js/613.da2777c4.js","9756","static/js/9756.65c7d9ea.js","97","static/js/97.c5459b6e.js","2649","static/js/2649.4d01838c.js","1857","static/js/1857.a01744c0.js","280","static/js/280.07f8eb1b.js","9123","static/js/9123.0b9c8079.js","6387","static/js/app/items/%5Bslug%5D/page.e8e51342.js"],""]
14:I[5749,["3524","static/js/2170a4aa.3678665e.js","6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","1057","static/js/1057.fef3cc4c.js","2282","static/js/2282.e20001b9.js","9234","static/js/9234.7cf96505.js","2926","static/js/2926.76e4f620.js","7511","static/js/7511.a52b23cb.js","8173","static/js/8173.582c8c90.js","613","static/js/613.da2777c4.js","9756","static/js/9756.65c7d9ea.js","97","static/js/97.c5459b6e.js","2649","static/js/2649.4d01838c.js","1857","static/js/1857.a01744c0.js","280","static/js/280.07f8eb1b.js","9123","static/js/9123.0b9c8079.js","6387","static/js/app/items/%5Bslug%5D/page.e8e51342.js"],"default"]
13:T13cf,{"@context":"https://schema.org/","@type":"Dataset","name":"elderLUCID: London UCL Older adults' clear speech in interaction database","description":"This collection contains the quantitative data resulting from the analysis of the elderLUCID audio corpus – a set of speech recordings collected for 83 adults aged 19 to 84 years inclusive. Recordings were made while participants carried out two types of collaborative tasks with a conversational partner who was a young adult of the same sex: (1) a ‘spot the difference’ picture task (‘diapix’) where the conversational partners had to collaborate to find 12 differences between their pictures and (2) a BKB sentence repetition task where the key participant had to read a set of sentences to their partner who had to repeat them back. The two tasks were carried out by each participant pair in four different conditions: (1) in good listening conditions when both could hear each other normally (NORM condition), or when perception was impaired for one or both of the participants by (2) simulating a severe-to-profound hearing loss in the conversational partner (HLS condition), (3) adding multispeaker babble noise to the audio channel for the conversational partner (BAB1 condition) or to the audio channel of both participants (BAB2 condition). The aim of the study was to examine the clarification strategies used by older adults and young adult controls to maintain effective communication in adverse communicative conditions. The SPSS spreadsheet contains, for each of the 83 participants, quantitative data resulting from (a) the acoustic analysis of the recordings, (b) measures of communication efficiency and (c) background auditory and cognitive measures.  Speech communication can be difficult for older people, due to the combined effects of age-related hearing loss, which is common over the age of 65, age-related decline in the quality of phonation and speech articulation, and cognitive problems such as poorer short-term memory and processing speed. Past studies of how older individuals perceive and produce speech sounds have tended to consider these abilities independently of each other using controlled materials, such as read words or sentences. These studies tell us little about how older speakers function when using speech for communicative purposes, and how these various factors interact. For example, it has been shown that older people benefit from seeing their interlocutor in conversations, but audiovisual speech places greater demands on cognitive processing than auditory speech which leads to increased listener effort and reduced information recall. \nIn our project, we propose to gain a comprehensive account of older people's speech production and perception in situations involving communication with another individual. Adults with age-related hearing loss and the rarer group of older adults with normal hearing will be included as well as younger adult controls. In Study 1, communication with another speaker, while reading sentences or completing a problem-solving task, will either be in good listening conditions, where both speakers hear each other normally, or in adverse conditions, where the participant has to get their message across to another speaker who has a simulated hearing loss or when both are speaking in a noisy background. These comparisons will enable us to get a sense of the degree to which an older person is able to adapt their speech to overcome difficult listening conditions, a skill which is of paramount importance in speech communication in everyday life. We will obtain high-quality digital recordings of the participants' speech but will also, via sensors placed on the neck, record information about their vocal fold vibration, which determines the quality of their voice. Video recordings will also be analysed to investigate whether older speakers make use of eye gaze and head gestures to signal aspects of discourse such as turn-taking and back-channelling (e.g., saying 'okay' to signal understanding), to the same degree as younger speakers. In Study 2, older and younger listeners with normal and impaired hearing will be presented some of the sentence materials recorded in Study 1 by all speaker groups in good and adverse listening conditions. Tests will be presented in both auditory-alone and audiovisual conditions. Intelligibility tests will be run to see what impact age, hearing status and visual cues have on speech understanding and to see whether the 'clear speech' adaptations made by older speakers to counter the effects of poor communication conditions gives the same benefit to that of younger speakers. Sentence recall tests will also be run to investigate whether the listening effort is reduced listening to 'clear speech'.","url":"https://harmonydata.ac.uk/search/items/elderlucid-london-ucl-older-adults-clear-speech-in-interaction-database","identifier":["http://dx.doi.org/10.5255/UKDA-SN-852906"],"keywords":["SPEECH","ACOUSTICS","AGEING POPULATION"],"temporalCoverage":"2014-08-01/2017-07-31"}15:T1224,This collection contains the quantitative data resulting from the analysis of the elderLUCID audio corpus – a set of speech recordings collected for 83 adults aged 19 to 84 years inclusive. Recordings were made while participants carried out two types of collaborative tasks with a conversational partner who was a young adult of the same sex: (1) a ‘spot the difference’ picture task (‘diapix’) where the conversational partners had to collaborate to find 12 differences between their pictures and (2) a BKB sentence repetition task where the key participant had to read a set of sentences to their partner who had to repeat them back. The two tasks were carried out by each participant pair in four different conditions: (1) in good listening conditions when both could hear each other normally (NORM condition), or when perception was impaired for one or both of the participants by (2) simulating a severe-to-profound hearing loss in the conversational partner (HLS condition), (3) adding multispeaker babble noise to the audio channel for the conversational partner (BAB1 condition) or to the audio channel of both participants (BAB2 condition). The aim of the study was to examine the clarification strategies used by older adults and young adult controls to maintain effective communication in adverse communicative conditions. The SPSS spreadsheet contains, for each of the 83 participants, quantitative data resulting from (a) the acoustic analysis of the recordings, (b) measures of communication efficiency and (c) background auditory and cognitive measures.  Speech communication can be difficult for older people, due to the combined effects of age-related hearing loss, which is common over the age of 65, age-related decline in the quality of phonation and speech articulation, and cognitive problems such as poorer short-term memory and processing speed. Past studies of how older individuals perceive and produce speech sounds have tended to consider these abilities independently of each other using controlled materials, such as read words or sentences. These studies tell us little about how older speakers function when using speech for communicative purposes, and how these various factors interact. For example, it has been shown that older people benefit from seeing their interlocutor in conversations, but audiovisual speech places greater demands on cognitive processing than auditory speech which leads to increased listener effort and reduced information recall. 
In our project, we propose to gain a comprehensive account of older people's speech production and perception in situations involving communication with another individual. Adults with age-related hearing loss and the rarer group of older adults with normal hearing will be included as well as younger adult controls. In Study 1, communication with another speaker, while reading sentences or completing a problem-solving task, will either be in good listening conditions, where both speakers hear each other normally, or in adverse conditions, where the participant has to get their message across to another speaker who has a simulated hearing loss or when both are speaking in a noisy background. These comparisons will enable us to get a sense of the degree to which an older person is able to adapt their speech to overcome difficult listening conditions, a skill which is of paramount importance in speech communication in everyday life. We will obtain high-quality digital recordings of the participants' speech but will also, via sensors placed on the neck, record information about their vocal fold vibration, which determines the quality of their voice. Video recordings will also be analysed to investigate whether older speakers make use of eye gaze and head gestures to signal aspects of discourse such as turn-taking and back-channelling (e.g., saying 'okay' to signal understanding), to the same degree as younger speakers. In Study 2, older and younger listeners with normal and impaired hearing will be presented some of the sentence materials recorded in Study 1 by all speaker groups in good and adverse listening conditions. Tests will be presented in both auditory-alone and audiovisual conditions. Intelligibility tests will be run to see what impact age, hearing status and visual cues have on speech understanding and to see whether the 'clear speech' adaptations made by older speakers to counter the effects of poor communication conditions gives the same benefit to that of younger speakers. Sentence recall tests will also be run to investigate whether the listening effort is reduced listening to 'clear speech'.a:[["$","$L12",null,{"strategy":"beforeInteractive","id":"structured-data","type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$13"}}],["$","$L14",null,{"dataset":{"title":"elderLUCID: London UCL Older adults' clear speech in interaction database","description":"$15","image":"$undefined","publisher":"$undefined","funders":"$undefined","geographicCoverage":"GB","temporalCoverage":"2014-08-01/2017-07-31","ageCoverage":"$undefined","studyDesign":[],"resourceType":"dataset","topics":["SPEECH","ACOUSTICS","AGEING POPULATION"],"instruments":[],"dataCatalogs":[{"name":"UK Data Service","url":"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=852906","logo":"$undefined"}],"matchedVariables":[],"allVariables":[],"additionalLinks":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=852906","https://reshare.ukdataservice.ac.uk/852906","http://dx.doi.org/10.5255/UKDA-SN-852906","http://dx.doi.org/10.5255/UKDA-SN-852906"],"child_datasets":[],"aiSummary":null}}]]
10:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
16:T1224,This collection contains the quantitative data resulting from the analysis of the elderLUCID audio corpus – a set of speech recordings collected for 83 adults aged 19 to 84 years inclusive. Recordings were made while participants carried out two types of collaborative tasks with a conversational partner who was a young adult of the same sex: (1) a ‘spot the difference’ picture task (‘diapix’) where the conversational partners had to collaborate to find 12 differences between their pictures and (2) a BKB sentence repetition task where the key participant had to read a set of sentences to their partner who had to repeat them back. The two tasks were carried out by each participant pair in four different conditions: (1) in good listening conditions when both could hear each other normally (NORM condition), or when perception was impaired for one or both of the participants by (2) simulating a severe-to-profound hearing loss in the conversational partner (HLS condition), (3) adding multispeaker babble noise to the audio channel for the conversational partner (BAB1 condition) or to the audio channel of both participants (BAB2 condition). The aim of the study was to examine the clarification strategies used by older adults and young adult controls to maintain effective communication in adverse communicative conditions. The SPSS spreadsheet contains, for each of the 83 participants, quantitative data resulting from (a) the acoustic analysis of the recordings, (b) measures of communication efficiency and (c) background auditory and cognitive measures.  Speech communication can be difficult for older people, due to the combined effects of age-related hearing loss, which is common over the age of 65, age-related decline in the quality of phonation and speech articulation, and cognitive problems such as poorer short-term memory and processing speed. Past studies of how older individuals perceive and produce speech sounds have tended to consider these abilities independently of each other using controlled materials, such as read words or sentences. These studies tell us little about how older speakers function when using speech for communicative purposes, and how these various factors interact. For example, it has been shown that older people benefit from seeing their interlocutor in conversations, but audiovisual speech places greater demands on cognitive processing than auditory speech which leads to increased listener effort and reduced information recall. 
In our project, we propose to gain a comprehensive account of older people's speech production and perception in situations involving communication with another individual. Adults with age-related hearing loss and the rarer group of older adults with normal hearing will be included as well as younger adult controls. In Study 1, communication with another speaker, while reading sentences or completing a problem-solving task, will either be in good listening conditions, where both speakers hear each other normally, or in adverse conditions, where the participant has to get their message across to another speaker who has a simulated hearing loss or when both are speaking in a noisy background. These comparisons will enable us to get a sense of the degree to which an older person is able to adapt their speech to overcome difficult listening conditions, a skill which is of paramount importance in speech communication in everyday life. We will obtain high-quality digital recordings of the participants' speech but will also, via sensors placed on the neck, record information about their vocal fold vibration, which determines the quality of their voice. Video recordings will also be analysed to investigate whether older speakers make use of eye gaze and head gestures to signal aspects of discourse such as turn-taking and back-channelling (e.g., saying 'okay' to signal understanding), to the same degree as younger speakers. In Study 2, older and younger listeners with normal and impaired hearing will be presented some of the sentence materials recorded in Study 1 by all speaker groups in good and adverse listening conditions. Tests will be presented in both auditory-alone and audiovisual conditions. Intelligibility tests will be run to see what impact age, hearing status and visual cues have on speech understanding and to see whether the 'clear speech' adaptations made by older speakers to counter the effects of poor communication conditions gives the same benefit to that of younger speakers. Sentence recall tests will also be run to investigate whether the listening effort is reduced listening to 'clear speech'.17:T1224,This collection contains the quantitative data resulting from the analysis of the elderLUCID audio corpus – a set of speech recordings collected for 83 adults aged 19 to 84 years inclusive. Recordings were made while participants carried out two types of collaborative tasks with a conversational partner who was a young adult of the same sex: (1) a ‘spot the difference’ picture task (‘diapix’) where the conversational partners had to collaborate to find 12 differences between their pictures and (2) a BKB sentence repetition task where the key participant had to read a set of sentences to their partner who had to repeat them back. The two tasks were carried out by each participant pair in four different conditions: (1) in good listening conditions when both could hear each other normally (NORM condition), or when perception was impaired for one or both of the participants by (2) simulating a severe-to-profound hearing loss in the conversational partner (HLS condition), (3) adding multispeaker babble noise to the audio channel for the conversational partner (BAB1 condition) or to the audio channel of both participants (BAB2 condition). The aim of the study was to examine the clarification strategies used by older adults and young adult controls to maintain effective communication in adverse communicative conditions. The SPSS spreadsheet contains, for each of the 83 participants, quantitative data resulting from (a) the acoustic analysis of the recordings, (b) measures of communication efficiency and (c) background auditory and cognitive measures.  Speech communication can be difficult for older people, due to the combined effects of age-related hearing loss, which is common over the age of 65, age-related decline in the quality of phonation and speech articulation, and cognitive problems such as poorer short-term memory and processing speed. Past studies of how older individuals perceive and produce speech sounds have tended to consider these abilities independently of each other using controlled materials, such as read words or sentences. These studies tell us little about how older speakers function when using speech for communicative purposes, and how these various factors interact. For example, it has been shown that older people benefit from seeing their interlocutor in conversations, but audiovisual speech places greater demands on cognitive processing than auditory speech which leads to increased listener effort and reduced information recall. 
In our project, we propose to gain a comprehensive account of older people's speech production and perception in situations involving communication with another individual. Adults with age-related hearing loss and the rarer group of older adults with normal hearing will be included as well as younger adult controls. In Study 1, communication with another speaker, while reading sentences or completing a problem-solving task, will either be in good listening conditions, where both speakers hear each other normally, or in adverse conditions, where the participant has to get their message across to another speaker who has a simulated hearing loss or when both are speaking in a noisy background. These comparisons will enable us to get a sense of the degree to which an older person is able to adapt their speech to overcome difficult listening conditions, a skill which is of paramount importance in speech communication in everyday life. We will obtain high-quality digital recordings of the participants' speech but will also, via sensors placed on the neck, record information about their vocal fold vibration, which determines the quality of their voice. Video recordings will also be analysed to investigate whether older speakers make use of eye gaze and head gestures to signal aspects of discourse such as turn-taking and back-channelling (e.g., saying 'okay' to signal understanding), to the same degree as younger speakers. In Study 2, older and younger listeners with normal and impaired hearing will be presented some of the sentence materials recorded in Study 1 by all speaker groups in good and adverse listening conditions. Tests will be presented in both auditory-alone and audiovisual conditions. Intelligibility tests will be run to see what impact age, hearing status and visual cues have on speech understanding and to see whether the 'clear speech' adaptations made by older speakers to counter the effects of poor communication conditions gives the same benefit to that of younger speakers. Sentence recall tests will also be run to investigate whether the listening effort is reduced listening to 'clear speech'.18:T1224,This collection contains the quantitative data resulting from the analysis of the elderLUCID audio corpus – a set of speech recordings collected for 83 adults aged 19 to 84 years inclusive. Recordings were made while participants carried out two types of collaborative tasks with a conversational partner who was a young adult of the same sex: (1) a ‘spot the difference’ picture task (‘diapix’) where the conversational partners had to collaborate to find 12 differences between their pictures and (2) a BKB sentence repetition task where the key participant had to read a set of sentences to their partner who had to repeat them back. The two tasks were carried out by each participant pair in four different conditions: (1) in good listening conditions when both could hear each other normally (NORM condition), or when perception was impaired for one or both of the participants by (2) simulating a severe-to-profound hearing loss in the conversational partner (HLS condition), (3) adding multispeaker babble noise to the audio channel for the conversational partner (BAB1 condition) or to the audio channel of both participants (BAB2 condition). The aim of the study was to examine the clarification strategies used by older adults and young adult controls to maintain effective communication in adverse communicative conditions. The SPSS spreadsheet contains, for each of the 83 participants, quantitative data resulting from (a) the acoustic analysis of the recordings, (b) measures of communication efficiency and (c) background auditory and cognitive measures.  Speech communication can be difficult for older people, due to the combined effects of age-related hearing loss, which is common over the age of 65, age-related decline in the quality of phonation and speech articulation, and cognitive problems such as poorer short-term memory and processing speed. Past studies of how older individuals perceive and produce speech sounds have tended to consider these abilities independently of each other using controlled materials, such as read words or sentences. These studies tell us little about how older speakers function when using speech for communicative purposes, and how these various factors interact. For example, it has been shown that older people benefit from seeing their interlocutor in conversations, but audiovisual speech places greater demands on cognitive processing than auditory speech which leads to increased listener effort and reduced information recall. 
In our project, we propose to gain a comprehensive account of older people's speech production and perception in situations involving communication with another individual. Adults with age-related hearing loss and the rarer group of older adults with normal hearing will be included as well as younger adult controls. In Study 1, communication with another speaker, while reading sentences or completing a problem-solving task, will either be in good listening conditions, where both speakers hear each other normally, or in adverse conditions, where the participant has to get their message across to another speaker who has a simulated hearing loss or when both are speaking in a noisy background. These comparisons will enable us to get a sense of the degree to which an older person is able to adapt their speech to overcome difficult listening conditions, a skill which is of paramount importance in speech communication in everyday life. We will obtain high-quality digital recordings of the participants' speech but will also, via sensors placed on the neck, record information about their vocal fold vibration, which determines the quality of their voice. Video recordings will also be analysed to investigate whether older speakers make use of eye gaze and head gestures to signal aspects of discourse such as turn-taking and back-channelling (e.g., saying 'okay' to signal understanding), to the same degree as younger speakers. In Study 2, older and younger listeners with normal and impaired hearing will be presented some of the sentence materials recorded in Study 1 by all speaker groups in good and adverse listening conditions. Tests will be presented in both auditory-alone and audiovisual conditions. Intelligibility tests will be run to see what impact age, hearing status and visual cues have on speech understanding and to see whether the 'clear speech' adaptations made by older speakers to counter the effects of poor communication conditions gives the same benefit to that of younger speakers. Sentence recall tests will also be run to investigate whether the listening effort is reduced listening to 'clear speech'.e:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"elderLUCID: London UCL Older adults' clear speech in interaction database"}],["$","meta","2",{"name":"description","content":"$16"}],["$","meta","3",{"property":"og:title","content":"elderLUCID: London UCL Older adults' clear speech in interaction database"}],["$","meta","4",{"property":"og:description","content":"$17"}],["$","meta","5",{"property":"og:url","content":"https://harmonydata.ac.uk/search/items/elderlucid-london-ucl-older-adults-clear-speech-in-interaction-database"}],["$","meta","6",{"property":"og:site_name","content":"Academic Resource Discovery"}],["$","meta","7",{"property":"og:locale","content":"en_US"}],["$","meta","8",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","meta","9",{"property":"og:image:width","content":"1200"}],["$","meta","10",{"property":"og:image:height","content":"630"}],["$","meta","11",{"property":"og:image:alt","content":"elderLUCID: London UCL Older adults' clear speech in interaction database"}],["$","meta","12",{"property":"og:type","content":"website"}],["$","meta","13",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","14",{"name":"twitter:title","content":"elderLUCID: London UCL Older adults' clear speech in interaction database"}],["$","meta","15",{"name":"twitter:description","content":"$18"}],["$","meta","16",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","link","17",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
c:null
