1:"$Sreact.fragment"
2:I[82104,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
3:I[10683,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"AuthProvider"]
4:I[63612,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"SearchProvider"]
5:I[68998,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
6:I[98904,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
7:I[15244,[],""]
8:I[43866,[],""]
9:I[14046,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"ToastContainer"]
b:I[86213,[],"OutletBoundary"]
d:I[86213,[],"MetadataBoundary"]
f:I[86213,[],"ViewportBoundary"]
11:I[34835,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/2c4d913f25bfc6bf.css","style"]
:HL["/search/_next/static/css/4921cfd18b262f8c.css","style"]
0:{"P":null,"b":"8r-g2-FTTcZL6JFFobnJN","p":"/search","c":["","items","fef489278ddabb8b03b4aca0f6a58cd2"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","fef489278ddabb8b03b4aca0f6a58cd2","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/2c4d913f25bfc6bf.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":["$","meta",null,{"name":"emotion-insertion-point","content":""}]}],["$","body",null,{"className":"__className_62a302","children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":[["$","$L5",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L6",null,{}],["$","$L5",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$L9",null,{"position":"bottom-right"}]]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","fef489278ddabb8b03b4aca0f6a58cd2","d"],["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$La",[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/4921cfd18b262f8c.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$Lb",null,{"children":"$Lc"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","N4NhGvBvNzPjZFUqBmnJ1",{"children":[["$","$Ld",null,{"children":"$Le"}],["$","$Lf",null,{"children":"$L10"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
10:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:["$","div",null,{"children":[["$","script",null,{"dangerouslySetInnerHTML":{"__html":"window.location.replace('/items/a-neuropsychological-approach-to-dissect-face-perception-and-perceptual-expertise-fmri-data');"}}],["$","p",null,{"children":["Redirecting to"," ",["$","a",null,{"href":"/items/a-neuropsychological-approach-to-dissect-face-perception-and-perceptual-expertise-fmri-data","children":["/items/","a-neuropsychological-approach-to-dissect-face-perception-and-perceptual-expertise-fmri-data"]}],"..."]}]]}]
12:T9ee,This collection contains the raw functional data for brain responses to faces and novel objects. In this phase, we adapted the FPVS paradigm used in the EEG study to the FMRI study. Subjects performed a fixation task during FMRI data collection. We also acquired structural and diffusion imaging data. 

The data for this study are organised into four collections. The first contains the visual stimuli used throughout the study. These includes faces and novel three-dimensional (3D) objects rendered from different viewpoints. This collection also contains the 3D model and Matlab scripts to help create more stimuli. The second contains the response time and accuracy data during the training sessions and during the pre-training and post-training test sessions. We used an inversion task for the test sessions. During all sessions, subjects were responding to faces or novel objects. The last collection contains the processed EEG responses to faces and novel objects during the pre-training and post-training test sessions. We used the fast periodic visual stimulation (FPVS) paradigm. Subjects performed a fixation task during EEG data collection. (Find the other collections under 'Related Resources')Recognising faces is at the heart of human social interactions. By adulthood, people are very good at extracting identity, sex, race, emotions, and social signals from faces. Therefore, impairments to this ability can drastically reduce their quality of life.

The aim of this project is to investigate the neural mechanisms underlying people’s ability to process faces and how these mechanisms adapt with experience. The approach is to test whether individuals with prosopagnosia can acquire expertise of novel non-face objects through training. These individuals had head trauma during adulthood that lead to damage in specific brain regions. These regions are thought to process only faces and no other object categories. However, these regions may be more generally involved in processing object categories for which people have expertise (eg, bird experts). In addition to neurological case studies, volunteers will also go through the training. Their brain will be scanned using magnetic resonance imaging to determine how the putative face-specific regions change over the course of training. Overall, the results will have an impact on clinical populations which can result in face recognition deficits, such as Alzheimer’s disease, stroke patients, and developmental disorders that affect social interactions (eg, Autism).13:T9ee,This collection contains the raw functional data for brain responses to faces and novel objects. In this phase, we adapted the FPVS paradigm used in the EEG study to the FMRI study. Subjects performed a fixation task during FMRI data collection. We also acquired structural and diffusion imaging data. 

The data for this study are organised into four collections. The first contains the visual stimuli used throughout the study. These includes faces and novel three-dimensional (3D) objects rendered from different viewpoints. This collection also contains the 3D model and Matlab scripts to help create more stimuli. The second contains the response time and accuracy data during the training sessions and during the pre-training and post-training test sessions. We used an inversion task for the test sessions. During all sessions, subjects were responding to faces or novel objects. The last collection contains the processed EEG responses to faces and novel objects during the pre-training and post-training test sessions. We used the fast periodic visual stimulation (FPVS) paradigm. Subjects performed a fixation task during EEG data collection. (Find the other collections under 'Related Resources')Recognising faces is at the heart of human social interactions. By adulthood, people are very good at extracting identity, sex, race, emotions, and social signals from faces. Therefore, impairments to this ability can drastically reduce their quality of life.

The aim of this project is to investigate the neural mechanisms underlying people’s ability to process faces and how these mechanisms adapt with experience. The approach is to test whether individuals with prosopagnosia can acquire expertise of novel non-face objects through training. These individuals had head trauma during adulthood that lead to damage in specific brain regions. These regions are thought to process only faces and no other object categories. However, these regions may be more generally involved in processing object categories for which people have expertise (eg, bird experts). In addition to neurological case studies, volunteers will also go through the training. Their brain will be scanned using magnetic resonance imaging to determine how the putative face-specific regions change over the course of training. Overall, the results will have an impact on clinical populations which can result in face recognition deficits, such as Alzheimer’s disease, stroke patients, and developmental disorders that affect social interactions (eg, Autism).14:T9ee,This collection contains the raw functional data for brain responses to faces and novel objects. In this phase, we adapted the FPVS paradigm used in the EEG study to the FMRI study. Subjects performed a fixation task during FMRI data collection. We also acquired structural and diffusion imaging data. 

The data for this study are organised into four collections. The first contains the visual stimuli used throughout the study. These includes faces and novel three-dimensional (3D) objects rendered from different viewpoints. This collection also contains the 3D model and Matlab scripts to help create more stimuli. The second contains the response time and accuracy data during the training sessions and during the pre-training and post-training test sessions. We used an inversion task for the test sessions. During all sessions, subjects were responding to faces or novel objects. The last collection contains the processed EEG responses to faces and novel objects during the pre-training and post-training test sessions. We used the fast periodic visual stimulation (FPVS) paradigm. Subjects performed a fixation task during EEG data collection. (Find the other collections under 'Related Resources')Recognising faces is at the heart of human social interactions. By adulthood, people are very good at extracting identity, sex, race, emotions, and social signals from faces. Therefore, impairments to this ability can drastically reduce their quality of life.

The aim of this project is to investigate the neural mechanisms underlying people’s ability to process faces and how these mechanisms adapt with experience. The approach is to test whether individuals with prosopagnosia can acquire expertise of novel non-face objects through training. These individuals had head trauma during adulthood that lead to damage in specific brain regions. These regions are thought to process only faces and no other object categories. However, these regions may be more generally involved in processing object categories for which people have expertise (eg, bird experts). In addition to neurological case studies, volunteers will also go through the training. Their brain will be scanned using magnetic resonance imaging to determine how the putative face-specific regions change over the course of training. Overall, the results will have an impact on clinical populations which can result in face recognition deficits, such as Alzheimer’s disease, stroke patients, and developmental disorders that affect social interactions (eg, Autism).e:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"A neuropsychological approach to dissect face perception and perceptual expertise: FMRI data"}],["$","meta","2",{"name":"description","content":"$12"}],["$","meta","3",{"property":"og:title","content":"A neuropsychological approach to dissect face perception and perceptual expertise: FMRI data"}],["$","meta","4",{"property":"og:description","content":"$13"}],["$","meta","5",{"property":"og:url","content":"https://discoverynext.vercel.app/items/fef489278ddabb8b03b4aca0f6a58cd2"}],["$","meta","6",{"property":"og:site_name","content":"Academic Resource Discovery"}],["$","meta","7",{"property":"og:locale","content":"en_US"}],["$","meta","8",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","meta","9",{"property":"og:image:width","content":"1200"}],["$","meta","10",{"property":"og:image:height","content":"630"}],["$","meta","11",{"property":"og:image:alt","content":"A neuropsychological approach to dissect face perception and perceptual expertise: FMRI data"}],["$","meta","12",{"property":"og:type","content":"website"}],["$","meta","13",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","14",{"name":"twitter:title","content":"A neuropsychological approach to dissect face perception and perceptual expertise: FMRI data"}],["$","meta","15",{"name":"twitter:description","content":"$14"}],["$","meta","16",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","link","17",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
c:null
