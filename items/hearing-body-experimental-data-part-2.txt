1:"$Sreact.fragment"
2:I[82104,["6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","9756","static/js/9756.65c7d9ea.js","5246","static/js/5246.d88343e0.js","7177","static/js/app/layout.2233f7cc.js"],"default"]
3:I[17146,["6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","9756","static/js/9756.65c7d9ea.js","5246","static/js/5246.d88343e0.js","7177","static/js/app/layout.2233f7cc.js"],"AuthProvider"]
4:I[63612,["6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","9756","static/js/9756.65c7d9ea.js","5246","static/js/5246.d88343e0.js","7177","static/js/app/layout.2233f7cc.js"],"SearchProvider"]
5:I[68998,["6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","9756","static/js/9756.65c7d9ea.js","5246","static/js/5246.d88343e0.js","7177","static/js/app/layout.2233f7cc.js"],"default"]
6:I[98904,["6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","9756","static/js/9756.65c7d9ea.js","5246","static/js/5246.d88343e0.js","7177","static/js/app/layout.2233f7cc.js"],"default"]
7:I[15244,[],""]
8:I[43866,[],""]
9:I[14046,["6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","9756","static/js/9756.65c7d9ea.js","5246","static/js/5246.d88343e0.js","7177","static/js/app/layout.2233f7cc.js"],"ToastContainer"]
b:I[86213,[],"OutletBoundary"]
d:I[86213,[],"MetadataBoundary"]
f:I[86213,[],"ViewportBoundary"]
11:I[34835,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/a38392bd344718e4.css","style"]
:HL["/search/_next/static/css/4921cfd18b262f8c.css","style"]
0:{"P":null,"b":"0Pw2B2A-ctu_hDkpgRDyX","p":"/search","c":["","items","hearing-body-experimental-data-part-2"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","hearing-body-experimental-data-part-2","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/a38392bd344718e4.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":["$","meta",null,{"name":"emotion-insertion-point","content":""}]}],["$","body",null,{"className":"__className_55b4bf","children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":[["$","$L5",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L6",null,{}],["$","$L5",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$L9",null,{"position":"bottom-right"}]]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","hearing-body-experimental-data-part-2","d"],["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$La",[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/4921cfd18b262f8c.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$Lb",null,{"children":"$Lc"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","qwLaKFKYuvddld91WZemh",{"children":[["$","$Ld",null,{"children":"$Le"}],["$","$Lf",null,{"children":"$L10"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
12:I[53704,["3524","static/js/2170a4aa.3678665e.js","6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","1057","static/js/1057.fef3cc4c.js","2282","static/js/2282.e20001b9.js","9234","static/js/9234.7cf96505.js","2926","static/js/2926.76e4f620.js","7511","static/js/7511.a52b23cb.js","8173","static/js/8173.582c8c90.js","613","static/js/613.da2777c4.js","9756","static/js/9756.65c7d9ea.js","97","static/js/97.c5459b6e.js","2649","static/js/2649.4d01838c.js","1857","static/js/1857.a01744c0.js","280","static/js/280.07f8eb1b.js","9123","static/js/9123.0b9c8079.js","6387","static/js/app/items/%5Bslug%5D/page.e8e51342.js"],""]
14:I[5749,["3524","static/js/2170a4aa.3678665e.js","6586","static/js/6586.2e946dbf.js","4889","static/js/4889.4efc83ef.js","1057","static/js/1057.fef3cc4c.js","2282","static/js/2282.e20001b9.js","9234","static/js/9234.7cf96505.js","2926","static/js/2926.76e4f620.js","7511","static/js/7511.a52b23cb.js","8173","static/js/8173.582c8c90.js","613","static/js/613.da2777c4.js","9756","static/js/9756.65c7d9ea.js","97","static/js/97.c5459b6e.js","2649","static/js/2649.4d01838c.js","1857","static/js/1857.a01744c0.js","280","static/js/280.07f8eb1b.js","9123","static/js/9123.0b9c8079.js","6387","static/js/app/items/%5Bslug%5D/page.e8e51342.js"],"default"]
13:Tceb,{"@context":"https://schema.org/","@type":"Dataset","name":"The hearing body: Experimental data, Part 2","description":"Here we present data that include subjective reports and behavioural data corresponding to finger touch behaviour from individuals interacting with a sonic interactive surface. We designed and tested a prototype that dynamically alters the texture-related sound feedback based on touch behavior, as in natural surface touch interactions. Data from this user study showed that the frequency of the sound feedback alters texture perception (coldness and material type) and touch behavior (velocity and pressure).\nThe data in this collection are part of The Hearing Body project, a project investigating how the manipulation of action sounds may alter the mental representation of one's body and the related emotional state and body behaviour. Other data collections part of The Hearing Body project have been deposited (Please see Related Resources section below). All 4 parts are experimental data, but they are data from different studies. Part 1 and 2 contain subjective reports and behavioural data, and Part 3 and 4 contain subjective reports, behavioural data and data on electrodermal activity changes. \nMore information on the system and measures used can be found in the related paper:  Tajadura-Jiménez, A., Liu, B., Berthouze, N., Bevilacqua, F. (2014) Using sound in multi-touch interfaces to change materiality and touch behavior. Proceedings of the 8th Nordic Conference on Human-Computer Interaction: Fun, Fast, Foundational, Pages 199-202, ACM (see Related resources).\nThe mental representation we have of our body is essential for successful interaction with the environment. This representation is not fixed, but is continuously updated in response to the available sensory information. While previous studies have highlighted the role of vision, touch and proprioception in constructing the body-representation in the brain, the role of auditory information remains largely unknown. Interestingly, the sounds that accompany almost every bodily movement are highly rich in information about the body and the space immediately surrounding it. For instance, the sounds produced when tapping on a surface inform us about the length and strength of our arm.\n\nThis project will investigate how auditory information generated by our bodies updates our body-representation. A series of psychological experiments will explore how altering self-produced sounds in real-time changes different body-representations, including the representation of the space surrounding the body, the potential actions that we can perform and the emotional states linked to our body capabilities. This multidisciplinary and innovative research project will provide novel insights into the nature of body-representations and, ultimately, guide the design of audio-based applications that can improve body-image, self-esteem, movement patterns and social interactions to support wellbeing and rehabilitation for people with movement impairments.","url":"https://harmonydata.ac.uk/search/items/hearing-body-experimental-data-part-2","identifier":["http://dx.doi.org/10.5255/UKDA-SN-852247"],"keywords":["SENSORY SYSTEM","PERCEPTION","SOUND RECORDINGS"],"temporalCoverage":"2012-11-01/2015-12-31"}15:Tb74,Here we present data that include subjective reports and behavioural data corresponding to finger touch behaviour from individuals interacting with a sonic interactive surface. We designed and tested a prototype that dynamically alters the texture-related sound feedback based on touch behavior, as in natural surface touch interactions. Data from this user study showed that the frequency of the sound feedback alters texture perception (coldness and material type) and touch behavior (velocity and pressure).
The data in this collection are part of The Hearing Body project, a project investigating how the manipulation of action sounds may alter the mental representation of one's body and the related emotional state and body behaviour. Other data collections part of The Hearing Body project have been deposited (Please see Related Resources section below). All 4 parts are experimental data, but they are data from different studies. Part 1 and 2 contain subjective reports and behavioural data, and Part 3 and 4 contain subjective reports, behavioural data and data on electrodermal activity changes. 
More information on the system and measures used can be found in the related paper:  Tajadura-Jiménez, A., Liu, B., Berthouze, N., Bevilacqua, F. (2014) Using sound in multi-touch interfaces to change materiality and touch behavior. Proceedings of the 8th Nordic Conference on Human-Computer Interaction: Fun, Fast, Foundational, Pages 199-202, ACM (see Related resources).
The mental representation we have of our body is essential for successful interaction with the environment. This representation is not fixed, but is continuously updated in response to the available sensory information. While previous studies have highlighted the role of vision, touch and proprioception in constructing the body-representation in the brain, the role of auditory information remains largely unknown. Interestingly, the sounds that accompany almost every bodily movement are highly rich in information about the body and the space immediately surrounding it. For instance, the sounds produced when tapping on a surface inform us about the length and strength of our arm.

This project will investigate how auditory information generated by our bodies updates our body-representation. A series of psychological experiments will explore how altering self-produced sounds in real-time changes different body-representations, including the representation of the space surrounding the body, the potential actions that we can perform and the emotional states linked to our body capabilities. This multidisciplinary and innovative research project will provide novel insights into the nature of body-representations and, ultimately, guide the design of audio-based applications that can improve body-image, self-esteem, movement patterns and social interactions to support wellbeing and rehabilitation for people with movement impairments.a:[["$","$L12",null,{"strategy":"beforeInteractive","id":"structured-data","type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$13"}}],["$","$L14",null,{"dataset":{"title":"The hearing body: Experimental data, Part 2","description":"$15","image":"$undefined","publisher":"$undefined","funders":"$undefined","geographicCoverage":"GB","temporalCoverage":"2012-11-01/2015-12-31","ageCoverage":"$undefined","studyDesign":[],"resourceType":"dataset","topics":["SENSORY SYSTEM","PERCEPTION","SOUND RECORDINGS"],"instruments":[],"dataCatalogs":[{"name":"UK Data Service","url":"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=852247","logo":"$undefined"}],"matchedVariables":[],"allVariables":[],"additionalLinks":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=852247","https://reshare.ukdataservice.ac.uk/852247","http://dx.doi.org/10.5255/UKDA-SN-852247","http://dx.doi.org/10.5255/UKDA-SN-852247"],"child_datasets":[],"aiSummary":null}}]]
10:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
16:Tb74,Here we present data that include subjective reports and behavioural data corresponding to finger touch behaviour from individuals interacting with a sonic interactive surface. We designed and tested a prototype that dynamically alters the texture-related sound feedback based on touch behavior, as in natural surface touch interactions. Data from this user study showed that the frequency of the sound feedback alters texture perception (coldness and material type) and touch behavior (velocity and pressure).
The data in this collection are part of The Hearing Body project, a project investigating how the manipulation of action sounds may alter the mental representation of one's body and the related emotional state and body behaviour. Other data collections part of The Hearing Body project have been deposited (Please see Related Resources section below). All 4 parts are experimental data, but they are data from different studies. Part 1 and 2 contain subjective reports and behavioural data, and Part 3 and 4 contain subjective reports, behavioural data and data on electrodermal activity changes. 
More information on the system and measures used can be found in the related paper:  Tajadura-Jiménez, A., Liu, B., Berthouze, N., Bevilacqua, F. (2014) Using sound in multi-touch interfaces to change materiality and touch behavior. Proceedings of the 8th Nordic Conference on Human-Computer Interaction: Fun, Fast, Foundational, Pages 199-202, ACM (see Related resources).
The mental representation we have of our body is essential for successful interaction with the environment. This representation is not fixed, but is continuously updated in response to the available sensory information. While previous studies have highlighted the role of vision, touch and proprioception in constructing the body-representation in the brain, the role of auditory information remains largely unknown. Interestingly, the sounds that accompany almost every bodily movement are highly rich in information about the body and the space immediately surrounding it. For instance, the sounds produced when tapping on a surface inform us about the length and strength of our arm.

This project will investigate how auditory information generated by our bodies updates our body-representation. A series of psychological experiments will explore how altering self-produced sounds in real-time changes different body-representations, including the representation of the space surrounding the body, the potential actions that we can perform and the emotional states linked to our body capabilities. This multidisciplinary and innovative research project will provide novel insights into the nature of body-representations and, ultimately, guide the design of audio-based applications that can improve body-image, self-esteem, movement patterns and social interactions to support wellbeing and rehabilitation for people with movement impairments.17:Tb74,Here we present data that include subjective reports and behavioural data corresponding to finger touch behaviour from individuals interacting with a sonic interactive surface. We designed and tested a prototype that dynamically alters the texture-related sound feedback based on touch behavior, as in natural surface touch interactions. Data from this user study showed that the frequency of the sound feedback alters texture perception (coldness and material type) and touch behavior (velocity and pressure).
The data in this collection are part of The Hearing Body project, a project investigating how the manipulation of action sounds may alter the mental representation of one's body and the related emotional state and body behaviour. Other data collections part of The Hearing Body project have been deposited (Please see Related Resources section below). All 4 parts are experimental data, but they are data from different studies. Part 1 and 2 contain subjective reports and behavioural data, and Part 3 and 4 contain subjective reports, behavioural data and data on electrodermal activity changes. 
More information on the system and measures used can be found in the related paper:  Tajadura-Jiménez, A., Liu, B., Berthouze, N., Bevilacqua, F. (2014) Using sound in multi-touch interfaces to change materiality and touch behavior. Proceedings of the 8th Nordic Conference on Human-Computer Interaction: Fun, Fast, Foundational, Pages 199-202, ACM (see Related resources).
The mental representation we have of our body is essential for successful interaction with the environment. This representation is not fixed, but is continuously updated in response to the available sensory information. While previous studies have highlighted the role of vision, touch and proprioception in constructing the body-representation in the brain, the role of auditory information remains largely unknown. Interestingly, the sounds that accompany almost every bodily movement are highly rich in information about the body and the space immediately surrounding it. For instance, the sounds produced when tapping on a surface inform us about the length and strength of our arm.

This project will investigate how auditory information generated by our bodies updates our body-representation. A series of psychological experiments will explore how altering self-produced sounds in real-time changes different body-representations, including the representation of the space surrounding the body, the potential actions that we can perform and the emotional states linked to our body capabilities. This multidisciplinary and innovative research project will provide novel insights into the nature of body-representations and, ultimately, guide the design of audio-based applications that can improve body-image, self-esteem, movement patterns and social interactions to support wellbeing and rehabilitation for people with movement impairments.18:Tb74,Here we present data that include subjective reports and behavioural data corresponding to finger touch behaviour from individuals interacting with a sonic interactive surface. We designed and tested a prototype that dynamically alters the texture-related sound feedback based on touch behavior, as in natural surface touch interactions. Data from this user study showed that the frequency of the sound feedback alters texture perception (coldness and material type) and touch behavior (velocity and pressure).
The data in this collection are part of The Hearing Body project, a project investigating how the manipulation of action sounds may alter the mental representation of one's body and the related emotional state and body behaviour. Other data collections part of The Hearing Body project have been deposited (Please see Related Resources section below). All 4 parts are experimental data, but they are data from different studies. Part 1 and 2 contain subjective reports and behavioural data, and Part 3 and 4 contain subjective reports, behavioural data and data on electrodermal activity changes. 
More information on the system and measures used can be found in the related paper:  Tajadura-Jiménez, A., Liu, B., Berthouze, N., Bevilacqua, F. (2014) Using sound in multi-touch interfaces to change materiality and touch behavior. Proceedings of the 8th Nordic Conference on Human-Computer Interaction: Fun, Fast, Foundational, Pages 199-202, ACM (see Related resources).
The mental representation we have of our body is essential for successful interaction with the environment. This representation is not fixed, but is continuously updated in response to the available sensory information. While previous studies have highlighted the role of vision, touch and proprioception in constructing the body-representation in the brain, the role of auditory information remains largely unknown. Interestingly, the sounds that accompany almost every bodily movement are highly rich in information about the body and the space immediately surrounding it. For instance, the sounds produced when tapping on a surface inform us about the length and strength of our arm.

This project will investigate how auditory information generated by our bodies updates our body-representation. A series of psychological experiments will explore how altering self-produced sounds in real-time changes different body-representations, including the representation of the space surrounding the body, the potential actions that we can perform and the emotional states linked to our body capabilities. This multidisciplinary and innovative research project will provide novel insights into the nature of body-representations and, ultimately, guide the design of audio-based applications that can improve body-image, self-esteem, movement patterns and social interactions to support wellbeing and rehabilitation for people with movement impairments.e:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"The hearing body: Experimental data, Part 2"}],["$","meta","2",{"name":"description","content":"$16"}],["$","meta","3",{"property":"og:title","content":"The hearing body: Experimental data, Part 2"}],["$","meta","4",{"property":"og:description","content":"$17"}],["$","meta","5",{"property":"og:url","content":"https://harmonydata.ac.uk/search/items/hearing-body-experimental-data-part-2"}],["$","meta","6",{"property":"og:site_name","content":"Academic Resource Discovery"}],["$","meta","7",{"property":"og:locale","content":"en_US"}],["$","meta","8",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","meta","9",{"property":"og:image:width","content":"1200"}],["$","meta","10",{"property":"og:image:height","content":"630"}],["$","meta","11",{"property":"og:image:alt","content":"The hearing body: Experimental data, Part 2"}],["$","meta","12",{"property":"og:type","content":"website"}],["$","meta","13",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","14",{"name":"twitter:title","content":"The hearing body: Experimental data, Part 2"}],["$","meta","15",{"name":"twitter:description","content":"$18"}],["$","meta","16",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","link","17",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
c:null
