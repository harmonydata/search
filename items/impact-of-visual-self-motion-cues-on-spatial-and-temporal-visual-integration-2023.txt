1:"$Sreact.fragment"
2:I[82104,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","7177","static/js/app/layout.e50c3fe1.js"],"default"]
3:I[17146,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","7177","static/js/app/layout.e50c3fe1.js"],"AuthProvider"]
4:I[63612,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","7177","static/js/app/layout.e50c3fe1.js"],"SearchProvider"]
5:I[68998,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","7177","static/js/app/layout.e50c3fe1.js"],"default"]
6:I[98904,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","7177","static/js/app/layout.e50c3fe1.js"],"default"]
7:I[15244,[],""]
8:I[43866,[],""]
9:I[14046,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","7177","static/js/app/layout.e50c3fe1.js"],"ToastContainer"]
b:I[86213,[],"OutletBoundary"]
d:I[86213,[],"MetadataBoundary"]
f:I[86213,[],"ViewportBoundary"]
11:I[34835,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/0d5b820fee8240e5.css","style"]
0:{"P":null,"b":"5NL-CI0Ox-nPUvNZv6Vu8","p":"/search","c":["","items","impact-of-visual-self-motion-cues-on-spatial-and-temporal-visual-integration-2023"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","impact-of-visual-self-motion-cues-on-spatial-and-temporal-visual-integration-2023","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/0d5b820fee8240e5.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":[["$","meta",null,{"name":"emotion-insertion-point","content":""}],["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"preconnect","href":"https://www.cataloguementalhealth.ac.uk"}],["$","link",null,{"rel":"dns-prefetch","href":"https://harmonydata.ac.uk"}],["$","style",null,{"dangerouslySetInnerHTML":{"__html":"\n            /* Ensure immediate rendering with Roboto and fallbacks */\n            * { \n              font-family: \"Roboto\", -apple-system, BlinkMacSystemFont, \"Segoe UI\", \"Oxygen\", \"Ubuntu\", \"Cantarell\", \"Fira Sans\", \"Droid Sans\", \"Helvetica Neue\", sans-serif !important;\n              font-display: swap;\n              -webkit-font-smoothing: antialiased;\n              -moz-osx-font-smoothing: grayscale;\n            }\n            body { \n              visibility: visible !important; \n              opacity: 1 !important; \n              margin: 0; \n              padding: 0; \n            }\n          "}}]]}],["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":[["$","$L5",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L6",null,{}],["$","$L5",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$L9",null,{"position":"bottom-right"}]]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","impact-of-visual-self-motion-cues-on-spatial-and-temporal-visual-integration-2023","d"],["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$La",null,["$","$Lb",null,{"children":"$Lc"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","S2362ZEyj8arSWPS5K1Ew",{"children":[["$","$Ld",null,{"children":"$Le"}],["$","$Lf",null,{"children":"$L10"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
12:I[53704,["6586","static/js/6586.2e946dbf.js","8378","static/js/8378.a1bea36e.js","2282","static/js/2282.e20001b9.js","5135","static/js/5135.b8bfc30e.js","9387","static/js/9387.65629b75.js","2649","static/js/2649.4d01838c.js","1857","static/js/1857.a01744c0.js","280","static/js/280.5152a9e2.js","7626","static/js/7626.f9409ee1.js","6387","static/js/app/items/%5Bslug%5D/page.4934bfd6.js"],""]
14:I[77626,["6586","static/js/6586.2e946dbf.js","8378","static/js/8378.a1bea36e.js","2282","static/js/2282.e20001b9.js","5135","static/js/5135.b8bfc30e.js","9387","static/js/9387.65629b75.js","2649","static/js/2649.4d01838c.js","1857","static/js/1857.a01744c0.js","280","static/js/280.5152a9e2.js","7626","static/js/7626.f9409ee1.js","6387","static/js/app/items/%5Bslug%5D/page.4934bfd6.js"],"default"]
13:T1256,{"@context":"https://schema.org/","@type":"Dataset","name":"Impact of Visual Self-Motion Cues on Spatial and Temporal Visual Integration, 2023","description":"We present two psychophysics experiments investigating the impact of visual self-motion cues on visual spatiotemporal summation.  \nThe first experiment is a 4AFC target detection task where participants give the location of a target that appeared in one of four known locations, against a moving background.  Each target is composed of two small probes which are separated by four diagonal pixels.  Probes always appear for one frame (8.33ms).  The probes are either presented concurrently (at the same time) or are separated by one of six interstimulus-intervals (ISIs) of 0 to 100ms.  The two probes are aligned radially, such that the second probe is either inward (closer to the centre of the screen) or outward than the first probe.  The background is an array of ~1000 dots which move in an expanding or contracting pattern to simulate forwards or backwards self-motion.  The main manipulation is the relationship between the alignment of the probes (inward or outward) and the background motion (expanding or contracting).  The relationship can either be congruent (both in same direction) or incongruent.  Staircases adjust the target luminance for each condition to identify the threshold where participants have 75% accuracy.  \nA second study has the same design but with targets appearing in three of the four known locations and participants identifying the location which did NOT contain a target.\nFinally a flow-parsing study was conducted to validate that our background stimulus was eliciting self-motion cues.  In this study participants saw a single moving target in one of four locations and indicated whether it was moving inwards or outwards.  A staircase procedure adjusted the target speed to identify the speed where participant gave an ‘inward’ response 50% of the time.\nThis version contains the full 12 iterations of the three main tasks and the figures and aggregate data files have been updated.Every time we move, the image of the world at the back of the eye changes. Despite this, our perception is of an unchanging world. How does the brain translate a continually changing image into a percept of a stable, stationary, rigid world? Does the brain use a map of the external environment (an \"allocentric map\") and the position of the observer within it, built up over time, to underpin the perception of stability? Does the brain continually update a map of where scene objects are relative to the observer (an \"egocentric map\"; e.g. there is an object straight ahead of me, if I walk forward I should expect it to get closer to me)? Does the brain not create a map but just divide up the image motion into that which is likely due to movement of the observer (and which can consequently be ignored) and that which is likely due to objects moving within the scene (which become a focus of attention)? The hypothesis that underpins this research project is that it is not a single one of these mechanisms that underpins perceptual stability, but that all of them, their contribution dependent on the task being performed by the observer. In some cases the task will require a fast estimate to support an ongoing action which might favour one mechanism, on another task, where timing is not so critical, a slower, but more accurate, mechanism might be more appropriate. This collaborative project, which combines complementary expertise in Psychology, Movement Sciences, and Computing from Germany, The Netherlands and the United Kingdom, and importantly, researchers that start from different theoretical perspectives, will test this hypothesis. We will study a diverse series of tasks that present a range of challenges to the moving observer. We will make use of various innovative experimental paradigms that exploit recent technological advances such as virtual reality combined with simultaneous motion tracking. Understanding where and how different mechanisms of perceptual stability play a role advances not only our scientific understanding, but also has the potential to inform industry as well as medicine about the circumstances in which disorientation or nausea in real or virtual environments can be minimised.","url":"https://harmonydata.ac.uk/search/items/impact-of-visual-self-motion-cues-on-spatial-and-temporal-visual-integration-2023","identifier":["http://dx.doi.org/10.5255/UKDA-SN-856908","http://dx.doi.org/10.5255/UKDA-SN-856941"],"keywords":["PSYCHOLOGICAL EFFECTS","PSYCHOLOGY","PSYCHOLOGICAL RESEARCH","HUMAN BEHAVIOUR"],"temporalCoverage":"2023-11-21/2024-01-16"}15:T103f,We present two psychophysics experiments investigating the impact of visual self-motion cues on visual spatiotemporal summation.  
The first experiment is a 4AFC target detection task where participants give the location of a target that appeared in one of four known locations, against a moving background.  Each target is composed of two small probes which are separated by four diagonal pixels.  Probes always appear for one frame (8.33ms).  The probes are either presented concurrently (at the same time) or are separated by one of six interstimulus-intervals (ISIs) of 0 to 100ms.  The two probes are aligned radially, such that the second probe is either inward (closer to the centre of the screen) or outward than the first probe.  The background is an array of ~1000 dots which move in an expanding or contracting pattern to simulate forwards or backwards self-motion.  The main manipulation is the relationship between the alignment of the probes (inward or outward) and the background motion (expanding or contracting).  The relationship can either be congruent (both in same direction) or incongruent.  Staircases adjust the target luminance for each condition to identify the threshold where participants have 75% accuracy.  
A second study has the same design but with targets appearing in three of the four known locations and participants identifying the location which did NOT contain a target.
Finally a flow-parsing study was conducted to validate that our background stimulus was eliciting self-motion cues.  In this study participants saw a single moving target in one of four locations and indicated whether it was moving inwards or outwards.  A staircase procedure adjusted the target speed to identify the speed where participant gave an ‘inward’ response 50% of the time.
This version contains the full 12 iterations of the three main tasks and the figures and aggregate data files have been updated.Every time we move, the image of the world at the back of the eye changes. Despite this, our perception is of an unchanging world. How does the brain translate a continually changing image into a percept of a stable, stationary, rigid world? Does the brain use a map of the external environment (an "allocentric map") and the position of the observer within it, built up over time, to underpin the perception of stability? Does the brain continually update a map of where scene objects are relative to the observer (an "egocentric map"; e.g. there is an object straight ahead of me, if I walk forward I should expect it to get closer to me)? Does the brain not create a map but just divide up the image motion into that which is likely due to movement of the observer (and which can consequently be ignored) and that which is likely due to objects moving within the scene (which become a focus of attention)? The hypothesis that underpins this research project is that it is not a single one of these mechanisms that underpins perceptual stability, but that all of them, their contribution dependent on the task being performed by the observer. In some cases the task will require a fast estimate to support an ongoing action which might favour one mechanism, on another task, where timing is not so critical, a slower, but more accurate, mechanism might be more appropriate. This collaborative project, which combines complementary expertise in Psychology, Movement Sciences, and Computing from Germany, The Netherlands and the United Kingdom, and importantly, researchers that start from different theoretical perspectives, will test this hypothesis. We will study a diverse series of tasks that present a range of challenges to the moving observer. We will make use of various innovative experimental paradigms that exploit recent technological advances such as virtual reality combined with simultaneous motion tracking. Understanding where and how different mechanisms of perceptual stability play a role advances not only our scientific understanding, but also has the potential to inform industry as well as medicine about the circumstances in which disorientation or nausea in real or virtual environments can be minimised.a:[["$","$L12",null,{"strategy":"beforeInteractive","id":"structured-data","type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$13"}}],["$","$L14",null,{"study":{"dataset_schema":{"@context":"https://schema.org/","@type":"Dataset","name":"Impact of Visual Self-Motion Cues on Spatial and Temporal Visual Integration, 2023","description":"$15","url":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=856908","https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=856941","https://reshare.ukdataservice.ac.uk/856908","https://reshare.ukdataservice.ac.uk/856941"],"keywords":["PSYCHOLOGICAL EFFECTS","PSYCHOLOGY","PSYCHOLOGICAL RESEARCH","HUMAN BEHAVIOUR"],"identifier":["http://dx.doi.org/10.5255/UKDA-SN-856908","http://dx.doi.org/10.5255/UKDA-SN-856941"],"includedInDataCatalog":[{"@type":"DataCatalog","name":"UK Data Service","url":"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=856941"},{"@type":"DataCatalog","name":"UK Data Service","url":"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=856908"}],"sponsor":[{"@type":"Organization","name":"Economic and Social Research Council"}],"temporalCoverage":"2023-11-21/2024-01-16"},"extra_data":{"study_design":[],"sex":"all","dois":["http://dx.doi.org/10.5255/UKDA-SN-856908","http://dx.doi.org/10.5255/UKDA-SN-856941"],"ai_summary":null,"name":"Impact of Visual Self-Motion Cues on Spatial and Temporal Visual Integration, 2023","num_variables":null,"country_codes":["GB"],"language_codes":["en"],"urls":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=856908","https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=856941","https://reshare.ukdataservice.ac.uk/856908","https://reshare.ukdataservice.ac.uk/856941"],"source":["ukds"],"geographic_coverage":"Cardiff","resource_type":"dataset","genetic_data_collected":false,"start_year":2023,"duration_years":1,"instruments":[],"slug":"impact-of-visual-self-motion-cues-on-spatial-and-temporal-visual-integration-2023","data_access":"The Data Collection is available to any user without the requirement for registration for download/access.","end_year":2024,"harmony_id":"ukds/856941","uuid":"2db9d61627bc351ce2c46242fa492f43"},"distance":0,"score":0,"parent":{},"ancestors":[]}}]]
10:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
16:T103f,We present two psychophysics experiments investigating the impact of visual self-motion cues on visual spatiotemporal summation.  
The first experiment is a 4AFC target detection task where participants give the location of a target that appeared in one of four known locations, against a moving background.  Each target is composed of two small probes which are separated by four diagonal pixels.  Probes always appear for one frame (8.33ms).  The probes are either presented concurrently (at the same time) or are separated by one of six interstimulus-intervals (ISIs) of 0 to 100ms.  The two probes are aligned radially, such that the second probe is either inward (closer to the centre of the screen) or outward than the first probe.  The background is an array of ~1000 dots which move in an expanding or contracting pattern to simulate forwards or backwards self-motion.  The main manipulation is the relationship between the alignment of the probes (inward or outward) and the background motion (expanding or contracting).  The relationship can either be congruent (both in same direction) or incongruent.  Staircases adjust the target luminance for each condition to identify the threshold where participants have 75% accuracy.  
A second study has the same design but with targets appearing in three of the four known locations and participants identifying the location which did NOT contain a target.
Finally a flow-parsing study was conducted to validate that our background stimulus was eliciting self-motion cues.  In this study participants saw a single moving target in one of four locations and indicated whether it was moving inwards or outwards.  A staircase procedure adjusted the target speed to identify the speed where participant gave an ‘inward’ response 50% of the time.
This version contains the full 12 iterations of the three main tasks and the figures and aggregate data files have been updated.Every time we move, the image of the world at the back of the eye changes. Despite this, our perception is of an unchanging world. How does the brain translate a continually changing image into a percept of a stable, stationary, rigid world? Does the brain use a map of the external environment (an "allocentric map") and the position of the observer within it, built up over time, to underpin the perception of stability? Does the brain continually update a map of where scene objects are relative to the observer (an "egocentric map"; e.g. there is an object straight ahead of me, if I walk forward I should expect it to get closer to me)? Does the brain not create a map but just divide up the image motion into that which is likely due to movement of the observer (and which can consequently be ignored) and that which is likely due to objects moving within the scene (which become a focus of attention)? The hypothesis that underpins this research project is that it is not a single one of these mechanisms that underpins perceptual stability, but that all of them, their contribution dependent on the task being performed by the observer. In some cases the task will require a fast estimate to support an ongoing action which might favour one mechanism, on another task, where timing is not so critical, a slower, but more accurate, mechanism might be more appropriate. This collaborative project, which combines complementary expertise in Psychology, Movement Sciences, and Computing from Germany, The Netherlands and the United Kingdom, and importantly, researchers that start from different theoretical perspectives, will test this hypothesis. We will study a diverse series of tasks that present a range of challenges to the moving observer. We will make use of various innovative experimental paradigms that exploit recent technological advances such as virtual reality combined with simultaneous motion tracking. Understanding where and how different mechanisms of perceptual stability play a role advances not only our scientific understanding, but also has the potential to inform industry as well as medicine about the circumstances in which disorientation or nausea in real or virtual environments can be minimised.17:T103f,We present two psychophysics experiments investigating the impact of visual self-motion cues on visual spatiotemporal summation.  
The first experiment is a 4AFC target detection task where participants give the location of a target that appeared in one of four known locations, against a moving background.  Each target is composed of two small probes which are separated by four diagonal pixels.  Probes always appear for one frame (8.33ms).  The probes are either presented concurrently (at the same time) or are separated by one of six interstimulus-intervals (ISIs) of 0 to 100ms.  The two probes are aligned radially, such that the second probe is either inward (closer to the centre of the screen) or outward than the first probe.  The background is an array of ~1000 dots which move in an expanding or contracting pattern to simulate forwards or backwards self-motion.  The main manipulation is the relationship between the alignment of the probes (inward or outward) and the background motion (expanding or contracting).  The relationship can either be congruent (both in same direction) or incongruent.  Staircases adjust the target luminance for each condition to identify the threshold where participants have 75% accuracy.  
A second study has the same design but with targets appearing in three of the four known locations and participants identifying the location which did NOT contain a target.
Finally a flow-parsing study was conducted to validate that our background stimulus was eliciting self-motion cues.  In this study participants saw a single moving target in one of four locations and indicated whether it was moving inwards or outwards.  A staircase procedure adjusted the target speed to identify the speed where participant gave an ‘inward’ response 50% of the time.
This version contains the full 12 iterations of the three main tasks and the figures and aggregate data files have been updated.Every time we move, the image of the world at the back of the eye changes. Despite this, our perception is of an unchanging world. How does the brain translate a continually changing image into a percept of a stable, stationary, rigid world? Does the brain use a map of the external environment (an "allocentric map") and the position of the observer within it, built up over time, to underpin the perception of stability? Does the brain continually update a map of where scene objects are relative to the observer (an "egocentric map"; e.g. there is an object straight ahead of me, if I walk forward I should expect it to get closer to me)? Does the brain not create a map but just divide up the image motion into that which is likely due to movement of the observer (and which can consequently be ignored) and that which is likely due to objects moving within the scene (which become a focus of attention)? The hypothesis that underpins this research project is that it is not a single one of these mechanisms that underpins perceptual stability, but that all of them, their contribution dependent on the task being performed by the observer. In some cases the task will require a fast estimate to support an ongoing action which might favour one mechanism, on another task, where timing is not so critical, a slower, but more accurate, mechanism might be more appropriate. This collaborative project, which combines complementary expertise in Psychology, Movement Sciences, and Computing from Germany, The Netherlands and the United Kingdom, and importantly, researchers that start from different theoretical perspectives, will test this hypothesis. We will study a diverse series of tasks that present a range of challenges to the moving observer. We will make use of various innovative experimental paradigms that exploit recent technological advances such as virtual reality combined with simultaneous motion tracking. Understanding where and how different mechanisms of perceptual stability play a role advances not only our scientific understanding, but also has the potential to inform industry as well as medicine about the circumstances in which disorientation or nausea in real or virtual environments can be minimised.18:T103f,We present two psychophysics experiments investigating the impact of visual self-motion cues on visual spatiotemporal summation.  
The first experiment is a 4AFC target detection task where participants give the location of a target that appeared in one of four known locations, against a moving background.  Each target is composed of two small probes which are separated by four diagonal pixels.  Probes always appear for one frame (8.33ms).  The probes are either presented concurrently (at the same time) or are separated by one of six interstimulus-intervals (ISIs) of 0 to 100ms.  The two probes are aligned radially, such that the second probe is either inward (closer to the centre of the screen) or outward than the first probe.  The background is an array of ~1000 dots which move in an expanding or contracting pattern to simulate forwards or backwards self-motion.  The main manipulation is the relationship between the alignment of the probes (inward or outward) and the background motion (expanding or contracting).  The relationship can either be congruent (both in same direction) or incongruent.  Staircases adjust the target luminance for each condition to identify the threshold where participants have 75% accuracy.  
A second study has the same design but with targets appearing in three of the four known locations and participants identifying the location which did NOT contain a target.
Finally a flow-parsing study was conducted to validate that our background stimulus was eliciting self-motion cues.  In this study participants saw a single moving target in one of four locations and indicated whether it was moving inwards or outwards.  A staircase procedure adjusted the target speed to identify the speed where participant gave an ‘inward’ response 50% of the time.
This version contains the full 12 iterations of the three main tasks and the figures and aggregate data files have been updated.Every time we move, the image of the world at the back of the eye changes. Despite this, our perception is of an unchanging world. How does the brain translate a continually changing image into a percept of a stable, stationary, rigid world? Does the brain use a map of the external environment (an "allocentric map") and the position of the observer within it, built up over time, to underpin the perception of stability? Does the brain continually update a map of where scene objects are relative to the observer (an "egocentric map"; e.g. there is an object straight ahead of me, if I walk forward I should expect it to get closer to me)? Does the brain not create a map but just divide up the image motion into that which is likely due to movement of the observer (and which can consequently be ignored) and that which is likely due to objects moving within the scene (which become a focus of attention)? The hypothesis that underpins this research project is that it is not a single one of these mechanisms that underpins perceptual stability, but that all of them, their contribution dependent on the task being performed by the observer. In some cases the task will require a fast estimate to support an ongoing action which might favour one mechanism, on another task, where timing is not so critical, a slower, but more accurate, mechanism might be more appropriate. This collaborative project, which combines complementary expertise in Psychology, Movement Sciences, and Computing from Germany, The Netherlands and the United Kingdom, and importantly, researchers that start from different theoretical perspectives, will test this hypothesis. We will study a diverse series of tasks that present a range of challenges to the moving observer. We will make use of various innovative experimental paradigms that exploit recent technological advances such as virtual reality combined with simultaneous motion tracking. Understanding where and how different mechanisms of perceptual stability play a role advances not only our scientific understanding, but also has the potential to inform industry as well as medicine about the circumstances in which disorientation or nausea in real or virtual environments can be minimised.e:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Impact of Visual Self-Motion Cues on Spatial and Temporal Visual Integration, 2023"}],["$","meta","2",{"name":"description","content":"$16"}],["$","meta","3",{"property":"og:title","content":"Impact of Visual Self-Motion Cues on Spatial and Temporal Visual Integration, 2023"}],["$","meta","4",{"property":"og:description","content":"$17"}],["$","meta","5",{"property":"og:url","content":"https://harmonydata.ac.uk/search/items/impact-of-visual-self-motion-cues-on-spatial-and-temporal-visual-integration-2023"}],["$","meta","6",{"property":"og:site_name","content":"Academic Resource Discovery"}],["$","meta","7",{"property":"og:locale","content":"en_US"}],["$","meta","8",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","meta","9",{"property":"og:image:width","content":"1200"}],["$","meta","10",{"property":"og:image:height","content":"630"}],["$","meta","11",{"property":"og:image:alt","content":"Impact of Visual Self-Motion Cues on Spatial and Temporal Visual Integration, 2023"}],["$","meta","12",{"property":"og:type","content":"website"}],["$","meta","13",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","14",{"name":"twitter:title","content":"Impact of Visual Self-Motion Cues on Spatial and Temporal Visual Integration, 2023"}],["$","meta","15",{"name":"twitter:description","content":"$18"}],["$","meta","16",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","link","17",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
c:null
