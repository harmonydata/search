1:"$Sreact.fragment"
2:I[52332,["9692","static/js/9692.83f9877c.js","421","static/js/421.98ca62d9.js","1759","static/js/1759.b13f3ee1.js","3535","static/js/3535.878ceef2.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","574","static/js/574.fd20103e.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.7ef30b9e.js"],"default"]
3:I[65380,["9692","static/js/9692.83f9877c.js","421","static/js/421.98ca62d9.js","1759","static/js/1759.b13f3ee1.js","3535","static/js/3535.878ceef2.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","574","static/js/574.fd20103e.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.7ef30b9e.js"],"AuthProvider"]
4:I[41627,["9692","static/js/9692.83f9877c.js","421","static/js/421.98ca62d9.js","1759","static/js/1759.b13f3ee1.js","3535","static/js/3535.878ceef2.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","574","static/js/574.fd20103e.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.7ef30b9e.js"],"FirebaseProvider"]
5:"$Sreact.suspense"
6:I[92114,["9692","static/js/9692.83f9877c.js","421","static/js/421.98ca62d9.js","1759","static/js/1759.b13f3ee1.js","3535","static/js/3535.878ceef2.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","574","static/js/574.fd20103e.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.7ef30b9e.js"],"SearchProvider"]
7:I[94049,["9692","static/js/9692.83f9877c.js","421","static/js/421.98ca62d9.js","1759","static/js/1759.b13f3ee1.js","3535","static/js/3535.878ceef2.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","574","static/js/574.fd20103e.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.7ef30b9e.js"],"default"]
8:I[20190,["9692","static/js/9692.83f9877c.js","421","static/js/421.98ca62d9.js","1759","static/js/1759.b13f3ee1.js","3535","static/js/3535.878ceef2.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","574","static/js/574.fd20103e.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.7ef30b9e.js"],"default"]
9:I[9766,[],""]
a:I[98924,[],""]
b:I[74744,["9692","static/js/9692.83f9877c.js","421","static/js/421.98ca62d9.js","1759","static/js/1759.b13f3ee1.js","3535","static/js/3535.878ceef2.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","574","static/js/574.fd20103e.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.7ef30b9e.js"],"ToastContainer"]
d:I[24431,[],"OutletBoundary"]
11:I[57150,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/e446a64f2ff89daf.css","style"]
0:{"P":null,"b":"PiYo4VwvF8xNxo2iFW7jS","p":"/search","c":["","items","mental-simulations-of-phonological-representations-are-causally-linked-to-silent-reading-of-direct-versus-indirect-speech-2016-2019"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","mental-simulations-of-phonological-representations-are-causally-linked-to-silent-reading-of-direct-versus-indirect-speech-2016-2019","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/e446a64f2ff89daf.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":[["$","meta",null,{"name":"emotion-insertion-point","content":""}],["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"preconnect","href":"https://www.cataloguementalhealth.ac.uk"}],["$","link",null,{"rel":"dns-prefetch","href":"https://harmonydata.ac.uk"}],["$","style",null,{"dangerouslySetInnerHTML":{"__html":"\n            /* Ensure immediate rendering with Roboto and fallbacks */\n            * { \n              font-family: \"Roboto\", -apple-system, BlinkMacSystemFont, \"Segoe UI\", \"Oxygen\", \"Ubuntu\", \"Cantarell\", \"Fira Sans\", \"Droid Sans\", \"Helvetica Neue\", sans-serif !important;\n              font-display: swap;\n              -webkit-font-smoothing: antialiased;\n              -moz-osx-font-smoothing: grayscale;\n            }\n            body { \n              visibility: visible !important; \n              opacity: 1 !important; \n              margin: 0; \n              padding: 0; \n            }\n          "}}]]}],["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":["$","$5",null,{"fallback":["$","div",null,{"children":"Loading..."}],"children":["$","$L6",null,{"children":[["$","$L7",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L8",null,{}],["$","$L7",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L9",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$Lb",null,{"position":"bottom-right"}]]}]}]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L9",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","mental-simulations-of-phonological-representations-are-causally-linked-to-silent-reading-of-direct-versus-indirect-speech-2016-2019","d"],["$","$1","c",{"children":[null,["$","$L9",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$Lc",null,["$","$Ld",null,{"children":["$Le","$Lf"]}]]}],{},null,false]},null,false]},null,false]},null,false],"$L10",false]],"m":"$undefined","G":["$11",[]],"s":false,"S":true}
12:I[15278,[],"AsyncMetadataOutlet"]
14:I[24431,[],"ViewportBoundary"]
16:I[24431,[],"MetadataBoundary"]
f:["$","$L12",null,{"promise":"$@13"}]
10:["$","$1","h",{"children":[null,[["$","$L14",null,{"children":"$L15"}],["$","meta",null,{"name":"next-size-adjust","content":""}]],["$","$L16",null,{"children":["$","div",null,{"hidden":true,"children":["$","$5",null,{"fallback":null,"children":"$L17"}]}]}]]}]
18:I[41402,["9692","static/js/9692.83f9877c.js","1759","static/js/1759.b13f3ee1.js","690","static/js/690.e023e61b.js","3535","static/js/3535.878ceef2.js","5332","static/js/5332.4ca3e6c6.js","2410","static/js/2410.ec36c5aa.js","5183","static/js/5183.9f1a7545.js","5738","static/js/5738.d28a9943.js","3055","static/js/3055.87b66c06.js","8977","static/js/8977.25169e64.js","6387","static/js/app/items/%5Bslug%5D/page.0819d17d.js"],""]
19:T1500,{"@context":"https://schema.org/","@type":"Dataset","name":"Mental simulations of phonological representations are causally linked to silent reading of direct versus indirect speech 2016-2019","description":"In three experiments, this project explored the phonological aspect and the causal role of speech simulations in silent reading of tongue twisters in direct speech, indirect speech and non-speech sentences. \nEmbodied theories propose that language is understood via mental simulations of sensory states related to perception and action. Given that direct speech (e.g., She says, “It’s a lovely day!”) is perceived to be more vivid than indirect speech (e.g., She says (that) it’s a lovely day) in perception, recent research shows in silent reading that more vivid speech representations are mentally simulated for direct speech than for indirect speech. This ‘simulated’ speech is found to contain suprasegmental prosodic representations (e.g., speech prosody) but its phonological detail and its causal role in silent reading of direct speech remain unclear. The results demonstrated greater visual tongue-twister effects (phonemic interference) during silent reading (Experiment 1) but not oral reading (Experiment 2) of direct speech as compared to indirect speech and non-speech. The tongue-twister effects in silent reading of direct speech were selectively disrupted by phonological interference (concurrent articulation) as compared to manual interference (finger tapping) (Experiment 3). The results replicated more vivid speech simulations in silent reading of direct speech, and additionally extended them to the phonological dimension. Crucially, they demonstrated a causal role of phonological simulations in silent reading of direct speech, at least in tongue-twister reading. The findings are discussed in relation to multidimensionality and task dependence of mental simulation and its mechanisms.Written communication (e.g., emails, news reports, social media) is a major form of social information exchange in today's world. However, it is sometimes difficult to interpret the intended meaning of a written message without hearing prosody (rhythm, stress, and intonation of speech) that is instrumental in understanding the writer's feelings, attitudes, and intentions. For example, a prosody-less \"thank you\" email can be confusing as to whether the sender is being sincere or sarcastic (Kruger et al., 2005). Emails like these are often misinterpreted as being more negative or neutral than intended; such miscommunications can damage social cohesiveness and group identity within organisations and communities, thereby undermining economic performance and societal stability (Byron, 2008).\n\nInterestingly, written words may not be entirely \"silent\" after all. My recent research showed that we mentally (or covertly) simulate speech prosody (or \"inner voices\") during silent reading of written direct quotations (Mary gasped: \"This dress is beautiful!\") as if we were hearing someone speaking (Yao et al., 2011, 2012). For example, Yao and colleagues (2011) observed that silent reading of direct quotations elicited higher neural activity in voice-selective areas of the auditory cortex as compared to silent reading of meaning-equivalent indirect speech (Mary gasped that the dress was beautiful.).\n\nCan such covert prosody compensate for the lack of overt speech prosody in written language and thus enhance written communication? To address this question, the proposed project will systematically examine the nature (is covert prosody sound- or action-based in nature?), mechanisms (what information processing systems are engaged?) and emotional consequences (does covert prosody induce emotions and thereby influence behaviour?) of covert prosodic processing in silent reading of written direct quotations.\n\nTheoretically motivated by the working neural models for \"overt\" emotional prosodic processing in speech (e.g., Schirmer & Kotz, 2006), the current proposal will probe \"where\" and \"when\" in the brain covert prosodic cues of various natures are mentally simulated and integrated into coherent covert prosodic representations and how these representations consequently induce emotional responses and aid in inferring the quoted speaker's mental state. Using complementary neuroimaging techniques, it will localise the neural substrates of systems engaged in covert emotional prosodic processing (fMRI), specify the time courses of the information processes within these systems (EEG, MEG), and integrate this information to form a unified spatio-temporal neural model for covert emotional prosodic processing.\n\nThe findings of this project have clear implications for the theoretical development of emotional prosody-based social communication, embodied cognition, and speech pragmatics, and will be of interest to all written language users (e.g., communication-based enterprises, social services, and the wider public).","url":"https://harmonydata.ac.uk/search/items/mental-simulations-of-phonological-representations-are-causally-linked-to-silent-reading-of-direct-versus-indirect-speech-2016-2019","identifier":["http://dx.doi.org/10.5255/UKDA-SN-854460"],"keywords":["READING COMPREHENSION","LANGUAGE STUDY","SPEECH","PERCEPTION","READING (ACTIVITY)"],"temporalCoverage":"2016-03-07/2019-12-31"}c:["$","$5",null,{"fallback":["$","div",null,{"children":"Loading..."}],"children":[["$","$L18",null,{"strategy":"beforeInteractive","id":"structured-data","type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$19"}}],"$L1a"]}]
1b:I[78977,["9692","static/js/9692.83f9877c.js","1759","static/js/1759.b13f3ee1.js","690","static/js/690.e023e61b.js","3535","static/js/3535.878ceef2.js","5332","static/js/5332.4ca3e6c6.js","2410","static/js/2410.ec36c5aa.js","5183","static/js/5183.9f1a7545.js","5738","static/js/5738.d28a9943.js","3055","static/js/3055.87b66c06.js","8977","static/js/8977.25169e64.js","6387","static/js/app/items/%5Bslug%5D/page.0819d17d.js"],"default"]
1c:T129e,In three experiments, this project explored the phonological aspect and the causal role of speech simulations in silent reading of tongue twisters in direct speech, indirect speech and non-speech sentences. 
Embodied theories propose that language is understood via mental simulations of sensory states related to perception and action. Given that direct speech (e.g., She says, “It’s a lovely day!”) is perceived to be more vivid than indirect speech (e.g., She says (that) it’s a lovely day) in perception, recent research shows in silent reading that more vivid speech representations are mentally simulated for direct speech than for indirect speech. This ‘simulated’ speech is found to contain suprasegmental prosodic representations (e.g., speech prosody) but its phonological detail and its causal role in silent reading of direct speech remain unclear. The results demonstrated greater visual tongue-twister effects (phonemic interference) during silent reading (Experiment 1) but not oral reading (Experiment 2) of direct speech as compared to indirect speech and non-speech. The tongue-twister effects in silent reading of direct speech were selectively disrupted by phonological interference (concurrent articulation) as compared to manual interference (finger tapping) (Experiment 3). The results replicated more vivid speech simulations in silent reading of direct speech, and additionally extended them to the phonological dimension. Crucially, they demonstrated a causal role of phonological simulations in silent reading of direct speech, at least in tongue-twister reading. The findings are discussed in relation to multidimensionality and task dependence of mental simulation and its mechanisms.Written communication (e.g., emails, news reports, social media) is a major form of social information exchange in today's world. However, it is sometimes difficult to interpret the intended meaning of a written message without hearing prosody (rhythm, stress, and intonation of speech) that is instrumental in understanding the writer's feelings, attitudes, and intentions. For example, a prosody-less "thank you" email can be confusing as to whether the sender is being sincere or sarcastic (Kruger et al., 2005). Emails like these are often misinterpreted as being more negative or neutral than intended; such miscommunications can damage social cohesiveness and group identity within organisations and communities, thereby undermining economic performance and societal stability (Byron, 2008).

Interestingly, written words may not be entirely "silent" after all. My recent research showed that we mentally (or covertly) simulate speech prosody (or "inner voices") during silent reading of written direct quotations (Mary gasped: "This dress is beautiful!") as if we were hearing someone speaking (Yao et al., 2011, 2012). For example, Yao and colleagues (2011) observed that silent reading of direct quotations elicited higher neural activity in voice-selective areas of the auditory cortex as compared to silent reading of meaning-equivalent indirect speech (Mary gasped that the dress was beautiful.).

Can such covert prosody compensate for the lack of overt speech prosody in written language and thus enhance written communication? To address this question, the proposed project will systematically examine the nature (is covert prosody sound- or action-based in nature?), mechanisms (what information processing systems are engaged?) and emotional consequences (does covert prosody induce emotions and thereby influence behaviour?) of covert prosodic processing in silent reading of written direct quotations.

Theoretically motivated by the working neural models for "overt" emotional prosodic processing in speech (e.g., Schirmer & Kotz, 2006), the current proposal will probe "where" and "when" in the brain covert prosodic cues of various natures are mentally simulated and integrated into coherent covert prosodic representations and how these representations consequently induce emotional responses and aid in inferring the quoted speaker's mental state. Using complementary neuroimaging techniques, it will localise the neural substrates of systems engaged in covert emotional prosodic processing (fMRI), specify the time courses of the information processes within these systems (EEG, MEG), and integrate this information to form a unified spatio-temporal neural model for covert emotional prosodic processing.

The findings of this project have clear implications for the theoretical development of emotional prosody-based social communication, embodied cognition, and speech pragmatics, and will be of interest to all written language users (e.g., communication-based enterprises, social services, and the wider public).1a:["$","$L1b",null,{"study":{"dataset_schema":{"@context":"https://schema.org/","@type":"Dataset","name":"Mental simulations of phonological representations are causally linked to silent reading of direct versus indirect speech 2016-2019","description":"$1c","url":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=854460","https://reshare.ukdataservice.ac.uk/854460"],"keywords":["READING COMPREHENSION","LANGUAGE STUDY","SPEECH","PERCEPTION","READING (ACTIVITY)"],"identifier":["http://dx.doi.org/10.5255/UKDA-SN-854460"],"includedInDataCatalog":[{"@type":"DataCatalog","name":"UK Data Service","url":"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=854460"}],"sponsor":[{"@type":"Organization","name":"Economic and Social Research Council"}],"temporalCoverage":"2016-03-07/2019-12-31"},"extra_data":{"geographic_coverage":"Greater Manchester","start_year":2016,"harmony_id":"ukds/854460","end_year":2019,"data_access":"The Data Collection is available to any user without the requirement for registration for download/access.","urls":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=854460","https://reshare.ukdataservice.ac.uk/854460"],"name":"Mental simulations of phonological representations are causally linked to silent reading of direct versus indirect speech 2016-2019","source":["ukds"],"slug":"mental-simulations-of-phonological-representations-are-causally-linked-to-silent-reading-of-direct-versus-indirect-speech-2016-2019","genetic_data_collected":false,"dois":["http://dx.doi.org/10.5255/UKDA-SN-854460"],"sex":"male","instruments":[],"study_design":[],"ai_summary":null,"country_codes":["GB"],"duration_years":3,"resource_type":"dataset","language_codes":["en"],"num_variables":null,"uuid":"acd1241628d643ee85f250457f272e1c"},"distance":0,"score":0,"parent":{},"ancestors":[]}}]
15:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
e:null
1d:T129e,In three experiments, this project explored the phonological aspect and the causal role of speech simulations in silent reading of tongue twisters in direct speech, indirect speech and non-speech sentences. 
Embodied theories propose that language is understood via mental simulations of sensory states related to perception and action. Given that direct speech (e.g., She says, “It’s a lovely day!”) is perceived to be more vivid than indirect speech (e.g., She says (that) it’s a lovely day) in perception, recent research shows in silent reading that more vivid speech representations are mentally simulated for direct speech than for indirect speech. This ‘simulated’ speech is found to contain suprasegmental prosodic representations (e.g., speech prosody) but its phonological detail and its causal role in silent reading of direct speech remain unclear. The results demonstrated greater visual tongue-twister effects (phonemic interference) during silent reading (Experiment 1) but not oral reading (Experiment 2) of direct speech as compared to indirect speech and non-speech. The tongue-twister effects in silent reading of direct speech were selectively disrupted by phonological interference (concurrent articulation) as compared to manual interference (finger tapping) (Experiment 3). The results replicated more vivid speech simulations in silent reading of direct speech, and additionally extended them to the phonological dimension. Crucially, they demonstrated a causal role of phonological simulations in silent reading of direct speech, at least in tongue-twister reading. The findings are discussed in relation to multidimensionality and task dependence of mental simulation and its mechanisms.Written communication (e.g., emails, news reports, social media) is a major form of social information exchange in today's world. However, it is sometimes difficult to interpret the intended meaning of a written message without hearing prosody (rhythm, stress, and intonation of speech) that is instrumental in understanding the writer's feelings, attitudes, and intentions. For example, a prosody-less "thank you" email can be confusing as to whether the sender is being sincere or sarcastic (Kruger et al., 2005). Emails like these are often misinterpreted as being more negative or neutral than intended; such miscommunications can damage social cohesiveness and group identity within organisations and communities, thereby undermining economic performance and societal stability (Byron, 2008).

Interestingly, written words may not be entirely "silent" after all. My recent research showed that we mentally (or covertly) simulate speech prosody (or "inner voices") during silent reading of written direct quotations (Mary gasped: "This dress is beautiful!") as if we were hearing someone speaking (Yao et al., 2011, 2012). For example, Yao and colleagues (2011) observed that silent reading of direct quotations elicited higher neural activity in voice-selective areas of the auditory cortex as compared to silent reading of meaning-equivalent indirect speech (Mary gasped that the dress was beautiful.).

Can such covert prosody compensate for the lack of overt speech prosody in written language and thus enhance written communication? To address this question, the proposed project will systematically examine the nature (is covert prosody sound- or action-based in nature?), mechanisms (what information processing systems are engaged?) and emotional consequences (does covert prosody induce emotions and thereby influence behaviour?) of covert prosodic processing in silent reading of written direct quotations.

Theoretically motivated by the working neural models for "overt" emotional prosodic processing in speech (e.g., Schirmer & Kotz, 2006), the current proposal will probe "where" and "when" in the brain covert prosodic cues of various natures are mentally simulated and integrated into coherent covert prosodic representations and how these representations consequently induce emotional responses and aid in inferring the quoted speaker's mental state. Using complementary neuroimaging techniques, it will localise the neural substrates of systems engaged in covert emotional prosodic processing (fMRI), specify the time courses of the information processes within these systems (EEG, MEG), and integrate this information to form a unified spatio-temporal neural model for covert emotional prosodic processing.

The findings of this project have clear implications for the theoretical development of emotional prosody-based social communication, embodied cognition, and speech pragmatics, and will be of interest to all written language users (e.g., communication-based enterprises, social services, and the wider public).13:{"metadata":[["$","title","0",{"children":"Mental simulations of phonological representations are causally linked to silent reading of direct versus indirect speech 2016-2019"}],["$","meta","1",{"name":"description","content":"$1d"}],"$L1e","$L1f","$L20","$L21","$L22","$L23","$L24","$L25","$L26","$L27","$L28","$L29","$L2a","$L2b","$L2c","$L2d"],"error":null,"digest":"$undefined"}
30:I[80622,[],"IconMark"]
1e:["$","meta","2",{"property":"og:title","content":"Mental simulations of phonological representations are causally linked to silent reading of direct versus indirect speech 2016-2019"}]
2e:T129e,In three experiments, this project explored the phonological aspect and the causal role of speech simulations in silent reading of tongue twisters in direct speech, indirect speech and non-speech sentences. 
Embodied theories propose that language is understood via mental simulations of sensory states related to perception and action. Given that direct speech (e.g., She says, “It’s a lovely day!”) is perceived to be more vivid than indirect speech (e.g., She says (that) it’s a lovely day) in perception, recent research shows in silent reading that more vivid speech representations are mentally simulated for direct speech than for indirect speech. This ‘simulated’ speech is found to contain suprasegmental prosodic representations (e.g., speech prosody) but its phonological detail and its causal role in silent reading of direct speech remain unclear. The results demonstrated greater visual tongue-twister effects (phonemic interference) during silent reading (Experiment 1) but not oral reading (Experiment 2) of direct speech as compared to indirect speech and non-speech. The tongue-twister effects in silent reading of direct speech were selectively disrupted by phonological interference (concurrent articulation) as compared to manual interference (finger tapping) (Experiment 3). The results replicated more vivid speech simulations in silent reading of direct speech, and additionally extended them to the phonological dimension. Crucially, they demonstrated a causal role of phonological simulations in silent reading of direct speech, at least in tongue-twister reading. The findings are discussed in relation to multidimensionality and task dependence of mental simulation and its mechanisms.Written communication (e.g., emails, news reports, social media) is a major form of social information exchange in today's world. However, it is sometimes difficult to interpret the intended meaning of a written message without hearing prosody (rhythm, stress, and intonation of speech) that is instrumental in understanding the writer's feelings, attitudes, and intentions. For example, a prosody-less "thank you" email can be confusing as to whether the sender is being sincere or sarcastic (Kruger et al., 2005). Emails like these are often misinterpreted as being more negative or neutral than intended; such miscommunications can damage social cohesiveness and group identity within organisations and communities, thereby undermining economic performance and societal stability (Byron, 2008).

Interestingly, written words may not be entirely "silent" after all. My recent research showed that we mentally (or covertly) simulate speech prosody (or "inner voices") during silent reading of written direct quotations (Mary gasped: "This dress is beautiful!") as if we were hearing someone speaking (Yao et al., 2011, 2012). For example, Yao and colleagues (2011) observed that silent reading of direct quotations elicited higher neural activity in voice-selective areas of the auditory cortex as compared to silent reading of meaning-equivalent indirect speech (Mary gasped that the dress was beautiful.).

Can such covert prosody compensate for the lack of overt speech prosody in written language and thus enhance written communication? To address this question, the proposed project will systematically examine the nature (is covert prosody sound- or action-based in nature?), mechanisms (what information processing systems are engaged?) and emotional consequences (does covert prosody induce emotions and thereby influence behaviour?) of covert prosodic processing in silent reading of written direct quotations.

Theoretically motivated by the working neural models for "overt" emotional prosodic processing in speech (e.g., Schirmer & Kotz, 2006), the current proposal will probe "where" and "when" in the brain covert prosodic cues of various natures are mentally simulated and integrated into coherent covert prosodic representations and how these representations consequently induce emotional responses and aid in inferring the quoted speaker's mental state. Using complementary neuroimaging techniques, it will localise the neural substrates of systems engaged in covert emotional prosodic processing (fMRI), specify the time courses of the information processes within these systems (EEG, MEG), and integrate this information to form a unified spatio-temporal neural model for covert emotional prosodic processing.

The findings of this project have clear implications for the theoretical development of emotional prosody-based social communication, embodied cognition, and speech pragmatics, and will be of interest to all written language users (e.g., communication-based enterprises, social services, and the wider public).1f:["$","meta","3",{"property":"og:description","content":"$2e"}]
20:["$","meta","4",{"property":"og:url","content":"https://harmonydata.ac.uk/search/items/mental-simulations-of-phonological-representations-are-causally-linked-to-silent-reading-of-direct-versus-indirect-speech-2016-2019"}]
21:["$","meta","5",{"property":"og:site_name","content":"Academic Resource Discovery"}]
22:["$","meta","6",{"property":"og:locale","content":"en_US"}]
23:["$","meta","7",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}]
24:["$","meta","8",{"property":"og:image:width","content":"1200"}]
25:["$","meta","9",{"property":"og:image:height","content":"630"}]
26:["$","meta","10",{"property":"og:image:alt","content":"Mental simulations of phonological representations are causally linked to silent reading of direct versus indirect speech 2016-2019"}]
27:["$","meta","11",{"property":"og:type","content":"website"}]
28:["$","meta","12",{"name":"twitter:card","content":"summary_large_image"}]
29:["$","meta","13",{"name":"twitter:title","content":"Mental simulations of phonological representations are causally linked to silent reading of direct versus indirect speech 2016-2019"}]
2f:T129e,In three experiments, this project explored the phonological aspect and the causal role of speech simulations in silent reading of tongue twisters in direct speech, indirect speech and non-speech sentences. 
Embodied theories propose that language is understood via mental simulations of sensory states related to perception and action. Given that direct speech (e.g., She says, “It’s a lovely day!”) is perceived to be more vivid than indirect speech (e.g., She says (that) it’s a lovely day) in perception, recent research shows in silent reading that more vivid speech representations are mentally simulated for direct speech than for indirect speech. This ‘simulated’ speech is found to contain suprasegmental prosodic representations (e.g., speech prosody) but its phonological detail and its causal role in silent reading of direct speech remain unclear. The results demonstrated greater visual tongue-twister effects (phonemic interference) during silent reading (Experiment 1) but not oral reading (Experiment 2) of direct speech as compared to indirect speech and non-speech. The tongue-twister effects in silent reading of direct speech were selectively disrupted by phonological interference (concurrent articulation) as compared to manual interference (finger tapping) (Experiment 3). The results replicated more vivid speech simulations in silent reading of direct speech, and additionally extended them to the phonological dimension. Crucially, they demonstrated a causal role of phonological simulations in silent reading of direct speech, at least in tongue-twister reading. The findings are discussed in relation to multidimensionality and task dependence of mental simulation and its mechanisms.Written communication (e.g., emails, news reports, social media) is a major form of social information exchange in today's world. However, it is sometimes difficult to interpret the intended meaning of a written message without hearing prosody (rhythm, stress, and intonation of speech) that is instrumental in understanding the writer's feelings, attitudes, and intentions. For example, a prosody-less "thank you" email can be confusing as to whether the sender is being sincere or sarcastic (Kruger et al., 2005). Emails like these are often misinterpreted as being more negative or neutral than intended; such miscommunications can damage social cohesiveness and group identity within organisations and communities, thereby undermining economic performance and societal stability (Byron, 2008).

Interestingly, written words may not be entirely "silent" after all. My recent research showed that we mentally (or covertly) simulate speech prosody (or "inner voices") during silent reading of written direct quotations (Mary gasped: "This dress is beautiful!") as if we were hearing someone speaking (Yao et al., 2011, 2012). For example, Yao and colleagues (2011) observed that silent reading of direct quotations elicited higher neural activity in voice-selective areas of the auditory cortex as compared to silent reading of meaning-equivalent indirect speech (Mary gasped that the dress was beautiful.).

Can such covert prosody compensate for the lack of overt speech prosody in written language and thus enhance written communication? To address this question, the proposed project will systematically examine the nature (is covert prosody sound- or action-based in nature?), mechanisms (what information processing systems are engaged?) and emotional consequences (does covert prosody induce emotions and thereby influence behaviour?) of covert prosodic processing in silent reading of written direct quotations.

Theoretically motivated by the working neural models for "overt" emotional prosodic processing in speech (e.g., Schirmer & Kotz, 2006), the current proposal will probe "where" and "when" in the brain covert prosodic cues of various natures are mentally simulated and integrated into coherent covert prosodic representations and how these representations consequently induce emotional responses and aid in inferring the quoted speaker's mental state. Using complementary neuroimaging techniques, it will localise the neural substrates of systems engaged in covert emotional prosodic processing (fMRI), specify the time courses of the information processes within these systems (EEG, MEG), and integrate this information to form a unified spatio-temporal neural model for covert emotional prosodic processing.

The findings of this project have clear implications for the theoretical development of emotional prosody-based social communication, embodied cognition, and speech pragmatics, and will be of interest to all written language users (e.g., communication-based enterprises, social services, and the wider public).2a:["$","meta","14",{"name":"twitter:description","content":"$2f"}]
2b:["$","meta","15",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}]
2c:["$","link","16",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]
2d:["$","$L30","17",{}]
17:"$13:metadata"
