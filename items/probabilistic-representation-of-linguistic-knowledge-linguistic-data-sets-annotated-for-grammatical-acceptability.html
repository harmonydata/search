<!DOCTYPE html><!--WLCgvrrdknkKZDhW_WCbK--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/search/_next/static/css/e446a64f2ff89daf.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/search/_next/static/js/webpack.ccaf2443.js"/><script src="/search/_next/static/js/4bd1b696.100b9d70.js" async=""></script><script src="/search/_next/static/js/1255.90e9842b.js" async=""></script><script src="/search/_next/static/js/main-app.0e7376d5.js" async=""></script><script src="/search/_next/static/js/9692.83f9877c.js" async=""></script><script src="/search/_next/static/js/421.98ca62d9.js" async=""></script><script src="/search/_next/static/js/1759.b13f3ee1.js" async=""></script><script src="/search/_next/static/js/3535.878ceef2.js" async=""></script><script src="/search/_next/static/js/2619.b8db57ac.js" async=""></script><script src="/search/_next/static/js/3820.af314958.js" async=""></script><script src="/search/_next/static/js/574.fd20103e.js" async=""></script><script src="/search/_next/static/js/5738.d28a9943.js" async=""></script><script src="/search/_next/static/js/app/layout.7ef30b9e.js" async=""></script><script src="/search/_next/static/js/690.e023e61b.js" async=""></script><script src="/search/_next/static/js/5332.4ca3e6c6.js" async=""></script><script src="/search/_next/static/js/2410.ec36c5aa.js" async=""></script><script src="/search/_next/static/js/5183.9f1a7545.js" async=""></script><script src="/search/_next/static/js/3055.8a1757af.js" async=""></script><script src="/search/_next/static/js/8977.d520dad7.js" async=""></script><script src="/search/_next/static/js/app/items/%5Bslug%5D/page.0819d17d.js" async=""></script><meta name="next-size-adjust" content=""/><meta name="emotion-insertion-point" content=""/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><link rel="preconnect" href="https://www.cataloguementalhealth.ac.uk"/><link rel="dns-prefetch" href="https://harmonydata.ac.uk"/><title>The probabilistic representation of linguistic knowledge: Linguistic data sets annotated for grammatical acceptability</title><meta name="description" content="The files contain crowd sourced (Amazon Mechanical Turk) speaker annotated sentences in several domains, and for several languages. The annotations are mean acceptability judgements in several modes of presentation. 

Full documentation of the experimental protocols through which the annotation of these data sets was obtained is provided on the Statistical Models of Grammaticality website (SMOG), please see the related resources section to access the (SMOG) website.

This data collection contains the linguistic data sets in excel, and two papers which explain the project and data and experiments in greater detail.SMOG is exploring the construction of an enriched stochastic model that represents the syntactic knowledge that native speakers of English have of their language.

We are hoping that this kind of model will provide a straightforward explanation for the fact that individual native speakers generally judge the well formedness of sentences along a continuum, rather than through the imposition of a sharp boundary between acceptable and unacceptable sentences.

We are experimenting with different sorts of language models that contain a variety of parameters encoding properties of sentences and probability distributions over corpora.

We are training these models on subsets of the British National Corpus (BNC), and we are testing them on additional subsets of the BNC into which we have introduced grammatical deformations and infelicities of varying degrees of severity and subtlety.

We hope to show that a sufficiently complex enriched language model can encode a fair amount of what native speakers know about the syntax of their language.
This research holds out the prospect of important impact in two areas.
(1) It can shed light on the relationship between the representation and acquisition of linguistic knowledge on one hand, and learning and the encoding of knowledge in other cognitive domains. This can, in turn, help to clarify the respective roles of biologically conditioned learning biases and data driven learning in human cognition.
(2) This work can contribute to the development of more effective language technology by providing insight into the way in which humans represent the syntactic properties of sentences in their language. To the extent that natural language processing systems take account of this class of representations they will provide more efficient tools for parsing and interpreting text and speech."/><meta property="og:title" content="The probabilistic representation of linguistic knowledge: Linguistic data sets annotated for grammatical acceptability"/><meta property="og:description" content="The files contain crowd sourced (Amazon Mechanical Turk) speaker annotated sentences in several domains, and for several languages. The annotations are mean acceptability judgements in several modes of presentation. 

Full documentation of the experimental protocols through which the annotation of these data sets was obtained is provided on the Statistical Models of Grammaticality website (SMOG), please see the related resources section to access the (SMOG) website.

This data collection contains the linguistic data sets in excel, and two papers which explain the project and data and experiments in greater detail.SMOG is exploring the construction of an enriched stochastic model that represents the syntactic knowledge that native speakers of English have of their language.

We are hoping that this kind of model will provide a straightforward explanation for the fact that individual native speakers generally judge the well formedness of sentences along a continuum, rather than through the imposition of a sharp boundary between acceptable and unacceptable sentences.

We are experimenting with different sorts of language models that contain a variety of parameters encoding properties of sentences and probability distributions over corpora.

We are training these models on subsets of the British National Corpus (BNC), and we are testing them on additional subsets of the BNC into which we have introduced grammatical deformations and infelicities of varying degrees of severity and subtlety.

We hope to show that a sufficiently complex enriched language model can encode a fair amount of what native speakers know about the syntax of their language.
This research holds out the prospect of important impact in two areas.
(1) It can shed light on the relationship between the representation and acquisition of linguistic knowledge on one hand, and learning and the encoding of knowledge in other cognitive domains. This can, in turn, help to clarify the respective roles of biologically conditioned learning biases and data driven learning in human cognition.
(2) This work can contribute to the development of more effective language technology by providing insight into the way in which humans represent the syntactic properties of sentences in their language. To the extent that natural language processing systems take account of this class of representations they will provide more efficient tools for parsing and interpreting text and speech."/><meta property="og:url" content="https://harmonydata.ac.uk/search/items/probabilistic-representation-of-linguistic-knowledge-linguistic-data-sets-annotated-for-grammatical-acceptability"/><meta property="og:site_name" content="Academic Resource Discovery"/><meta property="og:locale" content="en_US"/><meta property="og:image" content="https://harmonydata.ac.uk/search/harmony.png"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image:alt" content="The probabilistic representation of linguistic knowledge: Linguistic data sets annotated for grammatical acceptability"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="The probabilistic representation of linguistic knowledge: Linguistic data sets annotated for grammatical acceptability"/><meta name="twitter:description" content="The files contain crowd sourced (Amazon Mechanical Turk) speaker annotated sentences in several domains, and for several languages. The annotations are mean acceptability judgements in several modes of presentation. 

Full documentation of the experimental protocols through which the annotation of these data sets was obtained is provided on the Statistical Models of Grammaticality website (SMOG), please see the related resources section to access the (SMOG) website.

This data collection contains the linguistic data sets in excel, and two papers which explain the project and data and experiments in greater detail.SMOG is exploring the construction of an enriched stochastic model that represents the syntactic knowledge that native speakers of English have of their language.

We are hoping that this kind of model will provide a straightforward explanation for the fact that individual native speakers generally judge the well formedness of sentences along a continuum, rather than through the imposition of a sharp boundary between acceptable and unacceptable sentences.

We are experimenting with different sorts of language models that contain a variety of parameters encoding properties of sentences and probability distributions over corpora.

We are training these models on subsets of the British National Corpus (BNC), and we are testing them on additional subsets of the BNC into which we have introduced grammatical deformations and infelicities of varying degrees of severity and subtlety.

We hope to show that a sufficiently complex enriched language model can encode a fair amount of what native speakers know about the syntax of their language.
This research holds out the prospect of important impact in two areas.
(1) It can shed light on the relationship between the representation and acquisition of linguistic knowledge on one hand, and learning and the encoding of knowledge in other cognitive domains. This can, in turn, help to clarify the respective roles of biologically conditioned learning biases and data driven learning in human cognition.
(2) This work can contribute to the development of more effective language technology by providing insight into the way in which humans represent the syntactic properties of sentences in their language. To the extent that natural language processing systems take account of this class of representations they will provide more efficient tools for parsing and interpreting text and speech."/><meta name="twitter:image" content="https://harmonydata.ac.uk/search/harmony.png"/><link rel="icon" href="/search/favicon.ico" type="image/x-icon" sizes="16x16"/><style>
            /* Ensure immediate rendering with Roboto and fallbacks */
            * { 
              font-family: "Roboto", -apple-system, BlinkMacSystemFont, "Segoe UI", "Oxygen", "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue", sans-serif !important;
              font-display: swap;
              -webkit-font-smoothing: antialiased;
              -moz-osx-font-smoothing: grayscale;
            }
            body { 
              visibility: visible !important; 
              opacity: 1 !important; 
              margin: 0; 
              padding: 0; 
            }
          </style><script src="/search/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script><style data-emotion="mui-global v658lt">html{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;box-sizing:border-box;-webkit-text-size-adjust:100%;}*,*::before,*::after{box-sizing:inherit;}strong,b{font-weight:700;}body{margin:0;color:#1A1A1A;font-size:0.875rem;line-height:1.5;font-family:'Roboto','Roboto Fallback',-apple-system,BlinkMacSystemFont,Segoe UI,Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue,sans-serif;font-weight:400;background-color:#FFFFFF;}@media (min-width:600px){body{font-size:1rem;}}@media print{body{background-color:#fff;}}body::backdrop{background-color:#FFFFFF;}</style></head><body><div hidden=""><!--$--><!--/$--></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><div>Loading...</div><!--/$--><script src="/search/_next/static/js/webpack.ccaf2443.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[52332,[\"9692\",\"static/js/9692.83f9877c.js\",\"421\",\"static/js/421.98ca62d9.js\",\"1759\",\"static/js/1759.b13f3ee1.js\",\"3535\",\"static/js/3535.878ceef2.js\",\"2619\",\"static/js/2619.b8db57ac.js\",\"3820\",\"static/js/3820.af314958.js\",\"574\",\"static/js/574.fd20103e.js\",\"5738\",\"static/js/5738.d28a9943.js\",\"7177\",\"static/js/app/layout.7ef30b9e.js\"],\"default\"]\n3:I[65380,[\"9692\",\"static/js/9692.83f9877c.js\",\"421\",\"static/js/421.98ca62d9.js\",\"1759\",\"static/js/1759.b13f3ee1.js\",\"3535\",\"static/js/3535.878ceef2.js\",\"2619\",\"static/js/2619.b8db57ac.js\",\"3820\",\"static/js/3820.af314958.js\",\"574\",\"static/js/574.fd20103e.js\",\"5738\",\"static/js/5738.d28a9943.js\",\"7177\",\"static/js/app/layout.7ef30b9e.js\"],\"AuthProvider\"]\n4:I[41627,[\"9692\",\"static/js/9692.83f9877c.js\",\"421\",\"static/js/421.98ca62d9.js\",\"1759\",\"static/js/1759.b13f3ee1.js\",\"3535\",\"static/js/3535.878ceef2.js\",\"2619\",\"static/js/2619.b8db57ac.js\",\"3820\",\"static/js/3820.af314958.js\",\"574\",\"static/js/574.fd20103e.js\",\"5738\",\"static/js/5738.d28a9943.js\",\"7177\",\"static/js/app/layout.7ef30b9e.js\"],\"FirebaseProvider\"]\n5:\"$Sreact.suspense\"\n6:I[92114,[\"9692\",\"static/js/9692.83f9877c.js\",\"421\",\"static/js/421.98ca62d9.js\",\"1759\",\"static/js/1759.b13f3ee1.js\",\"3535\",\"static/js/3535.878ceef2.js\",\"2619\",\"static/js/2619.b8db57ac.js\",\"3820\",\"static/js/3820.af314958.js\",\"574\",\"static/js/574.fd20103e.js\",\"5738\",\"static/js/5738.d28a9943.js\",\"7177\",\"static/js/app/layout.7ef30b9e.js\"],\"SearchProvider\"]\n7:I[94049,[\"9692\",\"static/js/9692.83f9877c.js\",\"421\",\"static/js/421.98ca62d9.js\",\"1759\",\"static/js/1759.b13f3ee1.js\",\"3535\",\"static/js/3535.878ceef2.js\",\"2619\",\"static/js/2619.b8db57ac.js\",\"3820\",\"static/js/3820.af314958.js\",\"574\",\"static/js/574.fd20103e.js\",\"5738\",\"static/js/5738.d28a9943.js\",\"7177\",\"static/js/app/layout.7ef30b9e.js\"],\"default\"]\n8:I[20190,[\"9692\",\"static/js/9692.83f9877c.js\",\"421\",\"static/js/421.98ca62d9.js\",\"1759\",\"static/js/1759.b13f3ee1.js\",\"3535\",\"static/js/3535.878ceef2.js\",\"2619\",\"static/js/2619.b8db57ac.js\",\"3820\",\"static/js/3820.af314958.js\",\"574\",\"static/j"])</script><script>self.__next_f.push([1,"s/574.fd20103e.js\",\"5738\",\"static/js/5738.d28a9943.js\",\"7177\",\"static/js/app/layout.7ef30b9e.js\"],\"default\"]\n9:I[9766,[],\"\"]\na:I[98924,[],\"\"]\nb:I[74744,[\"9692\",\"static/js/9692.83f9877c.js\",\"421\",\"static/js/421.98ca62d9.js\",\"1759\",\"static/js/1759.b13f3ee1.js\",\"3535\",\"static/js/3535.878ceef2.js\",\"2619\",\"static/js/2619.b8db57ac.js\",\"3820\",\"static/js/3820.af314958.js\",\"574\",\"static/js/574.fd20103e.js\",\"5738\",\"static/js/5738.d28a9943.js\",\"7177\",\"static/js/app/layout.7ef30b9e.js\"],\"ToastContainer\"]\nd:I[24431,[],\"OutletBoundary\"]\nf:I[15278,[],\"AsyncMetadataOutlet\"]\n11:I[24431,[],\"ViewportBoundary\"]\n14:I[57150,[],\"\"]\n:HL[\"/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/search/_next/static/css/e446a64f2ff89daf.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"WLCgvrrdknkKZDhW-WCbK\",\"p\":\"/search\",\"c\":[\"\",\"items\",\"probabilistic-representation-of-linguistic-knowledge-linguistic-data-sets-annotated-for-grammatical-acceptability\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"items\",{\"children\":[[\"slug\",\"probabilistic-representation-of-linguistic-knowledge-linguistic-data-sets-annotated-for-grammatical-acceptability\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/search/_next/static/css/e446a64f2ff89daf.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"meta\",null,{\"name\":\"emotion-insertion-point\",\"content\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://fonts.googleapis.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://fonts.gstatic.com\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://www.cataloguementalhealth.ac.uk\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://harmonydata.ac.uk\"}],[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n            /* Ensure immediate rendering with Roboto and fallbacks */\\n            * { \\n              font-family: \\\"Roboto\\\", -apple-system, BlinkMacSystemFont, \\\"Segoe UI\\\", \\\"Oxygen\\\", \\\"Ubuntu\\\", \\\"Cantarell\\\", \\\"Fira Sans\\\", \\\"Droid Sans\\\", \\\"Helvetica Neue\\\", sans-serif !important;\\n              font-display: swap;\\n              -webkit-font-smoothing: antialiased;\\n              -moz-osx-font-smoothing: grayscale;\\n            }\\n            body { \\n              visibility: visible !important; \\n              opacity: 1 !important; \\n              margin: 0; \\n              padding: 0; \\n            }\\n          \"}}]]}],[\"$\",\"body\",null,{\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"children\":[\"$\",\"$L4\",null,{\"children\":[\"$\",\"$5\",null,{\"fallback\":[\"$\",\"div\",null,{\"children\":\"Loading...\"}],\"children\":[\"$\",\"$L6\",null,{\"children\":[[\"$\",\"$L7\",null,{\"sx\":{\"display\":\"flex\",\"flexDirection\":{\"xs\":\"column\",\"md\":\"row\"}},\"children\":[[\"$\",\"$L8\",null,{}],[\"$\",\"$L7\",null,{\"component\":\"main\",\"sx\":{\"flexGrow\":1,\"ml\":{\"xs\":0,\"md\":\"72px\"},\"mt\":{\"xs\":\"64px\",\"md\":0},\"minHeight\":{\"xs\":\"calc(100vh - 64px)\",\"md\":\"100vh\"},\"width\":{\"xs\":\"100%\",\"md\":\"calc(100% - 72px)\"}},\"children\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}],[\"$\",\"$Lb\",null,{\"position\":\"bottom-right\"}]]}]}]}]}]}]}]]}]]}],{\"children\":[\"items\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"probabilistic-representation-of-linguistic-knowledge-linguistic-data-sets-annotated-for-grammatical-acceptability\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$Lc\",null,[\"$\",\"$Ld\",null,{\"children\":[\"$Le\",[\"$\",\"$Lf\",null,{\"promise\":\"$@10\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L11\",null,{\"children\":\"$L12\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],\"$L13\"]}],false]],\"m\":\"$undefined\",\"G\":[\"$14\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"15:I[24431,[],\"MetadataBoundary\"]\n13:[\"$\",\"$L15\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$5\",null,{\"fallback\":null,\"children\":\"$L16\"}]}]}]\n"])</script><script>self.__next_f.push([1,"17:I[41402,[\"9692\",\"static/js/9692.83f9877c.js\",\"1759\",\"static/js/1759.b13f3ee1.js\",\"690\",\"static/js/690.e023e61b.js\",\"3535\",\"static/js/3535.878ceef2.js\",\"5332\",\"static/js/5332.4ca3e6c6.js\",\"2410\",\"static/js/2410.ec36c5aa.js\",\"5183\",\"static/js/5183.9f1a7545.js\",\"5738\",\"static/js/5738.d28a9943.js\",\"3055\",\"static/js/3055.8a1757af.js\",\"8977\",\"static/js/8977.d520dad7.js\",\"6387\",\"static/js/app/items/%5Bslug%5D/page.0819d17d.js\"],\"\"]\n19:I[78977,[\"9692\",\"static/js/9692.83f9877c.js\",\"1759\",\"static/js/1759.b13f3ee1.js\",\"690\",\"static/js/690.e023e61b.js\",\"3535\",\"static/js/3535.878ceef2.js\",\"5332\",\"static/js/5332.4ca3e6c6.js\",\"2410\",\"static/js/2410.ec36c5aa.js\",\"5183\",\"static/js/5183.9f1a7545.js\",\"5738\",\"static/js/5738.d28a9943.js\",\"3055\",\"static/js/3055.8a1757af.js\",\"8977\",\"static/js/8977.d520dad7.js\",\"6387\",\"static/js/app/items/%5Bslug%5D/page.0819d17d.js\"],\"default\"]\n18:Tb9b,"])</script><script>self.__next_f.push([1,"{\"@context\":\"https://schema.org/\",\"@type\":\"Dataset\",\"name\":\"The probabilistic representation of linguistic knowledge: Linguistic data sets annotated for grammatical acceptability\",\"description\":\"The files contain crowd sourced (Amazon Mechanical Turk) speaker annotated sentences in several domains, and for several languages. The annotations are mean acceptability judgements in several modes of presentation. \\n\\nFull documentation of the experimental protocols through which the annotation of these data sets was obtained is provided on the Statistical Models of Grammaticality website (SMOG), please see the related resources section to access the (SMOG) website.\\n\\nThis data collection contains the linguistic data sets in excel, and two papers which explain the project and data and experiments in greater detail.SMOG is exploring the construction of an enriched stochastic model that represents the syntactic knowledge that native speakers of English have of their language.\\n\\nWe are hoping that this kind of model will provide a straightforward explanation for the fact that individual native speakers generally judge the well formedness of sentences along a continuum, rather than through the imposition of a sharp boundary between acceptable and unacceptable sentences.\\n\\nWe are experimenting with different sorts of language models that contain a variety of parameters encoding properties of sentences and probability distributions over corpora.\\n\\nWe are training these models on subsets of the British National Corpus (BNC), and we are testing them on additional subsets of the BNC into which we have introduced grammatical deformations and infelicities of varying degrees of severity and subtlety.\\n\\nWe hope to show that a sufficiently complex enriched language model can encode a fair amount of what native speakers know about the syntax of their language.\\nThis research holds out the prospect of important impact in two areas.\\n(1) It can shed light on the relationship between the representation and acquisition of linguistic knowledge on one hand, and learning and the encoding of knowledge in other cognitive domains. This can, in turn, help to clarify the respective roles of biologically conditioned learning biases and data driven learning in human cognition.\\n(2) This work can contribute to the development of more effective language technology by providing insight into the way in which humans represent the syntactic properties of sentences in their language. To the extent that natural language processing systems take account of this class of representations they will provide more efficient tools for parsing and interpreting text and speech.\",\"url\":\"https://harmonydata.ac.uk/search/items/probabilistic-representation-of-linguistic-knowledge-linguistic-data-sets-annotated-for-grammatical-acceptability\",\"identifier\":[\"http://dx.doi.org/10.5255/UKDA-SN-851856\"],\"keywords\":[\"LINGUISTIC DATA\"],\"temporalCoverage\":\"2012-10-01/2015-09-30\"}"])</script><script>self.__next_f.push([1,"1a:T9a2,"])</script><script>self.__next_f.push([1,"The files contain crowd sourced (Amazon Mechanical Turk) speaker annotated sentences in several domains, and for several languages. The annotations are mean acceptability judgements in several modes of presentation. \n\nFull documentation of the experimental protocols through which the annotation of these data sets was obtained is provided on the Statistical Models of Grammaticality website (SMOG), please see the related resources section to access the (SMOG) website.\n\nThis data collection contains the linguistic data sets in excel, and two papers which explain the project and data and experiments in greater detail.SMOG is exploring the construction of an enriched stochastic model that represents the syntactic knowledge that native speakers of English have of their language.\n\nWe are hoping that this kind of model will provide a straightforward explanation for the fact that individual native speakers generally judge the well formedness of sentences along a continuum, rather than through the imposition of a sharp boundary between acceptable and unacceptable sentences.\n\nWe are experimenting with different sorts of language models that contain a variety of parameters encoding properties of sentences and probability distributions over corpora.\n\nWe are training these models on subsets of the British National Corpus (BNC), and we are testing them on additional subsets of the BNC into which we have introduced grammatical deformations and infelicities of varying degrees of severity and subtlety.\n\nWe hope to show that a sufficiently complex enriched language model can encode a fair amount of what native speakers know about the syntax of their language.\nThis research holds out the prospect of important impact in two areas.\n(1) It can shed light on the relationship between the representation and acquisition of linguistic knowledge on one hand, and learning and the encoding of knowledge in other cognitive domains. This can, in turn, help to clarify the respective roles of biologically conditioned learning biases and data driven learning in human cognition.\n(2) This work can contribute to the development of more effective language technology by providing insight into the way in which humans represent the syntactic properties of sentences in their language. To the extent that natural language processing systems take account of this class of representations they will provide more efficient tools for parsing and interpreting text and speech."])</script><script>self.__next_f.push([1,"c:[\"$\",\"$5\",null,{\"fallback\":[\"$\",\"div\",null,{\"children\":\"Loading...\"}],\"children\":[[\"$\",\"$L17\",null,{\"strategy\":\"beforeInteractive\",\"id\":\"structured-data\",\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"$18\"}}],[\"$\",\"$L19\",null,{\"study\":{\"dataset_schema\":{\"@context\":\"https://schema.org/\",\"@type\":\"Dataset\",\"name\":\"The probabilistic representation of linguistic knowledge: Linguistic data sets annotated for grammatical acceptability\",\"description\":\"$1a\",\"url\":[\"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=851856\",\"https://reshare.ukdataservice.ac.uk/851856\"],\"keywords\":[\"LINGUISTIC DATA\"],\"identifier\":[\"http://dx.doi.org/10.5255/UKDA-SN-851856\"],\"includedInDataCatalog\":[{\"@type\":\"DataCatalog\",\"name\":\"UK Data Service\",\"url\":\"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=851856\"}],\"sponsor\":[{\"@type\":\"Organization\",\"name\":\"Economic and Social Research Council\"}],\"temporalCoverage\":\"2012-10-01/2015-09-30\"},\"extra_data\":{\"study_design\":[],\"start_year\":2012,\"harmony_id\":\"ukds/851856\",\"end_year\":2015,\"data_access\":null,\"urls\":[\"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=851856\",\"https://reshare.ukdataservice.ac.uk/851856\"],\"name\":\"The probabilistic representation of linguistic knowledge: Linguistic data sets annotated for grammatical acceptability\",\"source\":[\"ukds\"],\"genetic_data_collected\":false,\"slug\":\"probabilistic-representation-of-linguistic-knowledge-linguistic-data-sets-annotated-for-grammatical-acceptability\",\"geographic_coverage\":\"Crowd sourced through Amazon Mechanical Turk\",\"language_codes\":[\"en\"],\"dois\":[\"http://dx.doi.org/10.5255/UKDA-SN-851856\"],\"sex\":\"male\",\"instruments\":[],\"num_variables\":null,\"ai_summary\":null,\"resource_type\":\"dataset\",\"duration_years\":3,\"country_codes\":[\"GB\"],\"uuid\":\"8a5c0c44952e6b4c85efd3969b87e4c1\"},\"distance\":0,\"score\":0,\"parent\":{},\"ancestors\":[]}}]]}]\n"])</script><script>self.__next_f.push([1,"12:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\ne:null\n"])</script><script>self.__next_f.push([1,"1b:T9a2,"])</script><script>self.__next_f.push([1,"The files contain crowd sourced (Amazon Mechanical Turk) speaker annotated sentences in several domains, and for several languages. The annotations are mean acceptability judgements in several modes of presentation. \n\nFull documentation of the experimental protocols through which the annotation of these data sets was obtained is provided on the Statistical Models of Grammaticality website (SMOG), please see the related resources section to access the (SMOG) website.\n\nThis data collection contains the linguistic data sets in excel, and two papers which explain the project and data and experiments in greater detail.SMOG is exploring the construction of an enriched stochastic model that represents the syntactic knowledge that native speakers of English have of their language.\n\nWe are hoping that this kind of model will provide a straightforward explanation for the fact that individual native speakers generally judge the well formedness of sentences along a continuum, rather than through the imposition of a sharp boundary between acceptable and unacceptable sentences.\n\nWe are experimenting with different sorts of language models that contain a variety of parameters encoding properties of sentences and probability distributions over corpora.\n\nWe are training these models on subsets of the British National Corpus (BNC), and we are testing them on additional subsets of the BNC into which we have introduced grammatical deformations and infelicities of varying degrees of severity and subtlety.\n\nWe hope to show that a sufficiently complex enriched language model can encode a fair amount of what native speakers know about the syntax of their language.\nThis research holds out the prospect of important impact in two areas.\n(1) It can shed light on the relationship between the representation and acquisition of linguistic knowledge on one hand, and learning and the encoding of knowledge in other cognitive domains. This can, in turn, help to clarify the respective roles of biologically conditioned learning biases and data driven learning in human cognition.\n(2) This work can contribute to the development of more effective language technology by providing insight into the way in which humans represent the syntactic properties of sentences in their language. To the extent that natural language processing systems take account of this class of representations they will provide more efficient tools for parsing and interpreting text and speech."])</script><script>self.__next_f.push([1,"1c:T9a2,"])</script><script>self.__next_f.push([1,"The files contain crowd sourced (Amazon Mechanical Turk) speaker annotated sentences in several domains, and for several languages. The annotations are mean acceptability judgements in several modes of presentation. \n\nFull documentation of the experimental protocols through which the annotation of these data sets was obtained is provided on the Statistical Models of Grammaticality website (SMOG), please see the related resources section to access the (SMOG) website.\n\nThis data collection contains the linguistic data sets in excel, and two papers which explain the project and data and experiments in greater detail.SMOG is exploring the construction of an enriched stochastic model that represents the syntactic knowledge that native speakers of English have of their language.\n\nWe are hoping that this kind of model will provide a straightforward explanation for the fact that individual native speakers generally judge the well formedness of sentences along a continuum, rather than through the imposition of a sharp boundary between acceptable and unacceptable sentences.\n\nWe are experimenting with different sorts of language models that contain a variety of parameters encoding properties of sentences and probability distributions over corpora.\n\nWe are training these models on subsets of the British National Corpus (BNC), and we are testing them on additional subsets of the BNC into which we have introduced grammatical deformations and infelicities of varying degrees of severity and subtlety.\n\nWe hope to show that a sufficiently complex enriched language model can encode a fair amount of what native speakers know about the syntax of their language.\nThis research holds out the prospect of important impact in two areas.\n(1) It can shed light on the relationship between the representation and acquisition of linguistic knowledge on one hand, and learning and the encoding of knowledge in other cognitive domains. This can, in turn, help to clarify the respective roles of biologically conditioned learning biases and data driven learning in human cognition.\n(2) This work can contribute to the development of more effective language technology by providing insight into the way in which humans represent the syntactic properties of sentences in their language. To the extent that natural language processing systems take account of this class of representations they will provide more efficient tools for parsing and interpreting text and speech."])</script><script>self.__next_f.push([1,"10:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"The probabilistic representation of linguistic knowledge: Linguistic data sets annotated for grammatical acceptability\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"$1b\"}],[\"$\",\"meta\",\"2\",{\"property\":\"og:title\",\"content\":\"The probabilistic representation of linguistic knowledge: Linguistic data sets annotated for grammatical acceptability\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:description\",\"content\":\"$1c\"}],\"$L1d\",\"$L1e\",\"$L1f\",\"$L20\",\"$L21\",\"$L22\",\"$L23\",\"$L24\",\"$L25\",\"$L26\",\"$L27\",\"$L28\",\"$L29\",\"$L2a\"],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"2c:I[80622,[],\"IconMark\"]\n1d:[\"$\",\"meta\",\"4\",{\"property\":\"og:url\",\"content\":\"https://harmonydata.ac.uk/search/items/probabilistic-representation-of-linguistic-knowledge-linguistic-data-sets-annotated-for-grammatical-acceptability\"}]\n1e:[\"$\",\"meta\",\"5\",{\"property\":\"og:site_name\",\"content\":\"Academic Resource Discovery\"}]\n1f:[\"$\",\"meta\",\"6\",{\"property\":\"og:locale\",\"content\":\"en_US\"}]\n20:[\"$\",\"meta\",\"7\",{\"property\":\"og:image\",\"content\":\"https://harmonydata.ac.uk/search/harmony.png\"}]\n21:[\"$\",\"meta\",\"8\",{\"property\":\"og:image:width\",\"content\":\"1200\"}]\n22:[\"$\",\"meta\",\"9\",{\"property\":\"og:image:height\",\"content\":\"630\"}]\n23:[\"$\",\"meta\",\"10\",{\"property\":\"og:image:alt\",\"content\":\"The probabilistic representation of linguistic knowledge: Linguistic data sets annotated for grammatical acceptability\"}]\n24:[\"$\",\"meta\",\"11\",{\"property\":\"og:type\",\"content\":\"website\"}]\n25:[\"$\",\"meta\",\"12\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}]\n26:[\"$\",\"meta\",\"13\",{\"name\":\"twitter:title\",\"content\":\"The probabilistic representation of linguistic knowledge: Linguistic data sets annotated for grammatical acceptability\"}]\n2b:T9a2,"])</script><script>self.__next_f.push([1,"The files contain crowd sourced (Amazon Mechanical Turk) speaker annotated sentences in several domains, and for several languages. The annotations are mean acceptability judgements in several modes of presentation. \n\nFull documentation of the experimental protocols through which the annotation of these data sets was obtained is provided on the Statistical Models of Grammaticality website (SMOG), please see the related resources section to access the (SMOG) website.\n\nThis data collection contains the linguistic data sets in excel, and two papers which explain the project and data and experiments in greater detail.SMOG is exploring the construction of an enriched stochastic model that represents the syntactic knowledge that native speakers of English have of their language.\n\nWe are hoping that this kind of model will provide a straightforward explanation for the fact that individual native speakers generally judge the well formedness of sentences along a continuum, rather than through the imposition of a sharp boundary between acceptable and unacceptable sentences.\n\nWe are experimenting with different sorts of language models that contain a variety of parameters encoding properties of sentences and probability distributions over corpora.\n\nWe are training these models on subsets of the British National Corpus (BNC), and we are testing them on additional subsets of the BNC into which we have introduced grammatical deformations and infelicities of varying degrees of severity and subtlety.\n\nWe hope to show that a sufficiently complex enriched language model can encode a fair amount of what native speakers know about the syntax of their language.\nThis research holds out the prospect of important impact in two areas.\n(1) It can shed light on the relationship between the representation and acquisition of linguistic knowledge on one hand, and learning and the encoding of knowledge in other cognitive domains. This can, in turn, help to clarify the respective roles of biologically conditioned learning biases and data driven learning in human cognition.\n(2) This work can contribute to the development of more effective language technology by providing insight into the way in which humans represent the syntactic properties of sentences in their language. To the extent that natural language processing systems take account of this class of representations they will provide more efficient tools for parsing and interpreting text and speech."])</script><script>self.__next_f.push([1,"27:[\"$\",\"meta\",\"14\",{\"name\":\"twitter:description\",\"content\":\"$2b\"}]\n28:[\"$\",\"meta\",\"15\",{\"name\":\"twitter:image\",\"content\":\"https://harmonydata.ac.uk/search/harmony.png\"}]\n29:[\"$\",\"link\",\"16\",{\"rel\":\"icon\",\"href\":\"/search/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}]\n2a:[\"$\",\"$L2c\",\"17\",{}]\n16:\"$10:metadata\"\n"])</script></body></html>