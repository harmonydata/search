1:"$Sreact.fragment"
2:I[82104,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","4398","static/js/4398.8644925b.js","7177","static/js/app/layout.0819bb7e.js"],"default"]
3:I[17146,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","4398","static/js/4398.8644925b.js","7177","static/js/app/layout.0819bb7e.js"],"AuthProvider"]
4:I[83705,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","4398","static/js/4398.8644925b.js","7177","static/js/app/layout.0819bb7e.js"],"FirebaseProvider"]
5:"$Sreact.suspense"
6:I[63612,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","4398","static/js/4398.8644925b.js","7177","static/js/app/layout.0819bb7e.js"],"SearchProvider"]
7:I[68998,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","4398","static/js/4398.8644925b.js","7177","static/js/app/layout.0819bb7e.js"],"default"]
8:I[98904,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","4398","static/js/4398.8644925b.js","7177","static/js/app/layout.0819bb7e.js"],"default"]
9:I[15244,[],""]
a:I[43866,[],""]
b:I[14046,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","4398","static/js/4398.8644925b.js","7177","static/js/app/layout.0819bb7e.js"],"ToastContainer"]
d:I[86213,[],"OutletBoundary"]
f:I[86213,[],"MetadataBoundary"]
11:I[86213,[],"ViewportBoundary"]
13:I[34835,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/0d5b820fee8240e5.css","style"]
0:{"P":null,"b":"hHFVRgYXxM8of7-CzK8yz","p":"/search","c":["","items","shaping-multilingual-access-through-respeaking-technology-project-data-2021"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","shaping-multilingual-access-through-respeaking-technology-project-data-2021","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/0d5b820fee8240e5.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":[["$","meta",null,{"name":"emotion-insertion-point","content":""}],["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"preconnect","href":"https://www.cataloguementalhealth.ac.uk"}],["$","link",null,{"rel":"dns-prefetch","href":"https://harmonydata.ac.uk"}],["$","style",null,{"dangerouslySetInnerHTML":{"__html":"\n            /* Ensure immediate rendering with Roboto and fallbacks */\n            * { \n              font-family: \"Roboto\", -apple-system, BlinkMacSystemFont, \"Segoe UI\", \"Oxygen\", \"Ubuntu\", \"Cantarell\", \"Fira Sans\", \"Droid Sans\", \"Helvetica Neue\", sans-serif !important;\n              font-display: swap;\n              -webkit-font-smoothing: antialiased;\n              -moz-osx-font-smoothing: grayscale;\n            }\n            body { \n              visibility: visible !important; \n              opacity: 1 !important; \n              margin: 0; \n              padding: 0; \n            }\n          "}}]]}],["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":["$","$5",null,{"fallback":["$","div",null,{"children":"Loading..."}],"children":["$","$L6",null,{"children":[["$","$L7",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L8",null,{}],["$","$L7",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L9",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$Lb",null,{"position":"bottom-right"}]]}]}]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L9",null,{"parallelRouterKey":"children","segmentPath":["children","items","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","shaping-multilingual-access-through-respeaking-technology-project-data-2021","d"],["$","$1","c",{"children":[null,["$","$L9",null,{"parallelRouterKey":"children","segmentPath":["children","items","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$Lc",null,["$","$Ld",null,{"children":"$Le"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","BdkIZYSeOoisgFHgH9P-M",{"children":[["$","$Lf",null,{"children":"$L10"}],["$","$L11",null,{"children":"$L12"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$13","$undefined"],"s":false,"S":true}
14:I[53704,["6586","static/js/6586.2e946dbf.js","8378","static/js/8378.a1bea36e.js","2282","static/js/2282.e20001b9.js","5135","static/js/5135.b8bfc30e.js","9387","static/js/9387.65629b75.js","2649","static/js/2649.95608f08.js","4398","static/js/4398.8644925b.js","1857","static/js/1857.a01744c0.js","7626","static/js/7626.2947408f.js","6387","static/js/app/items/%5Bslug%5D/page.0f65d92f.js"],""]
16:I[77626,["6586","static/js/6586.2e946dbf.js","8378","static/js/8378.a1bea36e.js","2282","static/js/2282.e20001b9.js","5135","static/js/5135.b8bfc30e.js","9387","static/js/9387.65629b75.js","2649","static/js/2649.95608f08.js","4398","static/js/4398.8644925b.js","1857","static/js/1857.a01744c0.js","7626","static/js/7626.2947408f.js","6387","static/js/app/items/%5Bslug%5D/page.0f65d92f.js"],"default"]
15:T1404,{"@context":"https://schema.org/","@type":"Dataset","name":"Shaping Multilingual Access through Respeaking Technology, Project Data, 2021","description":"The recent global surge in audiovisual content has emphasised the importance of accessibility for wider audiences. The Shaping Multilingual Access through Respeaking Technology (SMART) project addressed this by exploring interlingual respeaking, a novel practice combining speech recognition technology with human interpreting and subtitling skills to produce real-time speech-to-text services across languages. This method evolved from intralingual respeaking, which is widely used in broadcasting to create live subtitles for the deaf and hard-of-hearing. Interlingual respeaking, which involves translating live content into another language and subtitling it, could revolutionise subtitle production for foreign-language content, overcoming sensory and language barriers. Interlingual respeaking involves two shifts: interlingual (from one language to another) and intermodal (from spoken to written). This practice combines the challenges of simultaneous interpreting with the requirements of subtitling. Respeakers must accurately convey messages in another language to a speech recognition system, adding punctuation and making real-time edits for clarity and readability. This method leverages speech recognition technology and human translation skills to ensure efficient and high-quality translated subtitles. Interlingual respeaking offers immense potential for making multilingual content accessible to international and hearing-impaired audiences. It is particularly relevant for television, conferences, and live events. However, research into this practice is still in its early stages. The SMART project's main goals were to study interlingual respeaking's complexity, focusing on the acquisition and implementation of relevant skills (not only procedural, but also cognitive and interpersonal ones), and the accuracy of the final subtitles. The research involved fifty-one language professionals with backgrounds in relevant language-related practices (namely interpreting, translation, subtitling, and intralingual respeaking). The research programme examined three areas: process, product, and upskilling. It sought to understand the variables contributing to language professionals' performance, challenges faced during performance, and how performance can be sustained. Regarding the product, it aimed to identify factors affecting the accuracy of interlingual respeaking and the impact of various individual and content characteristics on accuracy. For upskilling, the focus was on the challenges and strengths of the course delivered as part of the experiment. Key findings included the importance of working memory in predicting high performance and the enhancement of certain cognitive abilities through training. Interpersonal traits like conscientiousness and integrated regulation were also examined. In terms of product accuracy, the average achieved across 153 performances three per participant) after 25h of upskilling was 95.37%, with omissions being the strongest negative predictor of accuracy. High performers outperformed low performers across all scenarios. The upskilling course was innovative, focusing on modular training and combining intralingual and interlingual practices. It addressed real-world challenges and was tailored to different professional backgrounds. The approach proved effective, with 82% of participants finding the course met their expectations and 86% acknowledging its challenging nature. The study confirmed the benefits of a modular and personalised training approach, highlighting the need for flexibility and adaptability to different skill levels and backgrounds.Our day and age is characterised by a proliferation of live multimedia and multilingual content, such as news and TV programmes. Such content is not accessible to everyone. In many countries, live subtitles in the same language are required by law to enable deaf and hard-of-hearing audiences to enjoy access to information, culture and entertainment. In the UK and other countries (e.g., Canada, Spain, Switzerland), intralingual respeaking is the most well-established technique to produce these subtitles: it relies on human-machine interaction whereby a respeaker listens to the original sound of a live programme or event and simultaneously dictates it to a speech recognition software that turns each sentence into subtitles displayed on the screen.\n\nRespeaking so far has been used to produce intralingual subtitles, i.e., in the same language. Given the current multilingual content boom, the SMART project aims to investigate whether respeaking can be used to produce interlingual subtitles, i.e. in a different language.","url":"https://harmonydata.ac.uk/search/items/shaping-multilingual-access-through-respeaking-technology-project-data-2021","identifier":["http://dx.doi.org/10.5255/UKDA-SN-856687"],"keywords":["QUALITY","QUANTITY","COGNITIVE PROCESSES","PRODUCTS","PSYCHOLOGY"],"temporalCoverage":"2021-04-01/2021-12-31"}17:T1236,The recent global surge in audiovisual content has emphasised the importance of accessibility for wider audiences. The Shaping Multilingual Access through Respeaking Technology (SMART) project addressed this by exploring interlingual respeaking, a novel practice combining speech recognition technology with human interpreting and subtitling skills to produce real-time speech-to-text services across languages. This method evolved from intralingual respeaking, which is widely used in broadcasting to create live subtitles for the deaf and hard-of-hearing. Interlingual respeaking, which involves translating live content into another language and subtitling it, could revolutionise subtitle production for foreign-language content, overcoming sensory and language barriers. Interlingual respeaking involves two shifts: interlingual (from one language to another) and intermodal (from spoken to written). This practice combines the challenges of simultaneous interpreting with the requirements of subtitling. Respeakers must accurately convey messages in another language to a speech recognition system, adding punctuation and making real-time edits for clarity and readability. This method leverages speech recognition technology and human translation skills to ensure efficient and high-quality translated subtitles. Interlingual respeaking offers immense potential for making multilingual content accessible to international and hearing-impaired audiences. It is particularly relevant for television, conferences, and live events. However, research into this practice is still in its early stages. The SMART project's main goals were to study interlingual respeaking's complexity, focusing on the acquisition and implementation of relevant skills (not only procedural, but also cognitive and interpersonal ones), and the accuracy of the final subtitles. The research involved fifty-one language professionals with backgrounds in relevant language-related practices (namely interpreting, translation, subtitling, and intralingual respeaking). The research programme examined three areas: process, product, and upskilling. It sought to understand the variables contributing to language professionals' performance, challenges faced during performance, and how performance can be sustained. Regarding the product, it aimed to identify factors affecting the accuracy of interlingual respeaking and the impact of various individual and content characteristics on accuracy. For upskilling, the focus was on the challenges and strengths of the course delivered as part of the experiment. Key findings included the importance of working memory in predicting high performance and the enhancement of certain cognitive abilities through training. Interpersonal traits like conscientiousness and integrated regulation were also examined. In terms of product accuracy, the average achieved across 153 performances three per participant) after 25h of upskilling was 95.37%, with omissions being the strongest negative predictor of accuracy. High performers outperformed low performers across all scenarios. The upskilling course was innovative, focusing on modular training and combining intralingual and interlingual practices. It addressed real-world challenges and was tailored to different professional backgrounds. The approach proved effective, with 82% of participants finding the course met their expectations and 86% acknowledging its challenging nature. The study confirmed the benefits of a modular and personalised training approach, highlighting the need for flexibility and adaptability to different skill levels and backgrounds.Our day and age is characterised by a proliferation of live multimedia and multilingual content, such as news and TV programmes. Such content is not accessible to everyone. In many countries, live subtitles in the same language are required by law to enable deaf and hard-of-hearing audiences to enjoy access to information, culture and entertainment. In the UK and other countries (e.g., Canada, Spain, Switzerland), intralingual respeaking is the most well-established technique to produce these subtitles: it relies on human-machine interaction whereby a respeaker listens to the original sound of a live programme or event and simultaneously dictates it to a speech recognition software that turns each sentence into subtitles displayed on the screen.

Respeaking so far has been used to produce intralingual subtitles, i.e., in the same language. Given the current multilingual content boom, the SMART project aims to investigate whether respeaking can be used to produce interlingual subtitles, i.e. in a different language.c:["$","$5",null,{"fallback":["$","div",null,{"children":"Loading..."}],"children":[["$","$L14",null,{"strategy":"beforeInteractive","id":"structured-data","type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$15"}}],["$","$L16",null,{"study":{"dataset_schema":{"@context":"https://schema.org/","@type":"Dataset","name":"Shaping Multilingual Access through Respeaking Technology, Project Data, 2021","description":"$17","url":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=856687","https://reshare.ukdataservice.ac.uk/856687"],"keywords":["QUALITY","QUANTITY","COGNITIVE PROCESSES","PRODUCTS","PSYCHOLOGY"],"identifier":["http://dx.doi.org/10.5255/UKDA-SN-856687"],"includedInDataCatalog":[{"@type":"DataCatalog","name":"UK Data Service","url":"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=856687"}],"sponsor":[{"@type":"Organization","name":"ESRC"}],"temporalCoverage":"2021-04-01/2021-12-31"},"extra_data":{"harmony_id":"ukds/856687","data_access":"The Data Collection is available for download to users registered with the UK Data Service. Commercial Use of data is not permitted.","genetic_data_collected":false,"language_codes":["en"],"urls":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=856687","https://reshare.ukdataservice.ac.uk/856687"],"sex":"all","source":["ukds"],"slug":"shaping-multilingual-access-through-respeaking-technology-project-data-2021","geographic_coverage":"Online data collection","duration_years":null,"resource_type":"dataset","ai_summary":null,"instruments":[],"study_design":[],"dois":["http://dx.doi.org/10.5255/UKDA-SN-856687"],"name":"Shaping Multilingual Access through Respeaking Technology, Project Data, 2021","country_codes":["GB"],"num_variables":null,"start_year":2021,"end_year":2021,"uuid":"d3b165e343af5c38b8b96a08c3b03386"},"distance":0,"score":0,"parent":{},"ancestors":[]}}]]}]
12:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
18:T1236,The recent global surge in audiovisual content has emphasised the importance of accessibility for wider audiences. The Shaping Multilingual Access through Respeaking Technology (SMART) project addressed this by exploring interlingual respeaking, a novel practice combining speech recognition technology with human interpreting and subtitling skills to produce real-time speech-to-text services across languages. This method evolved from intralingual respeaking, which is widely used in broadcasting to create live subtitles for the deaf and hard-of-hearing. Interlingual respeaking, which involves translating live content into another language and subtitling it, could revolutionise subtitle production for foreign-language content, overcoming sensory and language barriers. Interlingual respeaking involves two shifts: interlingual (from one language to another) and intermodal (from spoken to written). This practice combines the challenges of simultaneous interpreting with the requirements of subtitling. Respeakers must accurately convey messages in another language to a speech recognition system, adding punctuation and making real-time edits for clarity and readability. This method leverages speech recognition technology and human translation skills to ensure efficient and high-quality translated subtitles. Interlingual respeaking offers immense potential for making multilingual content accessible to international and hearing-impaired audiences. It is particularly relevant for television, conferences, and live events. However, research into this practice is still in its early stages. The SMART project's main goals were to study interlingual respeaking's complexity, focusing on the acquisition and implementation of relevant skills (not only procedural, but also cognitive and interpersonal ones), and the accuracy of the final subtitles. The research involved fifty-one language professionals with backgrounds in relevant language-related practices (namely interpreting, translation, subtitling, and intralingual respeaking). The research programme examined three areas: process, product, and upskilling. It sought to understand the variables contributing to language professionals' performance, challenges faced during performance, and how performance can be sustained. Regarding the product, it aimed to identify factors affecting the accuracy of interlingual respeaking and the impact of various individual and content characteristics on accuracy. For upskilling, the focus was on the challenges and strengths of the course delivered as part of the experiment. Key findings included the importance of working memory in predicting high performance and the enhancement of certain cognitive abilities through training. Interpersonal traits like conscientiousness and integrated regulation were also examined. In terms of product accuracy, the average achieved across 153 performances three per participant) after 25h of upskilling was 95.37%, with omissions being the strongest negative predictor of accuracy. High performers outperformed low performers across all scenarios. The upskilling course was innovative, focusing on modular training and combining intralingual and interlingual practices. It addressed real-world challenges and was tailored to different professional backgrounds. The approach proved effective, with 82% of participants finding the course met their expectations and 86% acknowledging its challenging nature. The study confirmed the benefits of a modular and personalised training approach, highlighting the need for flexibility and adaptability to different skill levels and backgrounds.Our day and age is characterised by a proliferation of live multimedia and multilingual content, such as news and TV programmes. Such content is not accessible to everyone. In many countries, live subtitles in the same language are required by law to enable deaf and hard-of-hearing audiences to enjoy access to information, culture and entertainment. In the UK and other countries (e.g., Canada, Spain, Switzerland), intralingual respeaking is the most well-established technique to produce these subtitles: it relies on human-machine interaction whereby a respeaker listens to the original sound of a live programme or event and simultaneously dictates it to a speech recognition software that turns each sentence into subtitles displayed on the screen.

Respeaking so far has been used to produce intralingual subtitles, i.e., in the same language. Given the current multilingual content boom, the SMART project aims to investigate whether respeaking can be used to produce interlingual subtitles, i.e. in a different language.19:T1236,The recent global surge in audiovisual content has emphasised the importance of accessibility for wider audiences. The Shaping Multilingual Access through Respeaking Technology (SMART) project addressed this by exploring interlingual respeaking, a novel practice combining speech recognition technology with human interpreting and subtitling skills to produce real-time speech-to-text services across languages. This method evolved from intralingual respeaking, which is widely used in broadcasting to create live subtitles for the deaf and hard-of-hearing. Interlingual respeaking, which involves translating live content into another language and subtitling it, could revolutionise subtitle production for foreign-language content, overcoming sensory and language barriers. Interlingual respeaking involves two shifts: interlingual (from one language to another) and intermodal (from spoken to written). This practice combines the challenges of simultaneous interpreting with the requirements of subtitling. Respeakers must accurately convey messages in another language to a speech recognition system, adding punctuation and making real-time edits for clarity and readability. This method leverages speech recognition technology and human translation skills to ensure efficient and high-quality translated subtitles. Interlingual respeaking offers immense potential for making multilingual content accessible to international and hearing-impaired audiences. It is particularly relevant for television, conferences, and live events. However, research into this practice is still in its early stages. The SMART project's main goals were to study interlingual respeaking's complexity, focusing on the acquisition and implementation of relevant skills (not only procedural, but also cognitive and interpersonal ones), and the accuracy of the final subtitles. The research involved fifty-one language professionals with backgrounds in relevant language-related practices (namely interpreting, translation, subtitling, and intralingual respeaking). The research programme examined three areas: process, product, and upskilling. It sought to understand the variables contributing to language professionals' performance, challenges faced during performance, and how performance can be sustained. Regarding the product, it aimed to identify factors affecting the accuracy of interlingual respeaking and the impact of various individual and content characteristics on accuracy. For upskilling, the focus was on the challenges and strengths of the course delivered as part of the experiment. Key findings included the importance of working memory in predicting high performance and the enhancement of certain cognitive abilities through training. Interpersonal traits like conscientiousness and integrated regulation were also examined. In terms of product accuracy, the average achieved across 153 performances three per participant) after 25h of upskilling was 95.37%, with omissions being the strongest negative predictor of accuracy. High performers outperformed low performers across all scenarios. The upskilling course was innovative, focusing on modular training and combining intralingual and interlingual practices. It addressed real-world challenges and was tailored to different professional backgrounds. The approach proved effective, with 82% of participants finding the course met their expectations and 86% acknowledging its challenging nature. The study confirmed the benefits of a modular and personalised training approach, highlighting the need for flexibility and adaptability to different skill levels and backgrounds.Our day and age is characterised by a proliferation of live multimedia and multilingual content, such as news and TV programmes. Such content is not accessible to everyone. In many countries, live subtitles in the same language are required by law to enable deaf and hard-of-hearing audiences to enjoy access to information, culture and entertainment. In the UK and other countries (e.g., Canada, Spain, Switzerland), intralingual respeaking is the most well-established technique to produce these subtitles: it relies on human-machine interaction whereby a respeaker listens to the original sound of a live programme or event and simultaneously dictates it to a speech recognition software that turns each sentence into subtitles displayed on the screen.

Respeaking so far has been used to produce intralingual subtitles, i.e., in the same language. Given the current multilingual content boom, the SMART project aims to investigate whether respeaking can be used to produce interlingual subtitles, i.e. in a different language.1a:T1236,The recent global surge in audiovisual content has emphasised the importance of accessibility for wider audiences. The Shaping Multilingual Access through Respeaking Technology (SMART) project addressed this by exploring interlingual respeaking, a novel practice combining speech recognition technology with human interpreting and subtitling skills to produce real-time speech-to-text services across languages. This method evolved from intralingual respeaking, which is widely used in broadcasting to create live subtitles for the deaf and hard-of-hearing. Interlingual respeaking, which involves translating live content into another language and subtitling it, could revolutionise subtitle production for foreign-language content, overcoming sensory and language barriers. Interlingual respeaking involves two shifts: interlingual (from one language to another) and intermodal (from spoken to written). This practice combines the challenges of simultaneous interpreting with the requirements of subtitling. Respeakers must accurately convey messages in another language to a speech recognition system, adding punctuation and making real-time edits for clarity and readability. This method leverages speech recognition technology and human translation skills to ensure efficient and high-quality translated subtitles. Interlingual respeaking offers immense potential for making multilingual content accessible to international and hearing-impaired audiences. It is particularly relevant for television, conferences, and live events. However, research into this practice is still in its early stages. The SMART project's main goals were to study interlingual respeaking's complexity, focusing on the acquisition and implementation of relevant skills (not only procedural, but also cognitive and interpersonal ones), and the accuracy of the final subtitles. The research involved fifty-one language professionals with backgrounds in relevant language-related practices (namely interpreting, translation, subtitling, and intralingual respeaking). The research programme examined three areas: process, product, and upskilling. It sought to understand the variables contributing to language professionals' performance, challenges faced during performance, and how performance can be sustained. Regarding the product, it aimed to identify factors affecting the accuracy of interlingual respeaking and the impact of various individual and content characteristics on accuracy. For upskilling, the focus was on the challenges and strengths of the course delivered as part of the experiment. Key findings included the importance of working memory in predicting high performance and the enhancement of certain cognitive abilities through training. Interpersonal traits like conscientiousness and integrated regulation were also examined. In terms of product accuracy, the average achieved across 153 performances three per participant) after 25h of upskilling was 95.37%, with omissions being the strongest negative predictor of accuracy. High performers outperformed low performers across all scenarios. The upskilling course was innovative, focusing on modular training and combining intralingual and interlingual practices. It addressed real-world challenges and was tailored to different professional backgrounds. The approach proved effective, with 82% of participants finding the course met their expectations and 86% acknowledging its challenging nature. The study confirmed the benefits of a modular and personalised training approach, highlighting the need for flexibility and adaptability to different skill levels and backgrounds.Our day and age is characterised by a proliferation of live multimedia and multilingual content, such as news and TV programmes. Such content is not accessible to everyone. In many countries, live subtitles in the same language are required by law to enable deaf and hard-of-hearing audiences to enjoy access to information, culture and entertainment. In the UK and other countries (e.g., Canada, Spain, Switzerland), intralingual respeaking is the most well-established technique to produce these subtitles: it relies on human-machine interaction whereby a respeaker listens to the original sound of a live programme or event and simultaneously dictates it to a speech recognition software that turns each sentence into subtitles displayed on the screen.

Respeaking so far has been used to produce intralingual subtitles, i.e., in the same language. Given the current multilingual content boom, the SMART project aims to investigate whether respeaking can be used to produce interlingual subtitles, i.e. in a different language.10:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Shaping Multilingual Access through Respeaking Technology, Project Data, 2021"}],["$","meta","2",{"name":"description","content":"$18"}],["$","meta","3",{"property":"og:title","content":"Shaping Multilingual Access through Respeaking Technology, Project Data, 2021"}],["$","meta","4",{"property":"og:description","content":"$19"}],["$","meta","5",{"property":"og:url","content":"https://harmonydata.ac.uk/search/items/shaping-multilingual-access-through-respeaking-technology-project-data-2021"}],["$","meta","6",{"property":"og:site_name","content":"Academic Resource Discovery"}],["$","meta","7",{"property":"og:locale","content":"en_US"}],["$","meta","8",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","meta","9",{"property":"og:image:width","content":"1200"}],["$","meta","10",{"property":"og:image:height","content":"630"}],["$","meta","11",{"property":"og:image:alt","content":"Shaping Multilingual Access through Respeaking Technology, Project Data, 2021"}],["$","meta","12",{"property":"og:type","content":"website"}],["$","meta","13",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","14",{"name":"twitter:title","content":"Shaping Multilingual Access through Respeaking Technology, Project Data, 2021"}],["$","meta","15",{"name":"twitter:description","content":"$1a"}],["$","meta","16",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","link","17",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
e:null
