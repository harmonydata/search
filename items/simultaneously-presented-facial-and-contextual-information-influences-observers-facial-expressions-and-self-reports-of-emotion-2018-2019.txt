1:"$Sreact.fragment"
2:I[52332,["9692","static/js/9692.83f9877c.js","421","static/js/421.98ca62d9.js","1759","static/js/1759.b13f3ee1.js","3535","static/js/3535.878ceef2.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","574","static/js/574.fd20103e.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.7ef30b9e.js"],"default"]
3:I[65380,["9692","static/js/9692.83f9877c.js","421","static/js/421.98ca62d9.js","1759","static/js/1759.b13f3ee1.js","3535","static/js/3535.878ceef2.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","574","static/js/574.fd20103e.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.7ef30b9e.js"],"AuthProvider"]
4:I[41627,["9692","static/js/9692.83f9877c.js","421","static/js/421.98ca62d9.js","1759","static/js/1759.b13f3ee1.js","3535","static/js/3535.878ceef2.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","574","static/js/574.fd20103e.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.7ef30b9e.js"],"FirebaseProvider"]
5:"$Sreact.suspense"
6:I[92114,["9692","static/js/9692.83f9877c.js","421","static/js/421.98ca62d9.js","1759","static/js/1759.b13f3ee1.js","3535","static/js/3535.878ceef2.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","574","static/js/574.fd20103e.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.7ef30b9e.js"],"SearchProvider"]
7:I[94049,["9692","static/js/9692.83f9877c.js","421","static/js/421.98ca62d9.js","1759","static/js/1759.b13f3ee1.js","3535","static/js/3535.878ceef2.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","574","static/js/574.fd20103e.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.7ef30b9e.js"],"default"]
8:I[20190,["9692","static/js/9692.83f9877c.js","421","static/js/421.98ca62d9.js","1759","static/js/1759.b13f3ee1.js","3535","static/js/3535.878ceef2.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","574","static/js/574.fd20103e.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.7ef30b9e.js"],"default"]
9:I[9766,[],""]
a:I[98924,[],""]
b:I[74744,["9692","static/js/9692.83f9877c.js","421","static/js/421.98ca62d9.js","1759","static/js/1759.b13f3ee1.js","3535","static/js/3535.878ceef2.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","574","static/js/574.fd20103e.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.7ef30b9e.js"],"ToastContainer"]
f:I[57150,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/e446a64f2ff89daf.css","style"]
0:{"P":null,"b":"PiYo4VwvF8xNxo2iFW7jS","p":"/search","c":["","items","simultaneously-presented-facial-and-contextual-information-influences-observers-facial-expressions-and-self-reports-of-emotion-2018-2019"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","simultaneously-presented-facial-and-contextual-information-influences-observers-facial-expressions-and-self-reports-of-emotion-2018-2019","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/e446a64f2ff89daf.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":[["$","meta",null,{"name":"emotion-insertion-point","content":""}],["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"preconnect","href":"https://www.cataloguementalhealth.ac.uk"}],["$","link",null,{"rel":"dns-prefetch","href":"https://harmonydata.ac.uk"}],["$","style",null,{"dangerouslySetInnerHTML":{"__html":"\n            /* Ensure immediate rendering with Roboto and fallbacks */\n            * { \n              font-family: \"Roboto\", -apple-system, BlinkMacSystemFont, \"Segoe UI\", \"Oxygen\", \"Ubuntu\", \"Cantarell\", \"Fira Sans\", \"Droid Sans\", \"Helvetica Neue\", sans-serif !important;\n              font-display: swap;\n              -webkit-font-smoothing: antialiased;\n              -moz-osx-font-smoothing: grayscale;\n            }\n            body { \n              visibility: visible !important; \n              opacity: 1 !important; \n              margin: 0; \n              padding: 0; \n            }\n          "}}]]}],["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":["$","$5",null,{"fallback":["$","div",null,{"children":"Loading..."}],"children":["$","$L6",null,{"children":[["$","$L7",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L8",null,{}],["$","$L7",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L9",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$Lb",null,{"position":"bottom-right"}]]}]}]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L9",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","simultaneously-presented-facial-and-contextual-information-influences-observers-facial-expressions-and-self-reports-of-emotion-2018-2019","d"],["$","$1","c",{"children":[null,["$","$L9",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$Lc",null,"$Ld"]}],{},null,false]},null,false]},null,false]},null,false],"$Le",false]],"m":"$undefined","G":["$f",[]],"s":false,"S":true}
10:I[24431,[],"OutletBoundary"]
12:I[15278,[],"AsyncMetadataOutlet"]
14:I[24431,[],"ViewportBoundary"]
16:I[24431,[],"MetadataBoundary"]
d:["$","$L10",null,{"children":["$L11",["$","$L12",null,{"promise":"$@13"}]]}]
e:["$","$1","h",{"children":[null,[["$","$L14",null,{"children":"$L15"}],["$","meta",null,{"name":"next-size-adjust","content":""}]],["$","$L16",null,{"children":["$","div",null,{"hidden":true,"children":["$","$5",null,{"fallback":null,"children":"$L17"}]}]}]]}]
18:I[41402,["9692","static/js/9692.83f9877c.js","1759","static/js/1759.b13f3ee1.js","690","static/js/690.e023e61b.js","3535","static/js/3535.878ceef2.js","5332","static/js/5332.4ca3e6c6.js","2410","static/js/2410.ec36c5aa.js","5183","static/js/5183.9f1a7545.js","5738","static/js/5738.d28a9943.js","3055","static/js/3055.87b66c06.js","8977","static/js/8977.25169e64.js","6387","static/js/app/items/%5Bslug%5D/page.0819d17d.js"],""]
19:Td4f,{"@context":"https://schema.org/","@type":"Dataset","name":"Simultaneously Presented Facial and Contextual Information Influences Observers' Facial Expressions and Self-reports of Emotion, 2018-2019","description":"We receive emotional signals from different sources, including the face and its surrounding context. Previous research has shown the effect that facial expressions and contextual affective information have on people’s brain responses. This study measured physiological responses and ratings of affect to face-context compounds of varied emotional content. Forty-two participants freely viewed face-context and context-only natural threat, mutilation, happy, erotic and neutral scenes whilst corrugator, zygomatic and startle eyeblink responses were recorded. Concerning the emotional content presented, participants’ corrugator, zygomatic, startle eyeblink responses and their valence and arousal ratings varied with the stimuli valence and arousal matched the stimuli valence. Face-context threat and mutilation scenes elicited more negative emotional experiences and larger corrugator responses than context-only scenes. In contrast, happy face-context scenes elicited more positive emotional experiences and a decreased corrugator response. The zygomatic showed an enhanced response to face-context scenes, regardless of the valence of the scenes. Our results show that the simultaneous perception of emotional signals from faces and contextual information induce enhanced facial reactions and affective responses.An increasing volume of violent and distressing imagery is being shared online, and this provides a challenge to any organisation that moderates their online content. The problem is particularly acute in public policing and social services organisations that must analyse such imagery in the course of investigation or child protection. Such organisations have a duty of care to protect their employees, and ensure their welfare. In support of this goal, this project will perform research which will facilitate the development of novel digital tools to assist users and reduce the mental health burden created by viewing this imagery. This objective will be attained by working in collaboration with the Child Exploitation Online Protection command of the National Crime Agency and the artificial intelligence technology firm, Qumodo. Three strands of research will be performed: 1) Evaluating emotional image recognition in the context of image manipulations guided by artificial intelligence to reduce emotional impact while still retaining scene information. 2) Experiments in social neuroscience that evaluate the effectiveness of the image manipulations from Strand 1, and help better to understand the nature of how the emotional processing of distressing images might compete with cognitive processing. 3) Determining what are the potential risks of implementing this new technology and how can these risks be dealt with effectively to maximise benefit to both employees and their organisations.","url":"https://harmonydata.ac.uk/search/items/simultaneously-presented-facial-and-contextual-information-influences-observers-facial-expressions-and-self-reports-of-emotion-2018-2019","identifier":["http://dx.doi.org/10.5255/UKDA-SN-855096"],"keywords":["EMOTIONAL STATES","ATTITUDES","PERCEPTION","EMOTIONAL DEVELOPMENT"],"temporalCoverage":"2017-10-01/2021-09-30"}c:["$","$5",null,{"fallback":["$","div",null,{"children":"Loading..."}],"children":[["$","$L18",null,{"strategy":"beforeInteractive","id":"structured-data","type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$19"}}],"$L1a"]}]
1b:I[78977,["9692","static/js/9692.83f9877c.js","1759","static/js/1759.b13f3ee1.js","690","static/js/690.e023e61b.js","3535","static/js/3535.878ceef2.js","5332","static/js/5332.4ca3e6c6.js","2410","static/js/2410.ec36c5aa.js","5183","static/js/5183.9f1a7545.js","5738","static/js/5738.d28a9943.js","3055","static/js/3055.87b66c06.js","8977","static/js/8977.25169e64.js","6387","static/js/app/items/%5Bslug%5D/page.0819d17d.js"],"default"]
1c:Tb08,We receive emotional signals from different sources, including the face and its surrounding context. Previous research has shown the effect that facial expressions and contextual affective information have on people’s brain responses. This study measured physiological responses and ratings of affect to face-context compounds of varied emotional content. Forty-two participants freely viewed face-context and context-only natural threat, mutilation, happy, erotic and neutral scenes whilst corrugator, zygomatic and startle eyeblink responses were recorded. Concerning the emotional content presented, participants’ corrugator, zygomatic, startle eyeblink responses and their valence and arousal ratings varied with the stimuli valence and arousal matched the stimuli valence. Face-context threat and mutilation scenes elicited more negative emotional experiences and larger corrugator responses than context-only scenes. In contrast, happy face-context scenes elicited more positive emotional experiences and a decreased corrugator response. The zygomatic showed an enhanced response to face-context scenes, regardless of the valence of the scenes. Our results show that the simultaneous perception of emotional signals from faces and contextual information induce enhanced facial reactions and affective responses.An increasing volume of violent and distressing imagery is being shared online, and this provides a challenge to any organisation that moderates their online content. The problem is particularly acute in public policing and social services organisations that must analyse such imagery in the course of investigation or child protection. Such organisations have a duty of care to protect their employees, and ensure their welfare. In support of this goal, this project will perform research which will facilitate the development of novel digital tools to assist users and reduce the mental health burden created by viewing this imagery. This objective will be attained by working in collaboration with the Child Exploitation Online Protection command of the National Crime Agency and the artificial intelligence technology firm, Qumodo. Three strands of research will be performed: 1) Evaluating emotional image recognition in the context of image manipulations guided by artificial intelligence to reduce emotional impact while still retaining scene information. 2) Experiments in social neuroscience that evaluate the effectiveness of the image manipulations from Strand 1, and help better to understand the nature of how the emotional processing of distressing images might compete with cognitive processing. 3) Determining what are the potential risks of implementing this new technology and how can these risks be dealt with effectively to maximise benefit to both employees and their organisations.1a:["$","$L1b",null,{"study":{"dataset_schema":{"@context":"https://schema.org/","@type":"Dataset","name":"Simultaneously Presented Facial and Contextual Information Influences Observers' Facial Expressions and Self-reports of Emotion, 2018-2019","description":"$1c","url":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=855096","https://reshare.ukdataservice.ac.uk/855096"],"keywords":["EMOTIONAL STATES","ATTITUDES","PERCEPTION","EMOTIONAL DEVELOPMENT"],"identifier":["http://dx.doi.org/10.5255/UKDA-SN-855096"],"includedInDataCatalog":[{"@type":"DataCatalog","name":"UK Data Service","url":"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=855096"}],"sponsor":[{"@type":"Organization","name":"ESRC"}],"temporalCoverage":"2017-10-01/2021-09-30"},"extra_data":{"language_codes":["en"],"harmony_id":"ukds/855096","start_year":2017,"end_year":2021,"data_access":"The Data Collection is available for download to users registered with the UK Data Service.","urls":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=855096","https://reshare.ukdataservice.ac.uk/855096"],"name":"Simultaneously Presented Facial and Contextual Information Influences Observers' Facial Expressions and Self-reports of Emotion, 2018-2019","source":["ukds"],"num_variables":null,"genetic_data_collected":false,"dois":["http://dx.doi.org/10.5255/UKDA-SN-855096"],"sex":"all","instruments":[],"geographic_coverage":"Glasgow","ai_summary":null,"resource_type":"dataset","country_codes":["GB"],"duration_years":4,"study_design":[],"slug":"simultaneously-presented-facial-and-contextual-information-influences-observers-facial-expressions-and-self-reports-of-emotion-2018-2019","uuid":"126f79c009da8353f346936e7184315b"},"distance":0,"score":0,"parent":{},"ancestors":[]}}]
15:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
11:null
1d:Tb08,We receive emotional signals from different sources, including the face and its surrounding context. Previous research has shown the effect that facial expressions and contextual affective information have on people’s brain responses. This study measured physiological responses and ratings of affect to face-context compounds of varied emotional content. Forty-two participants freely viewed face-context and context-only natural threat, mutilation, happy, erotic and neutral scenes whilst corrugator, zygomatic and startle eyeblink responses were recorded. Concerning the emotional content presented, participants’ corrugator, zygomatic, startle eyeblink responses and their valence and arousal ratings varied with the stimuli valence and arousal matched the stimuli valence. Face-context threat and mutilation scenes elicited more negative emotional experiences and larger corrugator responses than context-only scenes. In contrast, happy face-context scenes elicited more positive emotional experiences and a decreased corrugator response. The zygomatic showed an enhanced response to face-context scenes, regardless of the valence of the scenes. Our results show that the simultaneous perception of emotional signals from faces and contextual information induce enhanced facial reactions and affective responses.An increasing volume of violent and distressing imagery is being shared online, and this provides a challenge to any organisation that moderates their online content. The problem is particularly acute in public policing and social services organisations that must analyse such imagery in the course of investigation or child protection. Such organisations have a duty of care to protect their employees, and ensure their welfare. In support of this goal, this project will perform research which will facilitate the development of novel digital tools to assist users and reduce the mental health burden created by viewing this imagery. This objective will be attained by working in collaboration with the Child Exploitation Online Protection command of the National Crime Agency and the artificial intelligence technology firm, Qumodo. Three strands of research will be performed: 1) Evaluating emotional image recognition in the context of image manipulations guided by artificial intelligence to reduce emotional impact while still retaining scene information. 2) Experiments in social neuroscience that evaluate the effectiveness of the image manipulations from Strand 1, and help better to understand the nature of how the emotional processing of distressing images might compete with cognitive processing. 3) Determining what are the potential risks of implementing this new technology and how can these risks be dealt with effectively to maximise benefit to both employees and their organisations.1e:Tb08,We receive emotional signals from different sources, including the face and its surrounding context. Previous research has shown the effect that facial expressions and contextual affective information have on people’s brain responses. This study measured physiological responses and ratings of affect to face-context compounds of varied emotional content. Forty-two participants freely viewed face-context and context-only natural threat, mutilation, happy, erotic and neutral scenes whilst corrugator, zygomatic and startle eyeblink responses were recorded. Concerning the emotional content presented, participants’ corrugator, zygomatic, startle eyeblink responses and their valence and arousal ratings varied with the stimuli valence and arousal matched the stimuli valence. Face-context threat and mutilation scenes elicited more negative emotional experiences and larger corrugator responses than context-only scenes. In contrast, happy face-context scenes elicited more positive emotional experiences and a decreased corrugator response. The zygomatic showed an enhanced response to face-context scenes, regardless of the valence of the scenes. Our results show that the simultaneous perception of emotional signals from faces and contextual information induce enhanced facial reactions and affective responses.An increasing volume of violent and distressing imagery is being shared online, and this provides a challenge to any organisation that moderates their online content. The problem is particularly acute in public policing and social services organisations that must analyse such imagery in the course of investigation or child protection. Such organisations have a duty of care to protect their employees, and ensure their welfare. In support of this goal, this project will perform research which will facilitate the development of novel digital tools to assist users and reduce the mental health burden created by viewing this imagery. This objective will be attained by working in collaboration with the Child Exploitation Online Protection command of the National Crime Agency and the artificial intelligence technology firm, Qumodo. Three strands of research will be performed: 1) Evaluating emotional image recognition in the context of image manipulations guided by artificial intelligence to reduce emotional impact while still retaining scene information. 2) Experiments in social neuroscience that evaluate the effectiveness of the image manipulations from Strand 1, and help better to understand the nature of how the emotional processing of distressing images might compete with cognitive processing. 3) Determining what are the potential risks of implementing this new technology and how can these risks be dealt with effectively to maximise benefit to both employees and their organisations.13:{"metadata":[["$","title","0",{"children":"Simultaneously Presented Facial and Contextual Information Influences Observers' Facial Expressions and Self-reports of Emotion, 2018-2019"}],["$","meta","1",{"name":"description","content":"$1d"}],["$","meta","2",{"property":"og:title","content":"Simultaneously Presented Facial and Contextual Information Influences Observers' Facial Expressions and Self-reports of Emotion, 2018-2019"}],["$","meta","3",{"property":"og:description","content":"$1e"}],"$L1f","$L20","$L21","$L22","$L23","$L24","$L25","$L26","$L27","$L28","$L29","$L2a","$L2b","$L2c"],"error":null,"digest":"$undefined"}
2e:I[80622,[],"IconMark"]
1f:["$","meta","4",{"property":"og:url","content":"https://harmonydata.ac.uk/search/items/simultaneously-presented-facial-and-contextual-information-influences-observers-facial-expressions-and-self-reports-of-emotion-2018-2019"}]
20:["$","meta","5",{"property":"og:site_name","content":"Academic Resource Discovery"}]
21:["$","meta","6",{"property":"og:locale","content":"en_US"}]
22:["$","meta","7",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}]
23:["$","meta","8",{"property":"og:image:width","content":"1200"}]
24:["$","meta","9",{"property":"og:image:height","content":"630"}]
25:["$","meta","10",{"property":"og:image:alt","content":"Simultaneously Presented Facial and Contextual Information Influences Observers' Facial Expressions and Self-reports of Emotion, 2018-2019"}]
26:["$","meta","11",{"property":"og:type","content":"website"}]
27:["$","meta","12",{"name":"twitter:card","content":"summary_large_image"}]
28:["$","meta","13",{"name":"twitter:title","content":"Simultaneously Presented Facial and Contextual Information Influences Observers' Facial Expressions and Self-reports of Emotion, 2018-2019"}]
2d:Tb08,We receive emotional signals from different sources, including the face and its surrounding context. Previous research has shown the effect that facial expressions and contextual affective information have on people’s brain responses. This study measured physiological responses and ratings of affect to face-context compounds of varied emotional content. Forty-two participants freely viewed face-context and context-only natural threat, mutilation, happy, erotic and neutral scenes whilst corrugator, zygomatic and startle eyeblink responses were recorded. Concerning the emotional content presented, participants’ corrugator, zygomatic, startle eyeblink responses and their valence and arousal ratings varied with the stimuli valence and arousal matched the stimuli valence. Face-context threat and mutilation scenes elicited more negative emotional experiences and larger corrugator responses than context-only scenes. In contrast, happy face-context scenes elicited more positive emotional experiences and a decreased corrugator response. The zygomatic showed an enhanced response to face-context scenes, regardless of the valence of the scenes. Our results show that the simultaneous perception of emotional signals from faces and contextual information induce enhanced facial reactions and affective responses.An increasing volume of violent and distressing imagery is being shared online, and this provides a challenge to any organisation that moderates their online content. The problem is particularly acute in public policing and social services organisations that must analyse such imagery in the course of investigation or child protection. Such organisations have a duty of care to protect their employees, and ensure their welfare. In support of this goal, this project will perform research which will facilitate the development of novel digital tools to assist users and reduce the mental health burden created by viewing this imagery. This objective will be attained by working in collaboration with the Child Exploitation Online Protection command of the National Crime Agency and the artificial intelligence technology firm, Qumodo. Three strands of research will be performed: 1) Evaluating emotional image recognition in the context of image manipulations guided by artificial intelligence to reduce emotional impact while still retaining scene information. 2) Experiments in social neuroscience that evaluate the effectiveness of the image manipulations from Strand 1, and help better to understand the nature of how the emotional processing of distressing images might compete with cognitive processing. 3) Determining what are the potential risks of implementing this new technology and how can these risks be dealt with effectively to maximise benefit to both employees and their organisations.29:["$","meta","14",{"name":"twitter:description","content":"$2d"}]
2a:["$","meta","15",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}]
2b:["$","link","16",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]
2c:["$","$L2e","17",{}]
17:"$13:metadata"
