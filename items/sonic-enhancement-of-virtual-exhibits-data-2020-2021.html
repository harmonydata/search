<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/search/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/search/_next/static/css/2c4d913f25bfc6bf.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/search/_next/static/chunks/webpack-904c4041abd776f2.js"/><script src="/search/_next/static/chunks/4bd1b696-220750848fc52813.js" async=""></script><script src="/search/_next/static/chunks/1517-45045142ab33e6f1.js" async=""></script><script src="/search/_next/static/chunks/main-app-c0fb4dfbd302de72.js" async=""></script><script src="/search/_next/static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js" async=""></script><script src="/search/_next/static/chunks/f71d1b72-799ff7a6833dc50c.js" async=""></script><script src="/search/_next/static/chunks/6586-1013c110456598c2.js" async=""></script><script src="/search/_next/static/chunks/4889-f0599128dd4090a0.js" async=""></script><script src="/search/_next/static/chunks/9141-d17bf49085d8e296.js" async=""></script><script src="/search/_next/static/chunks/2926-f97573e569b0b5d8.js" async=""></script><script src="/search/_next/static/chunks/8173-30737ce2fc776efb.js" async=""></script><script src="/search/_next/static/chunks/9756-90c6220c809c4148.js" async=""></script><script src="/search/_next/static/chunks/3163-d1a03f172499fcd8.js" async=""></script><script src="/search/_next/static/chunks/app/layout-802ca43371b3eb9d.js" async=""></script><script src="/search/_next/static/chunks/834cb1aa-fe75579b2a50baac.js" async=""></script><script src="/search/_next/static/chunks/2170a4aa-66be1631595ccab0.js" async=""></script><script src="/search/_next/static/chunks/1057-d97430463abd6821.js" async=""></script><script src="/search/_next/static/chunks/2282-26bc5318a4471ee9.js" async=""></script><script src="/search/_next/static/chunks/9234-fce85e807baa599f.js" async=""></script><script src="/search/_next/static/chunks/5733-d0ad15157d7394e5.js" async=""></script><script src="/search/_next/static/chunks/613-3467f3d6fe7e6e6a.js" async=""></script><script src="/search/_next/static/chunks/8738-58586275b0d791e8.js" async=""></script><script src="/search/_next/static/chunks/2649-8ae63f8e6332939b.js" async=""></script><script src="/search/_next/static/chunks/1857-99747bd4076c313b.js" async=""></script><script src="/search/_next/static/chunks/2288-ffb609d77f258e27.js" async=""></script><script src="/search/_next/static/chunks/app/items/%5Bslug%5D/page-566168fa6ba89e63.js" async=""></script><link rel="preload" href="/search/_next/static/css/4921cfd18b262f8c.css" as="style"/><meta name="next-size-adjust" content=""/><meta name="emotion-insertion-point" content=""/><title>Sonic Enhancement of Virtual Exhibits Data, 2020-2021</title><meta name="description" content="We conducted an online experiment to explore how sound influences the interest level, emotional response, and engagement of individuals who view objects within a virtual exhibit. As part of this experiment, we designed a set of different soundscapes, which we presented to participants who viewed museum objects virtually. We then asked participants to report their felt affect and level of engagement with the exhibits.

This dataset contains the data we recorded in these experiments and used in the analysis presented in our paper entitled &#x27;Sonic Enhancement of Virtual Exhibits&#x27;.A common feature of UK museums is their recent embrace of virtual exhibits. However, despite this explosion of virtual exhibits, relatively little attention has been given to how sound may be used to create a more engaging experience for audiences. That is, although museums have certainly included sound as parts of virtual exhibits (e.g., through narrated tours or through the addition of music), there is little scholarship about how sound may be harnessed to influence the length of time spent looking at objects within a virtual exhibit; the emotional response audiences have while viewing these objects; or the level of attention or distraction experienced. We conducted a series of experiments to develop a more rigorous understanding of how sound shapes audiences’ experiences in museums. 

This project builds on our team’s expertise in sound-tracking and the use of sound effects to influence audience engagement. We also build on our research exploring how sound may be used to modify the affective state of an individual, as well as our research which showed how state-of-the-art neural engineering may be used to dynamically modulate sound and music over time to optimize an “affective trajectory” (a change in felt affect in an audience over time). The project specifically focused on the use of sound effects in the virtual environment, building on our previous studies aimed at understanding how new technologies affect perception and understanding of heritage."/><meta property="og:title" content="Sonic Enhancement of Virtual Exhibits Data, 2020-2021"/><meta property="og:description" content="We conducted an online experiment to explore how sound influences the interest level, emotional response, and engagement of individuals who view objects within a virtual exhibit. As part of this experiment, we designed a set of different soundscapes, which we presented to participants who viewed museum objects virtually. We then asked participants to report their felt affect and level of engagement with the exhibits.

This dataset contains the data we recorded in these experiments and used in the analysis presented in our paper entitled &#x27;Sonic Enhancement of Virtual Exhibits&#x27;.A common feature of UK museums is their recent embrace of virtual exhibits. However, despite this explosion of virtual exhibits, relatively little attention has been given to how sound may be used to create a more engaging experience for audiences. That is, although museums have certainly included sound as parts of virtual exhibits (e.g., through narrated tours or through the addition of music), there is little scholarship about how sound may be harnessed to influence the length of time spent looking at objects within a virtual exhibit; the emotional response audiences have while viewing these objects; or the level of attention or distraction experienced. We conducted a series of experiments to develop a more rigorous understanding of how sound shapes audiences’ experiences in museums. 

This project builds on our team’s expertise in sound-tracking and the use of sound effects to influence audience engagement. We also build on our research exploring how sound may be used to modify the affective state of an individual, as well as our research which showed how state-of-the-art neural engineering may be used to dynamically modulate sound and music over time to optimize an “affective trajectory” (a change in felt affect in an audience over time). The project specifically focused on the use of sound effects in the virtual environment, building on our previous studies aimed at understanding how new technologies affect perception and understanding of heritage."/><meta property="og:url" content="https://harmonydata.ac.uk/search/items/sonic-enhancement-of-virtual-exhibits-data-2020-2021"/><meta property="og:site_name" content="Academic Resource Discovery"/><meta property="og:locale" content="en_US"/><meta property="og:image" content="https://harmonydata.ac.uk/search/harmony.png"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image:alt" content="Sonic Enhancement of Virtual Exhibits Data, 2020-2021"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Sonic Enhancement of Virtual Exhibits Data, 2020-2021"/><meta name="twitter:description" content="We conducted an online experiment to explore how sound influences the interest level, emotional response, and engagement of individuals who view objects within a virtual exhibit. As part of this experiment, we designed a set of different soundscapes, which we presented to participants who viewed museum objects virtually. We then asked participants to report their felt affect and level of engagement with the exhibits.

This dataset contains the data we recorded in these experiments and used in the analysis presented in our paper entitled &#x27;Sonic Enhancement of Virtual Exhibits&#x27;.A common feature of UK museums is their recent embrace of virtual exhibits. However, despite this explosion of virtual exhibits, relatively little attention has been given to how sound may be used to create a more engaging experience for audiences. That is, although museums have certainly included sound as parts of virtual exhibits (e.g., through narrated tours or through the addition of music), there is little scholarship about how sound may be harnessed to influence the length of time spent looking at objects within a virtual exhibit; the emotional response audiences have while viewing these objects; or the level of attention or distraction experienced. We conducted a series of experiments to develop a more rigorous understanding of how sound shapes audiences’ experiences in museums. 

This project builds on our team’s expertise in sound-tracking and the use of sound effects to influence audience engagement. We also build on our research exploring how sound may be used to modify the affective state of an individual, as well as our research which showed how state-of-the-art neural engineering may be used to dynamically modulate sound and music over time to optimize an “affective trajectory” (a change in felt affect in an audience over time). The project specifically focused on the use of sound effects in the virtual environment, building on our previous studies aimed at understanding how new technologies affect perception and understanding of heritage."/><meta name="twitter:image" content="https://harmonydata.ac.uk/search/harmony.png"/><link rel="icon" href="/search/favicon.ico" type="image/x-icon" sizes="16x16"/><script src="/search/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script><style data-emotion="mui-global o39zl1">html{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;box-sizing:border-box;-webkit-text-size-adjust:100%;}*,*::before,*::after{box-sizing:inherit;}strong,b{font-weight:700;}body{margin:0;color:#1A1A1A;font-size:0.875rem;line-height:1.5;font-family:'Roboto','Roboto Fallback';font-weight:400;background-color:#FFFFFF;}@media (min-width:600px){body{font-size:1rem;}}@media print{body{background-color:#fff;}}body::backdrop{background-color:#FFFFFF;}</style></head><body class="__className_62a302"><script src="/search/_next/static/chunks/webpack-904c4041abd776f2.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[82104,[\"2992\",\"static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js\",\"9895\",\"static/chunks/f71d1b72-799ff7a6833dc50c.js\",\"6586\",\"static/chunks/6586-1013c110456598c2.js\",\"4889\",\"static/chunks/4889-f0599128dd4090a0.js\",\"9141\",\"static/chunks/9141-d17bf49085d8e296.js\",\"2926\",\"static/chunks/2926-f97573e569b0b5d8.js\",\"8173\",\"static/chunks/8173-30737ce2fc776efb.js\",\"9756\",\"static/chunks/9756-90c6220c809c4148.js\",\"3163\",\"static/chunks/3163-d1a03f172499fcd8.js\",\"7177\",\"static/chunks/app/layout-802ca43371b3eb9d.js\"],\"default\"]\n3:I[10683,[\"2992\",\"static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js\",\"9895\",\"static/chunks/f71d1b72-799ff7a6833dc50c.js\",\"6586\",\"static/chunks/6586-1013c110456598c2.js\",\"4889\",\"static/chunks/4889-f0599128dd4090a0.js\",\"9141\",\"static/chunks/9141-d17bf49085d8e296.js\",\"2926\",\"static/chunks/2926-f97573e569b0b5d8.js\",\"8173\",\"static/chunks/8173-30737ce2fc776efb.js\",\"9756\",\"static/chunks/9756-90c6220c809c4148.js\",\"3163\",\"static/chunks/3163-d1a03f172499fcd8.js\",\"7177\",\"static/chunks/app/layout-802ca43371b3eb9d.js\"],\"AuthProvider\"]\n4:I[63612,[\"2992\",\"static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js\",\"9895\",\"static/chunks/f71d1b72-799ff7a6833dc50c.js\",\"6586\",\"static/chunks/6586-1013c110456598c2.js\",\"4889\",\"static/chunks/4889-f0599128dd4090a0.js\",\"9141\",\"static/chunks/9141-d17bf49085d8e296.js\",\"2926\",\"static/chunks/2926-f97573e569b0b5d8.js\",\"8173\",\"static/chunks/8173-30737ce2fc776efb.js\",\"9756\",\"static/chunks/9756-90c6220c809c4148.js\",\"3163\",\"static/chunks/3163-d1a03f172499fcd8.js\",\"7177\",\"static/chunks/app/layout-802ca43371b3eb9d.js\"],\"SearchProvider\"]\n5:I[68998,[\"2992\",\"static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js\",\"9895\",\"static/chunks/f71d1b72-799ff7a6833dc50c.js\",\"6586\",\"static/chunks/6586-1013c110456598c2.js\",\"4889\",\"static/chunks/4889-f0599128dd4090a0.js\",\"9141\",\"static/chunks/9141-d17bf49085d8e296.js\",\"2926\",\"static/chunks/2926-f97573e569b0b5d8.js\",\"8173\",\"static/chunks/8173-30737ce2fc776efb.js\",\"9756\",\"static/chunks/9756-90c6220c809c4148.js\",\"3163\",\"static/chunks/3163-d1a03f172499fcd8.js\",\"7177\",\"stati"])</script><script>self.__next_f.push([1,"c/chunks/app/layout-802ca43371b3eb9d.js\"],\"default\"]\n6:I[98904,[\"2992\",\"static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js\",\"9895\",\"static/chunks/f71d1b72-799ff7a6833dc50c.js\",\"6586\",\"static/chunks/6586-1013c110456598c2.js\",\"4889\",\"static/chunks/4889-f0599128dd4090a0.js\",\"9141\",\"static/chunks/9141-d17bf49085d8e296.js\",\"2926\",\"static/chunks/2926-f97573e569b0b5d8.js\",\"8173\",\"static/chunks/8173-30737ce2fc776efb.js\",\"9756\",\"static/chunks/9756-90c6220c809c4148.js\",\"3163\",\"static/chunks/3163-d1a03f172499fcd8.js\",\"7177\",\"static/chunks/app/layout-802ca43371b3eb9d.js\"],\"default\"]\n7:I[15244,[],\"\"]\n8:I[43866,[],\"\"]\n9:I[14046,[\"2992\",\"static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js\",\"9895\",\"static/chunks/f71d1b72-799ff7a6833dc50c.js\",\"6586\",\"static/chunks/6586-1013c110456598c2.js\",\"4889\",\"static/chunks/4889-f0599128dd4090a0.js\",\"9141\",\"static/chunks/9141-d17bf49085d8e296.js\",\"2926\",\"static/chunks/2926-f97573e569b0b5d8.js\",\"8173\",\"static/chunks/8173-30737ce2fc776efb.js\",\"9756\",\"static/chunks/9756-90c6220c809c4148.js\",\"3163\",\"static/chunks/3163-d1a03f172499fcd8.js\",\"7177\",\"static/chunks/app/layout-802ca43371b3eb9d.js\"],\"ToastContainer\"]\nb:I[86213,[],\"OutletBoundary\"]\nd:I[86213,[],\"MetadataBoundary\"]\nf:I[86213,[],\"ViewportBoundary\"]\n11:I[34835,[],\"\"]\n:HL[\"/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/search/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/search/_next/static/css/2c4d913f25bfc6bf.css\",\"style\"]\n:HL[\"/search/_next/static/css/4921cfd18b262f8c.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"crTstUN6M3QDVzxd_YHmv\",\"p\":\"/search\",\"c\":[\"\",\"items\",\"sonic-enhancement-of-virtual-exhibits-data-2020-2021\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"items\",{\"children\":[[\"slug\",\"sonic-enhancement-of-virtual-exhibits-data-2020-2021\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/search/_next/static/css/2c4d913f25bfc6bf.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[\"$\",\"meta\",null,{\"name\":\"emotion-insertion-point\",\"content\":\"\"}]}],[\"$\",\"body\",null,{\"className\":\"__className_62a302\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"children\":[\"$\",\"$L4\",null,{\"children\":[[\"$\",\"$L5\",null,{\"sx\":{\"display\":\"flex\",\"flexDirection\":{\"xs\":\"column\",\"md\":\"row\"}},\"children\":[[\"$\",\"$L6\",null,{}],[\"$\",\"$L5\",null,{\"component\":\"main\",\"sx\":{\"flexGrow\":1,\"ml\":{\"xs\":0,\"md\":\"72px\"},\"mt\":{\"xs\":\"64px\",\"md\":0},\"minHeight\":{\"xs\":\"calc(100vh - 64px)\",\"md\":\"100vh\"},\"width\":{\"xs\":\"100%\",\"md\":\"calc(100% - 72px)\"}},\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}],[\"$\",\"$L9\",null,{\"position\":\"bottom-right\"}]]}]}]}]}]]}]]}],{\"children\":[\"items\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"items\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"sonic-enhancement-of-virtual-exhibits-data-2020-2021\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"items\",\"children\",\"$0:f:0:1:2:children:2:children:0\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$La\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/search/_next/static/css/4921cfd18b262f8c.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"byGZitm63mpNqCvTeipIE\",{\"children\":[[\"$\",\"$Ld\",null,{\"children\":\"$Le\"}],[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:I[53704,[\"2992\",\"static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js\",\"9895\",\"static/chunks/f71d1b72-799ff7a6833dc50c.js\",\"2154\",\"static/chunks/834cb1aa-fe75579b2a50baac.js\",\"3524\",\"static/chunks/2170a4aa-66be1631595ccab0.js\",\"6586\",\"static/chunks/6586-1013c110456598c2.js\",\"4889\",\"static/chunks/4889-f0599128dd4090a0.js\",\"1057\",\"static/chunks/1057-d97430463abd6821.js\",\"2282\",\"static/chunks/2282-26bc5318a4471ee9.js\",\"9234\",\"static/chunks/9234-fce85e807baa599f.js\",\"9141\",\"static/chunks/9141-d17bf49085d8e296.js\",\"2926\",\"static/chunks/2926-f97573e569b0b5d8.js\",\"5733\",\"static/chunks/5733-d0ad15157d7394e5.js\",\"8173\",\"static/chunks/8173-30737ce2fc776efb.js\",\"613\",\"static/chunks/613-3467f3d6fe7e6e6a.js\",\"9756\",\"static/chunks/9756-90c6220c809c4148.js\",\"8738\",\"static/chunks/8738-58586275b0d791e8.js\",\"2649\",\"static/chunks/2649-8ae63f8e6332939b.js\",\"1857\",\"static/chunks/1857-99747bd4076c313b.js\",\"2288\",\"static/chunks/2288-ffb609d77f258e27.js\",\"6387\",\"static/chunks/app/items/%5Bslug%5D/page-566168fa6ba89e63.js\"],\"\"]\n14:I[5749,[\"2992\",\"static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js\",\"9895\",\"static/chunks/f71d1b72-799ff7a6833dc50c.js\",\"2154\",\"static/chunks/834cb1aa-fe75579b2a50baac.js\",\"3524\",\"static/chunks/2170a4aa-66be1631595ccab0.js\",\"6586\",\"static/chunks/6586-1013c110456598c2.js\",\"4889\",\"static/chunks/4889-f0599128dd4090a0.js\",\"1057\",\"static/chunks/1057-d97430463abd6821.js\",\"2282\",\"static/chunks/2282-26bc5318a4471ee9.js\",\"9234\",\"static/chunks/9234-fce85e807baa599f.js\",\"9141\",\"static/chunks/9141-d17bf49085d8e296.js\",\"2926\",\"static/chunks/2926-f97573e569b0b5d8.js\",\"5733\",\"static/chunks/5733-d0ad15157d7394e5.js\",\"8173\",\"static/chunks/8173-30737ce2fc776efb.js\",\"613\",\"static/chunks/613-3467f3d6fe7e6e6a.js\",\"9756\",\"static/chunks/9756-90c6220c809c4148.js\",\"8738\",\"static/chunks/8738-58586275b0d791e8.js\",\"2649\",\"static/chunks/2649-8ae63f8e6332939b.js\",\"1857\",\"static/chunks/1857-99747bd4076c313b.js\",\"2288\",\"static/chunks/2288-ffb609d77f258e27.js\",\"6387\",\"static/chunks/app/items/%5Bslug%5D/page-566168fa6ba89e63.js\"],\"default\"]\n13:T9b9,"])</script><script>self.__next_f.push([1,"{\"@context\":\"https://schema.org/\",\"@type\":\"Dataset\",\"name\":\"Sonic Enhancement of Virtual Exhibits Data, 2020-2021\",\"description\":\"We conducted an online experiment to explore how sound influences the interest level, emotional response, and engagement of individuals who view objects within a virtual exhibit. As part of this experiment, we designed a set of different soundscapes, which we presented to participants who viewed museum objects virtually. We then asked participants to report their felt affect and level of engagement with the exhibits.\\n\\nThis dataset contains the data we recorded in these experiments and used in the analysis presented in our paper entitled 'Sonic Enhancement of Virtual Exhibits'.A common feature of UK museums is their recent embrace of virtual exhibits. However, despite this explosion of virtual exhibits, relatively little attention has been given to how sound may be used to create a more engaging experience for audiences. That is, although museums have certainly included sound as parts of virtual exhibits (e.g., through narrated tours or through the addition of music), there is little scholarship about how sound may be harnessed to influence the length of time spent looking at objects within a virtual exhibit; the emotional response audiences have while viewing these objects; or the level of attention or distraction experienced. We conducted a series of experiments to develop a more rigorous understanding of how sound shapes audiences’ experiences in museums. \\n\\nThis project builds on our team’s expertise in sound-tracking and the use of sound effects to influence audience engagement. We also build on our research exploring how sound may be used to modify the affective state of an individual, as well as our research which showed how state-of-the-art neural engineering may be used to dynamically modulate sound and music over time to optimize an “affective trajectory” (a change in felt affect in an audience over time). The project specifically focused on the use of sound effects in the virtual environment, building on our previous studies aimed at understanding how new technologies affect perception and understanding of heritage.\",\"url\":\"https://harmonydata.ac.uk/search/items/sonic-enhancement-of-virtual-exhibits-data-2020-2021\",\"identifier\":[\"http://dx.doi.org/10.5255/UKDA-SN-855667\"],\"keywords\":[\"SOUND RECORDINGS\",\"INTEREST (COGNITIVE PROCESSES)\",\"EMOTIONAL STATES\"],\"temporalCoverage\":\"2020-01-01/2021-06-01\"}"])</script><script>self.__next_f.push([1,"15:T814,"])</script><script>self.__next_f.push([1,"We conducted an online experiment to explore how sound influences the interest level, emotional response, and engagement of individuals who view objects within a virtual exhibit. As part of this experiment, we designed a set of different soundscapes, which we presented to participants who viewed museum objects virtually. We then asked participants to report their felt affect and level of engagement with the exhibits.\n\nThis dataset contains the data we recorded in these experiments and used in the analysis presented in our paper entitled 'Sonic Enhancement of Virtual Exhibits'.A common feature of UK museums is their recent embrace of virtual exhibits. However, despite this explosion of virtual exhibits, relatively little attention has been given to how sound may be used to create a more engaging experience for audiences. That is, although museums have certainly included sound as parts of virtual exhibits (e.g., through narrated tours or through the addition of music), there is little scholarship about how sound may be harnessed to influence the length of time spent looking at objects within a virtual exhibit; the emotional response audiences have while viewing these objects; or the level of attention or distraction experienced. We conducted a series of experiments to develop a more rigorous understanding of how sound shapes audiences’ experiences in museums. \n\nThis project builds on our team’s expertise in sound-tracking and the use of sound effects to influence audience engagement. We also build on our research exploring how sound may be used to modify the affective state of an individual, as well as our research which showed how state-of-the-art neural engineering may be used to dynamically modulate sound and music over time to optimize an “affective trajectory” (a change in felt affect in an audience over time). The project specifically focused on the use of sound effects in the virtual environment, building on our previous studies aimed at understanding how new technologies affect perception and understanding of heritage."])</script><script>self.__next_f.push([1,"a:[[\"$\",\"$L12\",null,{\"strategy\":\"beforeInteractive\",\"id\":\"structured-data\",\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"$13\"}}],[\"$\",\"$L14\",null,{\"dataset\":{\"title\":\"Sonic Enhancement of Virtual Exhibits Data, 2020-2021\",\"description\":\"$15\",\"image\":\"$undefined\",\"publisher\":\"$undefined\",\"funders\":\"$undefined\",\"geographicCoverage\":\"GB\",\"temporalCoverage\":\"2020-01-01/2021-06-01\",\"ageCoverage\":\"$undefined\",\"studyDesign\":[],\"resourceType\":\"dataset\",\"topics\":[\"SOUND RECORDINGS\",\"INTEREST (COGNITIVE PROCESSES)\",\"EMOTIONAL STATES\"],\"instruments\":[],\"dataCatalogs\":[{\"name\":\"UK Data Service\",\"url\":\"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=855667\",\"logo\":\"$undefined\"}],\"matchedVariables\":[],\"allVariables\":[],\"additionalLinks\":[\"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=855667\",\"https://reshare.ukdataservice.ac.uk/855667\",\"http://dx.doi.org/10.5255/UKDA-SN-855667\",\"http://dx.doi.org/10.5255/UKDA-SN-855667\"],\"child_datasets\":[],\"aiSummary\":null}}]]\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"16:T814,"])</script><script>self.__next_f.push([1,"We conducted an online experiment to explore how sound influences the interest level, emotional response, and engagement of individuals who view objects within a virtual exhibit. As part of this experiment, we designed a set of different soundscapes, which we presented to participants who viewed museum objects virtually. We then asked participants to report their felt affect and level of engagement with the exhibits.\n\nThis dataset contains the data we recorded in these experiments and used in the analysis presented in our paper entitled 'Sonic Enhancement of Virtual Exhibits'.A common feature of UK museums is their recent embrace of virtual exhibits. However, despite this explosion of virtual exhibits, relatively little attention has been given to how sound may be used to create a more engaging experience for audiences. That is, although museums have certainly included sound as parts of virtual exhibits (e.g., through narrated tours or through the addition of music), there is little scholarship about how sound may be harnessed to influence the length of time spent looking at objects within a virtual exhibit; the emotional response audiences have while viewing these objects; or the level of attention or distraction experienced. We conducted a series of experiments to develop a more rigorous understanding of how sound shapes audiences’ experiences in museums. \n\nThis project builds on our team’s expertise in sound-tracking and the use of sound effects to influence audience engagement. We also build on our research exploring how sound may be used to modify the affective state of an individual, as well as our research which showed how state-of-the-art neural engineering may be used to dynamically modulate sound and music over time to optimize an “affective trajectory” (a change in felt affect in an audience over time). The project specifically focused on the use of sound effects in the virtual environment, building on our previous studies aimed at understanding how new technologies affect perception and understanding of heritage."])</script><script>self.__next_f.push([1,"17:T814,"])</script><script>self.__next_f.push([1,"We conducted an online experiment to explore how sound influences the interest level, emotional response, and engagement of individuals who view objects within a virtual exhibit. As part of this experiment, we designed a set of different soundscapes, which we presented to participants who viewed museum objects virtually. We then asked participants to report their felt affect and level of engagement with the exhibits.\n\nThis dataset contains the data we recorded in these experiments and used in the analysis presented in our paper entitled 'Sonic Enhancement of Virtual Exhibits'.A common feature of UK museums is their recent embrace of virtual exhibits. However, despite this explosion of virtual exhibits, relatively little attention has been given to how sound may be used to create a more engaging experience for audiences. That is, although museums have certainly included sound as parts of virtual exhibits (e.g., through narrated tours or through the addition of music), there is little scholarship about how sound may be harnessed to influence the length of time spent looking at objects within a virtual exhibit; the emotional response audiences have while viewing these objects; or the level of attention or distraction experienced. We conducted a series of experiments to develop a more rigorous understanding of how sound shapes audiences’ experiences in museums. \n\nThis project builds on our team’s expertise in sound-tracking and the use of sound effects to influence audience engagement. We also build on our research exploring how sound may be used to modify the affective state of an individual, as well as our research which showed how state-of-the-art neural engineering may be used to dynamically modulate sound and music over time to optimize an “affective trajectory” (a change in felt affect in an audience over time). The project specifically focused on the use of sound effects in the virtual environment, building on our previous studies aimed at understanding how new technologies affect perception and understanding of heritage."])</script><script>self.__next_f.push([1,"18:T814,"])</script><script>self.__next_f.push([1,"We conducted an online experiment to explore how sound influences the interest level, emotional response, and engagement of individuals who view objects within a virtual exhibit. As part of this experiment, we designed a set of different soundscapes, which we presented to participants who viewed museum objects virtually. We then asked participants to report their felt affect and level of engagement with the exhibits.\n\nThis dataset contains the data we recorded in these experiments and used in the analysis presented in our paper entitled 'Sonic Enhancement of Virtual Exhibits'.A common feature of UK museums is their recent embrace of virtual exhibits. However, despite this explosion of virtual exhibits, relatively little attention has been given to how sound may be used to create a more engaging experience for audiences. That is, although museums have certainly included sound as parts of virtual exhibits (e.g., through narrated tours or through the addition of music), there is little scholarship about how sound may be harnessed to influence the length of time spent looking at objects within a virtual exhibit; the emotional response audiences have while viewing these objects; or the level of attention or distraction experienced. We conducted a series of experiments to develop a more rigorous understanding of how sound shapes audiences’ experiences in museums. \n\nThis project builds on our team’s expertise in sound-tracking and the use of sound effects to influence audience engagement. We also build on our research exploring how sound may be used to modify the affective state of an individual, as well as our research which showed how state-of-the-art neural engineering may be used to dynamically modulate sound and music over time to optimize an “affective trajectory” (a change in felt affect in an audience over time). The project specifically focused on the use of sound effects in the virtual environment, building on our previous studies aimed at understanding how new technologies affect perception and understanding of heritage."])</script><script>self.__next_f.push([1,"e:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Sonic Enhancement of Virtual Exhibits Data, 2020-2021\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"$16\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:title\",\"content\":\"Sonic Enhancement of Virtual Exhibits Data, 2020-2021\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:description\",\"content\":\"$17\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:url\",\"content\":\"https://harmonydata.ac.uk/search/items/sonic-enhancement-of-virtual-exhibits-data-2020-2021\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:site_name\",\"content\":\"Academic Resource Discovery\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:image\",\"content\":\"https://harmonydata.ac.uk/search/harmony.png\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:image:height\",\"content\":\"630\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:image:alt\",\"content\":\"Sonic Enhancement of Virtual Exhibits Data, 2020-2021\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:title\",\"content\":\"Sonic Enhancement of Virtual Exhibits Data, 2020-2021\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:description\",\"content\":\"$18\"}],[\"$\",\"meta\",\"16\",{\"name\":\"twitter:image\",\"content\":\"https://harmonydata.ac.uk/search/harmony.png\"}],[\"$\",\"link\",\"17\",{\"rel\":\"icon\",\"href\":\"/search/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}]]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script></body></html>