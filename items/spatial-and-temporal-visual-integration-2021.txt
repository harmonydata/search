1:"$Sreact.fragment"
2:I[82104,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
3:I[10683,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"AuthProvider"]
4:I[63612,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"SearchProvider"]
5:I[68998,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
6:I[98904,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
7:I[15244,[],""]
8:I[43866,[],""]
9:I[14046,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"ToastContainer"]
b:I[86213,[],"OutletBoundary"]
d:I[86213,[],"MetadataBoundary"]
f:I[86213,[],"ViewportBoundary"]
11:I[34835,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/2c4d913f25bfc6bf.css","style"]
:HL["/search/_next/static/css/4921cfd18b262f8c.css","style"]
0:{"P":null,"b":"otOHFudQRum6LRyZaqBep","p":"/search","c":["","items","spatial-and-temporal-visual-integration-2021"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","spatial-and-temporal-visual-integration-2021","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/2c4d913f25bfc6bf.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":["$","meta",null,{"name":"emotion-insertion-point","content":""}]}],["$","body",null,{"className":"__className_62a302","children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":[["$","$L5",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L6",null,{}],["$","$L5",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$L9",null,{"position":"bottom-right"}]]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","spatial-and-temporal-visual-integration-2021","d"],["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$La",[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/4921cfd18b262f8c.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$Lb",null,{"children":"$Lc"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","N2oXWyCO-xU8-alFSbbzX",{"children":[["$","$Ld",null,{"children":"$Le"}],["$","$Lf",null,{"children":"$L10"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
12:I[53704,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","2154","static/chunks/834cb1aa-fe75579b2a50baac.js","3524","static/chunks/2170a4aa-66be1631595ccab0.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","1057","static/chunks/1057-d97430463abd6821.js","2282","static/chunks/2282-26bc5318a4471ee9.js","9234","static/chunks/9234-fce85e807baa599f.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","5733","static/chunks/5733-d0ad15157d7394e5.js","8173","static/chunks/8173-30737ce2fc776efb.js","613","static/chunks/613-3467f3d6fe7e6e6a.js","9756","static/chunks/9756-90c6220c809c4148.js","8738","static/chunks/8738-58586275b0d791e8.js","2649","static/chunks/2649-8ae63f8e6332939b.js","1857","static/chunks/1857-99747bd4076c313b.js","2288","static/chunks/2288-ffb609d77f258e27.js","6387","static/chunks/app/items/%5Bslug%5D/page-566168fa6ba89e63.js"],""]
14:I[5749,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","2154","static/chunks/834cb1aa-fe75579b2a50baac.js","3524","static/chunks/2170a4aa-66be1631595ccab0.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","1057","static/chunks/1057-d97430463abd6821.js","2282","static/chunks/2282-26bc5318a4471ee9.js","9234","static/chunks/9234-fce85e807baa599f.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","5733","static/chunks/5733-d0ad15157d7394e5.js","8173","static/chunks/8173-30737ce2fc776efb.js","613","static/chunks/613-3467f3d6fe7e6e6a.js","9756","static/chunks/9756-90c6220c809c4148.js","8738","static/chunks/8738-58586275b0d791e8.js","2649","static/chunks/2649-8ae63f8e6332939b.js","1857","static/chunks/1857-99747bd4076c313b.js","2288","static/chunks/2288-ffb609d77f258e27.js","6387","static/chunks/app/items/%5Bslug%5D/page-566168fa6ba89e63.js"],"default"]
13:T134b,{"@context":"https://schema.org/","@type":"Dataset","name":"Spatial and Temporal Visual Integration, 2021","description":"We present four psychophysics experiments investigating spatiotemporal summation in various visual contexts. The experiments are 4AFC detection tasks where a target is briefly represented in one of four known locations. Stimuli consist of targets with various spatial (0 to .9dva) or temporal (0 to 100ms) properties.  Staircase procedures are used to identify luminance thresholds at which the targets are detected with 75% accuracy.  Lower thresholds are taken as indictive of greater summation than higher thresholds.  \nIn the first experiment, the target stimuli consist of two probes presented with varying spatial (0 to .9dva) and temporal (0 to 100ms) separation.  We find an interaction between spatial and temporal integration: as spatial separation increased, the temporal separation at which the probes are most easily detected also increased. That is, probes with moderate spatial separation are more easily detected when they also have temporal separation compared to when the two probes are presented simultaneously.  \nIn a second study, targets consist of a single probe, presented for various durations (8 to 100ms), with the aim of identifying the critical period during which complete summation occurs (e.g., Bloch’s critical duration).  Similarly, a third experiment used targets presented for 8ms with varying lengths of .01 to .9dva to identify the spatial area of complete summation (e.g., Ricco’s area). \nA fourth study builds on the first experiment, but rather than a single target appearing in one of four locations, three targets are presented and participants identify the location which did NOT contain a target.  Again, we find that for conditions with spatial separation, participants had higher lower thresholds when the probes were also temporally separated.  \nExperiments 1 and 4 provide robust evidence that detection thresholds can be lower for temporally separated targets than for concurrently presented targets.  The results from experiments 1 and 4 do not align with the Ricco area or Bloch’s critical duration.  However, the results can be interpreted in terms of a facilitating effect from motion detectors.  Experiment 4 suggests that this effect is the product of multiple local mechanisms, rather than due to some global motion processing.Every time we move, the image of the world at the back of the eye changes. Despite this, our perception is of an unchanging world. How does the brain translate a continually changing image into a percept of a stable, stationary, rigid world? Does the brain use a map of the external environment (an \"allocentric map\") and the position of the observer within it, built up over time, to underpin the perception of stability? Does the brain continually update a map of where scene objects are relative to the observer (an \"egocentric map\"; e.g. there is an object straight ahead of me, if I walk forward I should expect it to get closer to me)? Does the brain not create a map but just divide up the image motion into that which is likely due to movement of the observer (and which can consequently be ignored) and that which is likely due to objects moving within the scene (which become a focus of attention)? The hypothesis that underpins this research project is that it is not a single one of these mechanisms that underpins perceptual stability, but that all of them, their contribution dependent on the task being performed by the observer. In some cases the task will require a fast estimate to support an ongoing action which might favour one mechanism, on another task, where timing is not so critical, a slower, but more accurate, mechanism might be more appropriate. This collaborative project, which combines complementary expertise in Psychology, Movement Sciences, and Computing from Germany, The Netherlands and the United Kingdom, and importantly, researchers that start from different theoretical perspectives, will test this hypothesis. We will study a diverse series of tasks that present a range of challenges to the moving observer. We will make use of various innovative experimental paradigms that exploit recent technological advances such as virtual reality combined with simultaneous motion tracking. Understanding where and how different mechanisms of perceptual stability play a role advances not only our scientific understanding, but also has the potential to inform industry as well as medicine about the circumstances in which disorientation or nausea in real or virtual environments can be minimised.","url":"https://harmonydata.ac.uk/search/items/spatial-and-temporal-visual-integration-2021","identifier":["http://dx.doi.org/10.5255/UKDA-SN-855535"],"keywords":["PSYCHOLOGY","PSYCHOLOGICAL EFFECTS","PSYCHOLOGICAL RESEARCH","HUMAN BEHAVIOUR"],"temporalCoverage":"2019-03-27/2023-08-31"}15:T11a9,We present four psychophysics experiments investigating spatiotemporal summation in various visual contexts. The experiments are 4AFC detection tasks where a target is briefly represented in one of four known locations. Stimuli consist of targets with various spatial (0 to .9dva) or temporal (0 to 100ms) properties.  Staircase procedures are used to identify luminance thresholds at which the targets are detected with 75% accuracy.  Lower thresholds are taken as indictive of greater summation than higher thresholds.  
In the first experiment, the target stimuli consist of two probes presented with varying spatial (0 to .9dva) and temporal (0 to 100ms) separation.  We find an interaction between spatial and temporal integration: as spatial separation increased, the temporal separation at which the probes are most easily detected also increased. That is, probes with moderate spatial separation are more easily detected when they also have temporal separation compared to when the two probes are presented simultaneously.  
In a second study, targets consist of a single probe, presented for various durations (8 to 100ms), with the aim of identifying the critical period during which complete summation occurs (e.g., Bloch’s critical duration).  Similarly, a third experiment used targets presented for 8ms with varying lengths of .01 to .9dva to identify the spatial area of complete summation (e.g., Ricco’s area). 
A fourth study builds on the first experiment, but rather than a single target appearing in one of four locations, three targets are presented and participants identify the location which did NOT contain a target.  Again, we find that for conditions with spatial separation, participants had higher lower thresholds when the probes were also temporally separated.  
Experiments 1 and 4 provide robust evidence that detection thresholds can be lower for temporally separated targets than for concurrently presented targets.  The results from experiments 1 and 4 do not align with the Ricco area or Bloch’s critical duration.  However, the results can be interpreted in terms of a facilitating effect from motion detectors.  Experiment 4 suggests that this effect is the product of multiple local mechanisms, rather than due to some global motion processing.Every time we move, the image of the world at the back of the eye changes. Despite this, our perception is of an unchanging world. How does the brain translate a continually changing image into a percept of a stable, stationary, rigid world? Does the brain use a map of the external environment (an "allocentric map") and the position of the observer within it, built up over time, to underpin the perception of stability? Does the brain continually update a map of where scene objects are relative to the observer (an "egocentric map"; e.g. there is an object straight ahead of me, if I walk forward I should expect it to get closer to me)? Does the brain not create a map but just divide up the image motion into that which is likely due to movement of the observer (and which can consequently be ignored) and that which is likely due to objects moving within the scene (which become a focus of attention)? The hypothesis that underpins this research project is that it is not a single one of these mechanisms that underpins perceptual stability, but that all of them, their contribution dependent on the task being performed by the observer. In some cases the task will require a fast estimate to support an ongoing action which might favour one mechanism, on another task, where timing is not so critical, a slower, but more accurate, mechanism might be more appropriate. This collaborative project, which combines complementary expertise in Psychology, Movement Sciences, and Computing from Germany, The Netherlands and the United Kingdom, and importantly, researchers that start from different theoretical perspectives, will test this hypothesis. We will study a diverse series of tasks that present a range of challenges to the moving observer. We will make use of various innovative experimental paradigms that exploit recent technological advances such as virtual reality combined with simultaneous motion tracking. Understanding where and how different mechanisms of perceptual stability play a role advances not only our scientific understanding, but also has the potential to inform industry as well as medicine about the circumstances in which disorientation or nausea in real or virtual environments can be minimised.a:[["$","$L12",null,{"strategy":"beforeInteractive","id":"structured-data","type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$13"}}],["$","$L14",null,{"dataset":{"title":"Spatial and Temporal Visual Integration, 2021","description":"$15","image":"$undefined","publisher":"$undefined","funders":"$undefined","geographicCoverage":"GB","temporalCoverage":"2019-03-27/2023-08-31","ageCoverage":"$undefined","studyDesign":[],"resourceType":"dataset","topics":["PSYCHOLOGY","PSYCHOLOGICAL EFFECTS","PSYCHOLOGICAL RESEARCH","HUMAN BEHAVIOUR"],"instruments":[],"dataCatalogs":[{"name":"UK Data Service","url":"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=855535","logo":"$undefined"}],"matchedVariables":[],"allVariables":[],"additionalLinks":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=855535","https://reshare.ukdataservice.ac.uk/855535","http://dx.doi.org/10.5255/UKDA-SN-855535","http://dx.doi.org/10.5255/UKDA-SN-855535"],"child_datasets":[],"aiSummary":null}}]]
10:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
16:T11a9,We present four psychophysics experiments investigating spatiotemporal summation in various visual contexts. The experiments are 4AFC detection tasks where a target is briefly represented in one of four known locations. Stimuli consist of targets with various spatial (0 to .9dva) or temporal (0 to 100ms) properties.  Staircase procedures are used to identify luminance thresholds at which the targets are detected with 75% accuracy.  Lower thresholds are taken as indictive of greater summation than higher thresholds.  
In the first experiment, the target stimuli consist of two probes presented with varying spatial (0 to .9dva) and temporal (0 to 100ms) separation.  We find an interaction between spatial and temporal integration: as spatial separation increased, the temporal separation at which the probes are most easily detected also increased. That is, probes with moderate spatial separation are more easily detected when they also have temporal separation compared to when the two probes are presented simultaneously.  
In a second study, targets consist of a single probe, presented for various durations (8 to 100ms), with the aim of identifying the critical period during which complete summation occurs (e.g., Bloch’s critical duration).  Similarly, a third experiment used targets presented for 8ms with varying lengths of .01 to .9dva to identify the spatial area of complete summation (e.g., Ricco’s area). 
A fourth study builds on the first experiment, but rather than a single target appearing in one of four locations, three targets are presented and participants identify the location which did NOT contain a target.  Again, we find that for conditions with spatial separation, participants had higher lower thresholds when the probes were also temporally separated.  
Experiments 1 and 4 provide robust evidence that detection thresholds can be lower for temporally separated targets than for concurrently presented targets.  The results from experiments 1 and 4 do not align with the Ricco area or Bloch’s critical duration.  However, the results can be interpreted in terms of a facilitating effect from motion detectors.  Experiment 4 suggests that this effect is the product of multiple local mechanisms, rather than due to some global motion processing.Every time we move, the image of the world at the back of the eye changes. Despite this, our perception is of an unchanging world. How does the brain translate a continually changing image into a percept of a stable, stationary, rigid world? Does the brain use a map of the external environment (an "allocentric map") and the position of the observer within it, built up over time, to underpin the perception of stability? Does the brain continually update a map of where scene objects are relative to the observer (an "egocentric map"; e.g. there is an object straight ahead of me, if I walk forward I should expect it to get closer to me)? Does the brain not create a map but just divide up the image motion into that which is likely due to movement of the observer (and which can consequently be ignored) and that which is likely due to objects moving within the scene (which become a focus of attention)? The hypothesis that underpins this research project is that it is not a single one of these mechanisms that underpins perceptual stability, but that all of them, their contribution dependent on the task being performed by the observer. In some cases the task will require a fast estimate to support an ongoing action which might favour one mechanism, on another task, where timing is not so critical, a slower, but more accurate, mechanism might be more appropriate. This collaborative project, which combines complementary expertise in Psychology, Movement Sciences, and Computing from Germany, The Netherlands and the United Kingdom, and importantly, researchers that start from different theoretical perspectives, will test this hypothesis. We will study a diverse series of tasks that present a range of challenges to the moving observer. We will make use of various innovative experimental paradigms that exploit recent technological advances such as virtual reality combined with simultaneous motion tracking. Understanding where and how different mechanisms of perceptual stability play a role advances not only our scientific understanding, but also has the potential to inform industry as well as medicine about the circumstances in which disorientation or nausea in real or virtual environments can be minimised.17:T11a9,We present four psychophysics experiments investigating spatiotemporal summation in various visual contexts. The experiments are 4AFC detection tasks where a target is briefly represented in one of four known locations. Stimuli consist of targets with various spatial (0 to .9dva) or temporal (0 to 100ms) properties.  Staircase procedures are used to identify luminance thresholds at which the targets are detected with 75% accuracy.  Lower thresholds are taken as indictive of greater summation than higher thresholds.  
In the first experiment, the target stimuli consist of two probes presented with varying spatial (0 to .9dva) and temporal (0 to 100ms) separation.  We find an interaction between spatial and temporal integration: as spatial separation increased, the temporal separation at which the probes are most easily detected also increased. That is, probes with moderate spatial separation are more easily detected when they also have temporal separation compared to when the two probes are presented simultaneously.  
In a second study, targets consist of a single probe, presented for various durations (8 to 100ms), with the aim of identifying the critical period during which complete summation occurs (e.g., Bloch’s critical duration).  Similarly, a third experiment used targets presented for 8ms with varying lengths of .01 to .9dva to identify the spatial area of complete summation (e.g., Ricco’s area). 
A fourth study builds on the first experiment, but rather than a single target appearing in one of four locations, three targets are presented and participants identify the location which did NOT contain a target.  Again, we find that for conditions with spatial separation, participants had higher lower thresholds when the probes were also temporally separated.  
Experiments 1 and 4 provide robust evidence that detection thresholds can be lower for temporally separated targets than for concurrently presented targets.  The results from experiments 1 and 4 do not align with the Ricco area or Bloch’s critical duration.  However, the results can be interpreted in terms of a facilitating effect from motion detectors.  Experiment 4 suggests that this effect is the product of multiple local mechanisms, rather than due to some global motion processing.Every time we move, the image of the world at the back of the eye changes. Despite this, our perception is of an unchanging world. How does the brain translate a continually changing image into a percept of a stable, stationary, rigid world? Does the brain use a map of the external environment (an "allocentric map") and the position of the observer within it, built up over time, to underpin the perception of stability? Does the brain continually update a map of where scene objects are relative to the observer (an "egocentric map"; e.g. there is an object straight ahead of me, if I walk forward I should expect it to get closer to me)? Does the brain not create a map but just divide up the image motion into that which is likely due to movement of the observer (and which can consequently be ignored) and that which is likely due to objects moving within the scene (which become a focus of attention)? The hypothesis that underpins this research project is that it is not a single one of these mechanisms that underpins perceptual stability, but that all of them, their contribution dependent on the task being performed by the observer. In some cases the task will require a fast estimate to support an ongoing action which might favour one mechanism, on another task, where timing is not so critical, a slower, but more accurate, mechanism might be more appropriate. This collaborative project, which combines complementary expertise in Psychology, Movement Sciences, and Computing from Germany, The Netherlands and the United Kingdom, and importantly, researchers that start from different theoretical perspectives, will test this hypothesis. We will study a diverse series of tasks that present a range of challenges to the moving observer. We will make use of various innovative experimental paradigms that exploit recent technological advances such as virtual reality combined with simultaneous motion tracking. Understanding where and how different mechanisms of perceptual stability play a role advances not only our scientific understanding, but also has the potential to inform industry as well as medicine about the circumstances in which disorientation or nausea in real or virtual environments can be minimised.18:T11a9,We present four psychophysics experiments investigating spatiotemporal summation in various visual contexts. The experiments are 4AFC detection tasks where a target is briefly represented in one of four known locations. Stimuli consist of targets with various spatial (0 to .9dva) or temporal (0 to 100ms) properties.  Staircase procedures are used to identify luminance thresholds at which the targets are detected with 75% accuracy.  Lower thresholds are taken as indictive of greater summation than higher thresholds.  
In the first experiment, the target stimuli consist of two probes presented with varying spatial (0 to .9dva) and temporal (0 to 100ms) separation.  We find an interaction between spatial and temporal integration: as spatial separation increased, the temporal separation at which the probes are most easily detected also increased. That is, probes with moderate spatial separation are more easily detected when they also have temporal separation compared to when the two probes are presented simultaneously.  
In a second study, targets consist of a single probe, presented for various durations (8 to 100ms), with the aim of identifying the critical period during which complete summation occurs (e.g., Bloch’s critical duration).  Similarly, a third experiment used targets presented for 8ms with varying lengths of .01 to .9dva to identify the spatial area of complete summation (e.g., Ricco’s area). 
A fourth study builds on the first experiment, but rather than a single target appearing in one of four locations, three targets are presented and participants identify the location which did NOT contain a target.  Again, we find that for conditions with spatial separation, participants had higher lower thresholds when the probes were also temporally separated.  
Experiments 1 and 4 provide robust evidence that detection thresholds can be lower for temporally separated targets than for concurrently presented targets.  The results from experiments 1 and 4 do not align with the Ricco area or Bloch’s critical duration.  However, the results can be interpreted in terms of a facilitating effect from motion detectors.  Experiment 4 suggests that this effect is the product of multiple local mechanisms, rather than due to some global motion processing.Every time we move, the image of the world at the back of the eye changes. Despite this, our perception is of an unchanging world. How does the brain translate a continually changing image into a percept of a stable, stationary, rigid world? Does the brain use a map of the external environment (an "allocentric map") and the position of the observer within it, built up over time, to underpin the perception of stability? Does the brain continually update a map of where scene objects are relative to the observer (an "egocentric map"; e.g. there is an object straight ahead of me, if I walk forward I should expect it to get closer to me)? Does the brain not create a map but just divide up the image motion into that which is likely due to movement of the observer (and which can consequently be ignored) and that which is likely due to objects moving within the scene (which become a focus of attention)? The hypothesis that underpins this research project is that it is not a single one of these mechanisms that underpins perceptual stability, but that all of them, their contribution dependent on the task being performed by the observer. In some cases the task will require a fast estimate to support an ongoing action which might favour one mechanism, on another task, where timing is not so critical, a slower, but more accurate, mechanism might be more appropriate. This collaborative project, which combines complementary expertise in Psychology, Movement Sciences, and Computing from Germany, The Netherlands and the United Kingdom, and importantly, researchers that start from different theoretical perspectives, will test this hypothesis. We will study a diverse series of tasks that present a range of challenges to the moving observer. We will make use of various innovative experimental paradigms that exploit recent technological advances such as virtual reality combined with simultaneous motion tracking. Understanding where and how different mechanisms of perceptual stability play a role advances not only our scientific understanding, but also has the potential to inform industry as well as medicine about the circumstances in which disorientation or nausea in real or virtual environments can be minimised.e:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Spatial and Temporal Visual Integration, 2021"}],["$","meta","2",{"name":"description","content":"$16"}],["$","meta","3",{"property":"og:title","content":"Spatial and Temporal Visual Integration, 2021"}],["$","meta","4",{"property":"og:description","content":"$17"}],["$","meta","5",{"property":"og:url","content":"https://harmonydata.ac.uk/search/items/spatial-and-temporal-visual-integration-2021"}],["$","meta","6",{"property":"og:site_name","content":"Academic Resource Discovery"}],["$","meta","7",{"property":"og:locale","content":"en_US"}],["$","meta","8",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","meta","9",{"property":"og:image:width","content":"1200"}],["$","meta","10",{"property":"og:image:height","content":"630"}],["$","meta","11",{"property":"og:image:alt","content":"Spatial and Temporal Visual Integration, 2021"}],["$","meta","12",{"property":"og:type","content":"website"}],["$","meta","13",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","14",{"name":"twitter:title","content":"Spatial and Temporal Visual Integration, 2021"}],["$","meta","15",{"name":"twitter:description","content":"$18"}],["$","meta","16",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","link","17",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
c:null
