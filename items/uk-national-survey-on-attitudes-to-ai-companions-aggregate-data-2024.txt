1:"$Sreact.fragment"
2:I[52332,["9692","static/js/9692.83f9877c.js","1828","static/js/1828.31087444.js","7213","static/js/7213.f8248d79.js","690","static/js/690.e023e61b.js","7133","static/js/7133.521b2ecd.js","9829","static/js/9829.124a89b0.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","5906","static/js/5906.206ff298.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.079f6f03.js"],"default"]
3:I[65380,["9692","static/js/9692.83f9877c.js","1828","static/js/1828.31087444.js","7213","static/js/7213.f8248d79.js","690","static/js/690.e023e61b.js","7133","static/js/7133.521b2ecd.js","9829","static/js/9829.124a89b0.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","5906","static/js/5906.206ff298.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.079f6f03.js"],"AuthProvider"]
4:I[41627,["9692","static/js/9692.83f9877c.js","1828","static/js/1828.31087444.js","7213","static/js/7213.f8248d79.js","690","static/js/690.e023e61b.js","7133","static/js/7133.521b2ecd.js","9829","static/js/9829.124a89b0.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","5906","static/js/5906.206ff298.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.079f6f03.js"],"FirebaseProvider"]
5:"$Sreact.suspense"
6:I[92114,["9692","static/js/9692.83f9877c.js","1828","static/js/1828.31087444.js","7213","static/js/7213.f8248d79.js","690","static/js/690.e023e61b.js","7133","static/js/7133.521b2ecd.js","9829","static/js/9829.124a89b0.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","5906","static/js/5906.206ff298.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.079f6f03.js"],"SearchProvider"]
7:I[94049,["9692","static/js/9692.83f9877c.js","1828","static/js/1828.31087444.js","7213","static/js/7213.f8248d79.js","690","static/js/690.e023e61b.js","7133","static/js/7133.521b2ecd.js","9829","static/js/9829.124a89b0.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","5906","static/js/5906.206ff298.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.079f6f03.js"],"default"]
8:I[20190,["9692","static/js/9692.83f9877c.js","1828","static/js/1828.31087444.js","7213","static/js/7213.f8248d79.js","690","static/js/690.e023e61b.js","7133","static/js/7133.521b2ecd.js","9829","static/js/9829.124a89b0.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","5906","static/js/5906.206ff298.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.079f6f03.js"],"default"]
9:I[9766,[],""]
a:I[98924,[],""]
b:I[74744,["9692","static/js/9692.83f9877c.js","1828","static/js/1828.31087444.js","7213","static/js/7213.f8248d79.js","690","static/js/690.e023e61b.js","7133","static/js/7133.521b2ecd.js","9829","static/js/9829.124a89b0.js","2619","static/js/2619.b8db57ac.js","3820","static/js/3820.af314958.js","5906","static/js/5906.206ff298.js","5738","static/js/5738.d28a9943.js","7177","static/js/app/layout.079f6f03.js"],"ToastContainer"]
d:I[24431,[],"OutletBoundary"]
f:I[15278,[],"AsyncMetadataOutlet"]
11:I[24431,[],"ViewportBoundary"]
13:I[24431,[],"MetadataBoundary"]
15:I[57150,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/e446a64f2ff89daf.css","style"]
0:{"P":null,"b":"669YHYfowfluJ5cB834U0","p":"/search","c":["","items","uk-national-survey-on-attitudes-to-ai-companions-aggregate-data-2024"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","uk-national-survey-on-attitudes-to-ai-companions-aggregate-data-2024","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/e446a64f2ff89daf.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":[["$","meta",null,{"name":"emotion-insertion-point","content":""}],["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"preconnect","href":"https://www.cataloguementalhealth.ac.uk"}],["$","link",null,{"rel":"dns-prefetch","href":"https://harmonydata.ac.uk"}],["$","style",null,{"dangerouslySetInnerHTML":{"__html":"\n            /* Ensure immediate rendering with Roboto and fallbacks */\n            * { \n              font-family: \"Roboto\", -apple-system, BlinkMacSystemFont, \"Segoe UI\", \"Oxygen\", \"Ubuntu\", \"Cantarell\", \"Fira Sans\", \"Droid Sans\", \"Helvetica Neue\", sans-serif !important;\n              font-display: swap;\n              -webkit-font-smoothing: antialiased;\n              -moz-osx-font-smoothing: grayscale;\n            }\n            body { \n              visibility: visible !important; \n              opacity: 1 !important; \n              margin: 0; \n              padding: 0; \n            }\n          "}}]]}],["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":["$","$5",null,{"fallback":["$","div",null,{"children":"Loading..."}],"children":["$","$L6",null,{"children":[["$","$L7",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L8",null,{}],["$","$L7",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L9",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$Lb",null,{"position":"bottom-right"}]]}]}]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L9",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","uk-national-survey-on-attitudes-to-ai-companions-aggregate-data-2024","d"],["$","$1","c",{"children":[null,["$","$L9",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$La",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$Lc",null,["$","$Ld",null,{"children":["$Le",["$","$Lf",null,{"promise":"$@10"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,[["$","$L11",null,{"children":"$L12"}],["$","meta",null,{"name":"next-size-adjust","content":""}]],["$","$L13",null,{"children":["$","div",null,{"hidden":true,"children":["$","$5",null,{"fallback":null,"children":"$L14"}]}]}]]}],false]],"m":"$undefined","G":["$15",[]],"s":false,"S":true}
16:I[41402,["9692","static/js/9692.83f9877c.js","1828","static/js/1828.31087444.js","690","static/js/690.e023e61b.js","7133","static/js/7133.521b2ecd.js","9829","static/js/9829.124a89b0.js","867","static/js/867.7f6bef5e.js","2939","static/js/2939.aa50df5c.js","5183","static/js/5183.9f1a7545.js","5738","static/js/5738.d28a9943.js","3055","static/js/3055.87b66c06.js","8977","static/js/8977.89625695.js","6387","static/js/app/items/%5Bslug%5D/page.cedc9485.js"],""]
17:T100f,{"@context":"https://schema.org/","@type":"Dataset","name":"UK National Survey on Attitudes to AI Companions: Aggregate Data, 2024","description":"To ascertain how the British public feel about AI companions, we conducted a UK-wide demographically representative national survey (implemented by professional company Walnut Unlimited - a human understanding agency, part of the Unlimited Group), across 10-12 December 2024, 2073 respondents aged 18 years or over, online omnibus). This was a part of a Responsible AI award, to create soft governance of autonomous systems that interact with human emotions and/or emulate empathy.\n\nThe survey asks 22 closed-ended, multiple-choice questions on AI companions. The first set of questions (Q.1-2) glean participants’ familiarity with, and usage of, companion apps. The second set of questions (Q.3-4) explore the acceptability of design features of AI companions. The third set of questions (Q.5-7) explore the broad benefits and concerns from using AI companions. The fourth set of questions (Q.8-13) explore views on children and companion apps. The fifth set of questions (Q.14-15) explore views on older adults and companion apps. The sixth set of questions (Q.16-18) explore views on mental health issues and companion apps. The seventh set of questions (Q.19-21) explore views on desired governance of companion apps to consider the practicalities of what societies should do about AI companions, if anything. The final question (Q.22) is an evaluative question on whether participants feel AI companions are generally a positive or negative addition to society.Funded by the Responsible AI UK Impact Accelerator, project AEGIS sees Bangor University's Emotional AI Lab partnering with Japan’s National Institute of Informatics, the Institute of Electrical and Electronics Engineers (IEEE), Monash University (Indonesia) and engaging the Information Commissioner's Office (UK). \nThe goal of AEGIS is to host a series of workshops, assemble a diverse expert working group and develop a ‘technical standard’ to address use of emulated empathy in general-purpose artificial intelligence systems for human-AI partnerships. \nProvisionally titled Recommended Practice for Ethical Considerations of Emulated Empathy in Partner-based General-Purpose Artificial Intelligence Systems, this IEEE standard will define ethical considerations, detail good practices, and augment and complement international human rights and regional law. \nUse cases encompass general-purpose artificial intelligence products marketed as ‘empathic partners’, ‘personal AI’, ‘co-pilots’, ‘assistants’, and related phrasing for ‘human-AI partnering’. Current and nascent domains of use include work, therapy, education, life coaching, legal problems, fitness, entertainment, and more. \nThese systems raise ethical questions that are global in nature, yet benefitting from diverse ethical approaches, especially where systems feed into the design of human-centered technologies. Some ethical questions are familiar (e.g. transparency, accountability, bias and fairness), but others are specific and unique, including psychological interactions and dependencies, child appropriateness, fiduciary issues, animism, and manipulation through partnerships with general-purpose artificial intelligence systems.\nThe project augments the Emotional AI Lab's UK-Japan social science work by conducting a UK national demographically representative survey, and considering results in light of studies on AI ethics. It also sees global value in drawing a range of ethical frames of reference by which to account for human-AI partnerships, not least Japan and ethically aligned regions, given their long-standing interests in human-technology partnerships.","url":"https://harmonydata.ac.uk/search/items/uk-national-survey-on-attitudes-to-ai-companions-aggregate-data-2024","identifier":["http://dx.doi.org/10.5255/UKDA-SN-857731"],"keywords":["AI","PUBLIC OPINION","ETHICS","REGULATIONS","CHILD SAFETY","OLD AGE","MENTAL HEALTH"],"temporalCoverage":"2024-12-10/2024-12-12"}c:["$","$5",null,{"fallback":["$","div",null,{"children":"Loading..."}],"children":[["$","$L16",null,{"strategy":"beforeInteractive","id":"structured-data","type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$17"}}],"$L18"]}]
19:I[78977,["9692","static/js/9692.83f9877c.js","1828","static/js/1828.31087444.js","690","static/js/690.e023e61b.js","7133","static/js/7133.521b2ecd.js","9829","static/js/9829.124a89b0.js","867","static/js/867.7f6bef5e.js","2939","static/js/2939.aa50df5c.js","5183","static/js/5183.9f1a7545.js","5738","static/js/5738.d28a9943.js","3055","static/js/3055.87b66c06.js","8977","static/js/8977.89625695.js","6387","static/js/app/items/%5Bslug%5D/page.cedc9485.js"],"default"]
1a:Te37,To ascertain how the British public feel about AI companions, we conducted a UK-wide demographically representative national survey (implemented by professional company Walnut Unlimited - a human understanding agency, part of the Unlimited Group), across 10-12 December 2024, 2073 respondents aged 18 years or over, online omnibus). This was a part of a Responsible AI award, to create soft governance of autonomous systems that interact with human emotions and/or emulate empathy.

The survey asks 22 closed-ended, multiple-choice questions on AI companions. The first set of questions (Q.1-2) glean participants’ familiarity with, and usage of, companion apps. The second set of questions (Q.3-4) explore the acceptability of design features of AI companions. The third set of questions (Q.5-7) explore the broad benefits and concerns from using AI companions. The fourth set of questions (Q.8-13) explore views on children and companion apps. The fifth set of questions (Q.14-15) explore views on older adults and companion apps. The sixth set of questions (Q.16-18) explore views on mental health issues and companion apps. The seventh set of questions (Q.19-21) explore views on desired governance of companion apps to consider the practicalities of what societies should do about AI companions, if anything. The final question (Q.22) is an evaluative question on whether participants feel AI companions are generally a positive or negative addition to society.Funded by the Responsible AI UK Impact Accelerator, project AEGIS sees Bangor University's Emotional AI Lab partnering with Japan’s National Institute of Informatics, the Institute of Electrical and Electronics Engineers (IEEE), Monash University (Indonesia) and engaging the Information Commissioner's Office (UK). 
The goal of AEGIS is to host a series of workshops, assemble a diverse expert working group and develop a ‘technical standard’ to address use of emulated empathy in general-purpose artificial intelligence systems for human-AI partnerships. 
Provisionally titled Recommended Practice for Ethical Considerations of Emulated Empathy in Partner-based General-Purpose Artificial Intelligence Systems, this IEEE standard will define ethical considerations, detail good practices, and augment and complement international human rights and regional law. 
Use cases encompass general-purpose artificial intelligence products marketed as ‘empathic partners’, ‘personal AI’, ‘co-pilots’, ‘assistants’, and related phrasing for ‘human-AI partnering’. Current and nascent domains of use include work, therapy, education, life coaching, legal problems, fitness, entertainment, and more. 
These systems raise ethical questions that are global in nature, yet benefitting from diverse ethical approaches, especially where systems feed into the design of human-centered technologies. Some ethical questions are familiar (e.g. transparency, accountability, bias and fairness), but others are specific and unique, including psychological interactions and dependencies, child appropriateness, fiduciary issues, animism, and manipulation through partnerships with general-purpose artificial intelligence systems.
The project augments the Emotional AI Lab's UK-Japan social science work by conducting a UK national demographically representative survey, and considering results in light of studies on AI ethics. It also sees global value in drawing a range of ethical frames of reference by which to account for human-AI partnerships, not least Japan and ethically aligned regions, given their long-standing interests in human-technology partnerships.18:["$","$L19",null,{"study":{"dataset_schema":{"@context":"https://schema.org/","@type":"Dataset","name":"UK National Survey on Attitudes to AI Companions: Aggregate Data, 2024","description":"$1a","url":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=857731","https://reshare.ukdataservice.ac.uk/857731"],"keywords":["AI","PUBLIC OPINION","ETHICS","REGULATIONS","CHILD SAFETY","OLD AGE","MENTAL HEALTH"],"identifier":["http://dx.doi.org/10.5255/UKDA-SN-857731"],"includedInDataCatalog":[{"@type":"DataCatalog","name":"UK Data Service","url":"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=857731"}],"sponsor":[{"@type":"Organization","name":"Engineering and Physical Sciences Research Council"}],"temporalCoverage":"2024-12-10/2024-12-12"},"extra_data":{"num_variables":null,"harmony_id":"ukds/857731","start_year":2024,"end_year":2024,"data_access":"The Data Collection is available to any user without the requirement for registration for download/access. Commercial Use of data is not permitted.","urls":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=857731","https://reshare.ukdataservice.ac.uk/857731"],"geographic_coverage":"UK","source":["ukds"],"genetic_data_collected":false,"slug":"uk-national-survey-on-attitudes-to-ai-companions-aggregate-data-2024","dois":["http://dx.doi.org/10.5255/UKDA-SN-857731"],"sex":"all","instruments":[],"language_codes":["en"],"ai_summary":null,"duration_years":null,"resource_type":"dataset","country_codes":["GB"],"study_design":[],"name":"UK National Survey on Attitudes to AI Companions: Aggregate Data, 2024","uuid":"602235458f990a903064c001543181a7"},"distance":0,"score":0,"parent":{},"ancestors":[]}}]
12:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
e:null
1b:Te37,To ascertain how the British public feel about AI companions, we conducted a UK-wide demographically representative national survey (implemented by professional company Walnut Unlimited - a human understanding agency, part of the Unlimited Group), across 10-12 December 2024, 2073 respondents aged 18 years or over, online omnibus). This was a part of a Responsible AI award, to create soft governance of autonomous systems that interact with human emotions and/or emulate empathy.

The survey asks 22 closed-ended, multiple-choice questions on AI companions. The first set of questions (Q.1-2) glean participants’ familiarity with, and usage of, companion apps. The second set of questions (Q.3-4) explore the acceptability of design features of AI companions. The third set of questions (Q.5-7) explore the broad benefits and concerns from using AI companions. The fourth set of questions (Q.8-13) explore views on children and companion apps. The fifth set of questions (Q.14-15) explore views on older adults and companion apps. The sixth set of questions (Q.16-18) explore views on mental health issues and companion apps. The seventh set of questions (Q.19-21) explore views on desired governance of companion apps to consider the practicalities of what societies should do about AI companions, if anything. The final question (Q.22) is an evaluative question on whether participants feel AI companions are generally a positive or negative addition to society.Funded by the Responsible AI UK Impact Accelerator, project AEGIS sees Bangor University's Emotional AI Lab partnering with Japan’s National Institute of Informatics, the Institute of Electrical and Electronics Engineers (IEEE), Monash University (Indonesia) and engaging the Information Commissioner's Office (UK). 
The goal of AEGIS is to host a series of workshops, assemble a diverse expert working group and develop a ‘technical standard’ to address use of emulated empathy in general-purpose artificial intelligence systems for human-AI partnerships. 
Provisionally titled Recommended Practice for Ethical Considerations of Emulated Empathy in Partner-based General-Purpose Artificial Intelligence Systems, this IEEE standard will define ethical considerations, detail good practices, and augment and complement international human rights and regional law. 
Use cases encompass general-purpose artificial intelligence products marketed as ‘empathic partners’, ‘personal AI’, ‘co-pilots’, ‘assistants’, and related phrasing for ‘human-AI partnering’. Current and nascent domains of use include work, therapy, education, life coaching, legal problems, fitness, entertainment, and more. 
These systems raise ethical questions that are global in nature, yet benefitting from diverse ethical approaches, especially where systems feed into the design of human-centered technologies. Some ethical questions are familiar (e.g. transparency, accountability, bias and fairness), but others are specific and unique, including psychological interactions and dependencies, child appropriateness, fiduciary issues, animism, and manipulation through partnerships with general-purpose artificial intelligence systems.
The project augments the Emotional AI Lab's UK-Japan social science work by conducting a UK national demographically representative survey, and considering results in light of studies on AI ethics. It also sees global value in drawing a range of ethical frames of reference by which to account for human-AI partnerships, not least Japan and ethically aligned regions, given their long-standing interests in human-technology partnerships.10:{"metadata":[["$","title","0",{"children":"UK National Survey on Attitudes to AI Companions: Aggregate Data, 2024"}],["$","meta","1",{"name":"description","content":"$1b"}],"$L1c","$L1d","$L1e","$L1f","$L20","$L21","$L22","$L23","$L24","$L25","$L26","$L27","$L28","$L29","$L2a","$L2b"],"error":null,"digest":"$undefined"}
2e:I[80622,[],"IconMark"]
1c:["$","meta","2",{"property":"og:title","content":"UK National Survey on Attitudes to AI Companions: Aggregate Data, 2024"}]
2c:Te37,To ascertain how the British public feel about AI companions, we conducted a UK-wide demographically representative national survey (implemented by professional company Walnut Unlimited - a human understanding agency, part of the Unlimited Group), across 10-12 December 2024, 2073 respondents aged 18 years or over, online omnibus). This was a part of a Responsible AI award, to create soft governance of autonomous systems that interact with human emotions and/or emulate empathy.

The survey asks 22 closed-ended, multiple-choice questions on AI companions. The first set of questions (Q.1-2) glean participants’ familiarity with, and usage of, companion apps. The second set of questions (Q.3-4) explore the acceptability of design features of AI companions. The third set of questions (Q.5-7) explore the broad benefits and concerns from using AI companions. The fourth set of questions (Q.8-13) explore views on children and companion apps. The fifth set of questions (Q.14-15) explore views on older adults and companion apps. The sixth set of questions (Q.16-18) explore views on mental health issues and companion apps. The seventh set of questions (Q.19-21) explore views on desired governance of companion apps to consider the practicalities of what societies should do about AI companions, if anything. The final question (Q.22) is an evaluative question on whether participants feel AI companions are generally a positive or negative addition to society.Funded by the Responsible AI UK Impact Accelerator, project AEGIS sees Bangor University's Emotional AI Lab partnering with Japan’s National Institute of Informatics, the Institute of Electrical and Electronics Engineers (IEEE), Monash University (Indonesia) and engaging the Information Commissioner's Office (UK). 
The goal of AEGIS is to host a series of workshops, assemble a diverse expert working group and develop a ‘technical standard’ to address use of emulated empathy in general-purpose artificial intelligence systems for human-AI partnerships. 
Provisionally titled Recommended Practice for Ethical Considerations of Emulated Empathy in Partner-based General-Purpose Artificial Intelligence Systems, this IEEE standard will define ethical considerations, detail good practices, and augment and complement international human rights and regional law. 
Use cases encompass general-purpose artificial intelligence products marketed as ‘empathic partners’, ‘personal AI’, ‘co-pilots’, ‘assistants’, and related phrasing for ‘human-AI partnering’. Current and nascent domains of use include work, therapy, education, life coaching, legal problems, fitness, entertainment, and more. 
These systems raise ethical questions that are global in nature, yet benefitting from diverse ethical approaches, especially where systems feed into the design of human-centered technologies. Some ethical questions are familiar (e.g. transparency, accountability, bias and fairness), but others are specific and unique, including psychological interactions and dependencies, child appropriateness, fiduciary issues, animism, and manipulation through partnerships with general-purpose artificial intelligence systems.
The project augments the Emotional AI Lab's UK-Japan social science work by conducting a UK national demographically representative survey, and considering results in light of studies on AI ethics. It also sees global value in drawing a range of ethical frames of reference by which to account for human-AI partnerships, not least Japan and ethically aligned regions, given their long-standing interests in human-technology partnerships.1d:["$","meta","3",{"property":"og:description","content":"$2c"}]
1e:["$","meta","4",{"property":"og:url","content":"https://harmonydata.ac.uk/search/items/uk-national-survey-on-attitudes-to-ai-companions-aggregate-data-2024"}]
1f:["$","meta","5",{"property":"og:site_name","content":"Academic Resource Discovery"}]
20:["$","meta","6",{"property":"og:locale","content":"en_US"}]
21:["$","meta","7",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}]
22:["$","meta","8",{"property":"og:image:width","content":"1200"}]
23:["$","meta","9",{"property":"og:image:height","content":"630"}]
24:["$","meta","10",{"property":"og:image:alt","content":"UK National Survey on Attitudes to AI Companions: Aggregate Data, 2024"}]
25:["$","meta","11",{"property":"og:type","content":"website"}]
26:["$","meta","12",{"name":"twitter:card","content":"summary_large_image"}]
27:["$","meta","13",{"name":"twitter:title","content":"UK National Survey on Attitudes to AI Companions: Aggregate Data, 2024"}]
2d:Te37,To ascertain how the British public feel about AI companions, we conducted a UK-wide demographically representative national survey (implemented by professional company Walnut Unlimited - a human understanding agency, part of the Unlimited Group), across 10-12 December 2024, 2073 respondents aged 18 years or over, online omnibus). This was a part of a Responsible AI award, to create soft governance of autonomous systems that interact with human emotions and/or emulate empathy.

The survey asks 22 closed-ended, multiple-choice questions on AI companions. The first set of questions (Q.1-2) glean participants’ familiarity with, and usage of, companion apps. The second set of questions (Q.3-4) explore the acceptability of design features of AI companions. The third set of questions (Q.5-7) explore the broad benefits and concerns from using AI companions. The fourth set of questions (Q.8-13) explore views on children and companion apps. The fifth set of questions (Q.14-15) explore views on older adults and companion apps. The sixth set of questions (Q.16-18) explore views on mental health issues and companion apps. The seventh set of questions (Q.19-21) explore views on desired governance of companion apps to consider the practicalities of what societies should do about AI companions, if anything. The final question (Q.22) is an evaluative question on whether participants feel AI companions are generally a positive or negative addition to society.Funded by the Responsible AI UK Impact Accelerator, project AEGIS sees Bangor University's Emotional AI Lab partnering with Japan’s National Institute of Informatics, the Institute of Electrical and Electronics Engineers (IEEE), Monash University (Indonesia) and engaging the Information Commissioner's Office (UK). 
The goal of AEGIS is to host a series of workshops, assemble a diverse expert working group and develop a ‘technical standard’ to address use of emulated empathy in general-purpose artificial intelligence systems for human-AI partnerships. 
Provisionally titled Recommended Practice for Ethical Considerations of Emulated Empathy in Partner-based General-Purpose Artificial Intelligence Systems, this IEEE standard will define ethical considerations, detail good practices, and augment and complement international human rights and regional law. 
Use cases encompass general-purpose artificial intelligence products marketed as ‘empathic partners’, ‘personal AI’, ‘co-pilots’, ‘assistants’, and related phrasing for ‘human-AI partnering’. Current and nascent domains of use include work, therapy, education, life coaching, legal problems, fitness, entertainment, and more. 
These systems raise ethical questions that are global in nature, yet benefitting from diverse ethical approaches, especially where systems feed into the design of human-centered technologies. Some ethical questions are familiar (e.g. transparency, accountability, bias and fairness), but others are specific and unique, including psychological interactions and dependencies, child appropriateness, fiduciary issues, animism, and manipulation through partnerships with general-purpose artificial intelligence systems.
The project augments the Emotional AI Lab's UK-Japan social science work by conducting a UK national demographically representative survey, and considering results in light of studies on AI ethics. It also sees global value in drawing a range of ethical frames of reference by which to account for human-AI partnerships, not least Japan and ethically aligned regions, given their long-standing interests in human-technology partnerships.28:["$","meta","14",{"name":"twitter:description","content":"$2d"}]
29:["$","meta","15",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}]
2a:["$","link","16",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]
2b:["$","$L2e","17",{}]
14:"$10:metadata"
