1:"$Sreact.fragment"
2:I[82104,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","7177","static/js/app/layout.e50c3fe1.js"],"default"]
3:I[17146,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","7177","static/js/app/layout.e50c3fe1.js"],"AuthProvider"]
4:I[63612,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","7177","static/js/app/layout.e50c3fe1.js"],"SearchProvider"]
5:I[68998,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","7177","static/js/app/layout.e50c3fe1.js"],"default"]
6:I[98904,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","7177","static/js/app/layout.e50c3fe1.js"],"default"]
7:I[15244,[],""]
8:I[43866,[],""]
9:I[14046,["6586","static/js/6586.2e946dbf.js","9197","static/js/9197.61b93e42.js","8378","static/js/8378.a1bea36e.js","2926","static/js/2926.76e4f620.js","8173","static/js/8173.582c8c90.js","1702","static/js/1702.de0c2d51.js","1983","static/js/1983.ec5be3f4.js","7184","static/js/7184.52d31c32.js","7177","static/js/app/layout.e50c3fe1.js"],"ToastContainer"]
b:I[86213,[],"OutletBoundary"]
d:I[86213,[],"MetadataBoundary"]
f:I[86213,[],"ViewportBoundary"]
11:I[34835,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/0d5b820fee8240e5.css","style"]
0:{"P":null,"b":"EbxAGC0uqTvxrhJPKAqev","p":"/search","c":["","items","understanding-speech-in-the-presence-of-other-speech-perceptual-mechanisms-for-auditory-scene-analysis-in-human-listeners"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","understanding-speech-in-the-presence-of-other-speech-perceptual-mechanisms-for-auditory-scene-analysis-in-human-listeners","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/0d5b820fee8240e5.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":[["$","meta",null,{"name":"emotion-insertion-point","content":""}],["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"preconnect","href":"https://www.cataloguementalhealth.ac.uk"}],["$","link",null,{"rel":"dns-prefetch","href":"https://harmonydata.ac.uk"}],["$","style",null,{"dangerouslySetInnerHTML":{"__html":"\n            /* Ensure immediate rendering with Roboto and fallbacks */\n            * { \n              font-family: \"Roboto\", -apple-system, BlinkMacSystemFont, \"Segoe UI\", \"Oxygen\", \"Ubuntu\", \"Cantarell\", \"Fira Sans\", \"Droid Sans\", \"Helvetica Neue\", sans-serif !important;\n              font-display: swap;\n              -webkit-font-smoothing: antialiased;\n              -moz-osx-font-smoothing: grayscale;\n            }\n            body { \n              visibility: visible !important; \n              opacity: 1 !important; \n              margin: 0; \n              padding: 0; \n            }\n          "}}]]}],["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":[["$","$L5",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L6",null,{}],["$","$L5",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$L9",null,{"position":"bottom-right"}]]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","understanding-speech-in-the-presence-of-other-speech-perceptual-mechanisms-for-auditory-scene-analysis-in-human-listeners","d"],["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$La",null,["$","$Lb",null,{"children":"$Lc"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","sL-l31ivl7UOv6zSp-050",{"children":[["$","$Ld",null,{"children":"$Le"}],["$","$Lf",null,{"children":"$L10"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
12:I[53704,["6586","static/js/6586.2e946dbf.js","8378","static/js/8378.a1bea36e.js","2282","static/js/2282.e20001b9.js","5135","static/js/5135.b8bfc30e.js","9387","static/js/9387.65629b75.js","2649","static/js/2649.4d01838c.js","1857","static/js/1857.a01744c0.js","280","static/js/280.5152a9e2.js","7626","static/js/7626.f9409ee1.js","6387","static/js/app/items/%5Bslug%5D/page.4934bfd6.js"],""]
14:I[77626,["6586","static/js/6586.2e946dbf.js","8378","static/js/8378.a1bea36e.js","2282","static/js/2282.e20001b9.js","5135","static/js/5135.b8bfc30e.js","9387","static/js/9387.65629b75.js","2649","static/js/2649.4d01838c.js","1857","static/js/1857.a01744c0.js","280","static/js/280.5152a9e2.js","7626","static/js/7626.f9409ee1.js","6387","static/js/app/items/%5Bslug%5D/page.4934bfd6.js"],"default"]
13:T7f9,{"@context":"https://schema.org/","@type":"Dataset","name":"Understanding speech in the presence of other speech: Perceptual mechanisms for auditory scene analysis in human listeners","description":"The datasets comprise transcriptions of stimuli (simplified analogues of spoken utterances) and associated keyword scores (proportion correct). The transcriptions are those entered by the participant using a keyboard. \n\nIt is unusual to hear the speech of a particular talker in isolation; speech is typically heard in the presence of interfering sounds, such as the voices of other talkers. This project will elucidate the mechanisms by which listeners segregate the formants constituting one speech sound from another.\n\nThere is extensive evidence that grouping “primitives” such as common onset time are important for the perceptual grouping of non-speech sounds, but few studies have investigated the role of such cues in the grouping of speech formants. Psychophysical experiments will be conducted using simplified speech stimuli that permit at least two possible perceptual organisations to compete with each other. These competitive configurations will be used to quantify the relative impact of extraneous formants on speech intelligibility as their acoustic properties are manipulated. This will enable a detailed investigation of the extent to which across-formant grouping is determined by general-purpose grouping cues and by speech-specific grouping cues.\n\nThe relationship between primitive and high-level grouping constraints will also be explored, by determining whether linguistic information presented just prior to a speech stimulus can increase the perceptual exclusion of a competitor formant.","url":"https://harmonydata.ac.uk/search/items/understanding-speech-in-the-presence-of-other-speech-perceptual-mechanisms-for-auditory-scene-analysis-in-human-listeners","identifier":["http://dx.doi.org/10.5255/UKDA-SN-852318"],"keywords":["HEARING","PERCEPTION","SPEECH"],"temporalCoverage":"2013-04-01/2016-03-31"}15:T5ef,The datasets comprise transcriptions of stimuli (simplified analogues of spoken utterances) and associated keyword scores (proportion correct). The transcriptions are those entered by the participant using a keyboard. 

It is unusual to hear the speech of a particular talker in isolation; speech is typically heard in the presence of interfering sounds, such as the voices of other talkers. This project will elucidate the mechanisms by which listeners segregate the formants constituting one speech sound from another.

There is extensive evidence that grouping “primitives” such as common onset time are important for the perceptual grouping of non-speech sounds, but few studies have investigated the role of such cues in the grouping of speech formants. Psychophysical experiments will be conducted using simplified speech stimuli that permit at least two possible perceptual organisations to compete with each other. These competitive configurations will be used to quantify the relative impact of extraneous formants on speech intelligibility as their acoustic properties are manipulated. This will enable a detailed investigation of the extent to which across-formant grouping is determined by general-purpose grouping cues and by speech-specific grouping cues.

The relationship between primitive and high-level grouping constraints will also be explored, by determining whether linguistic information presented just prior to a speech stimulus can increase the perceptual exclusion of a competitor formant.a:[["$","$L12",null,{"strategy":"beforeInteractive","id":"structured-data","type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$13"}}],["$","$L14",null,{"study":{"dataset_schema":{"@context":"https://schema.org/","@type":"Dataset","name":"Understanding speech in the presence of other speech: Perceptual mechanisms for auditory scene analysis in human listeners","description":"$15","url":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=852318","https://reshare.ukdataservice.ac.uk/852318"],"keywords":["HEARING","PERCEPTION","SPEECH"],"identifier":["http://dx.doi.org/10.5255/UKDA-SN-852318"],"includedInDataCatalog":[{"@type":"DataCatalog","name":"UK Data Service","url":"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=852318"}],"sponsor":[{"@type":"Organization","name":"ESRC"}],"temporalCoverage":"2013-04-01/2016-03-31"},"extra_data":{"study_design":[],"duration_years":3,"slug":"understanding-speech-in-the-presence-of-other-speech-perceptual-mechanisms-for-auditory-scene-analysis-in-human-listeners","name":"Understanding speech in the presence of other speech: Perceptual mechanisms for auditory scene analysis in human listeners","language_codes":["en"],"instruments":[],"geographic_coverage":"Aston University, Birmingham, UK","num_variables":null,"source":["ukds"],"sex":"all","ai_summary":null,"data_access":null,"resource_type":"dataset","dois":["http://dx.doi.org/10.5255/UKDA-SN-852318"],"end_year":2016,"genetic_data_collected":false,"harmony_id":"ukds/852318","urls":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=852318","https://reshare.ukdataservice.ac.uk/852318"],"start_year":2013,"country_codes":["GB"],"uuid":"c38320e5fbcaa7e00784496f888bafaa"},"distance":0,"score":0,"parent":{},"ancestors":[]}}]]
10:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
16:T5ef,The datasets comprise transcriptions of stimuli (simplified analogues of spoken utterances) and associated keyword scores (proportion correct). The transcriptions are those entered by the participant using a keyboard. 

It is unusual to hear the speech of a particular talker in isolation; speech is typically heard in the presence of interfering sounds, such as the voices of other talkers. This project will elucidate the mechanisms by which listeners segregate the formants constituting one speech sound from another.

There is extensive evidence that grouping “primitives” such as common onset time are important for the perceptual grouping of non-speech sounds, but few studies have investigated the role of such cues in the grouping of speech formants. Psychophysical experiments will be conducted using simplified speech stimuli that permit at least two possible perceptual organisations to compete with each other. These competitive configurations will be used to quantify the relative impact of extraneous formants on speech intelligibility as their acoustic properties are manipulated. This will enable a detailed investigation of the extent to which across-formant grouping is determined by general-purpose grouping cues and by speech-specific grouping cues.

The relationship between primitive and high-level grouping constraints will also be explored, by determining whether linguistic information presented just prior to a speech stimulus can increase the perceptual exclusion of a competitor formant.17:T5ef,The datasets comprise transcriptions of stimuli (simplified analogues of spoken utterances) and associated keyword scores (proportion correct). The transcriptions are those entered by the participant using a keyboard. 

It is unusual to hear the speech of a particular talker in isolation; speech is typically heard in the presence of interfering sounds, such as the voices of other talkers. This project will elucidate the mechanisms by which listeners segregate the formants constituting one speech sound from another.

There is extensive evidence that grouping “primitives” such as common onset time are important for the perceptual grouping of non-speech sounds, but few studies have investigated the role of such cues in the grouping of speech formants. Psychophysical experiments will be conducted using simplified speech stimuli that permit at least two possible perceptual organisations to compete with each other. These competitive configurations will be used to quantify the relative impact of extraneous formants on speech intelligibility as their acoustic properties are manipulated. This will enable a detailed investigation of the extent to which across-formant grouping is determined by general-purpose grouping cues and by speech-specific grouping cues.

The relationship between primitive and high-level grouping constraints will also be explored, by determining whether linguistic information presented just prior to a speech stimulus can increase the perceptual exclusion of a competitor formant.18:T5ef,The datasets comprise transcriptions of stimuli (simplified analogues of spoken utterances) and associated keyword scores (proportion correct). The transcriptions are those entered by the participant using a keyboard. 

It is unusual to hear the speech of a particular talker in isolation; speech is typically heard in the presence of interfering sounds, such as the voices of other talkers. This project will elucidate the mechanisms by which listeners segregate the formants constituting one speech sound from another.

There is extensive evidence that grouping “primitives” such as common onset time are important for the perceptual grouping of non-speech sounds, but few studies have investigated the role of such cues in the grouping of speech formants. Psychophysical experiments will be conducted using simplified speech stimuli that permit at least two possible perceptual organisations to compete with each other. These competitive configurations will be used to quantify the relative impact of extraneous formants on speech intelligibility as their acoustic properties are manipulated. This will enable a detailed investigation of the extent to which across-formant grouping is determined by general-purpose grouping cues and by speech-specific grouping cues.

The relationship between primitive and high-level grouping constraints will also be explored, by determining whether linguistic information presented just prior to a speech stimulus can increase the perceptual exclusion of a competitor formant.e:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Understanding speech in the presence of other speech: Perceptual mechanisms for auditory scene analysis in human listeners"}],["$","meta","2",{"name":"description","content":"$16"}],["$","meta","3",{"property":"og:title","content":"Understanding speech in the presence of other speech: Perceptual mechanisms for auditory scene analysis in human listeners"}],["$","meta","4",{"property":"og:description","content":"$17"}],["$","meta","5",{"property":"og:url","content":"https://harmonydata.ac.uk/search/items/understanding-speech-in-the-presence-of-other-speech-perceptual-mechanisms-for-auditory-scene-analysis-in-human-listeners"}],["$","meta","6",{"property":"og:site_name","content":"Academic Resource Discovery"}],["$","meta","7",{"property":"og:locale","content":"en_US"}],["$","meta","8",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","meta","9",{"property":"og:image:width","content":"1200"}],["$","meta","10",{"property":"og:image:height","content":"630"}],["$","meta","11",{"property":"og:image:alt","content":"Understanding speech in the presence of other speech: Perceptual mechanisms for auditory scene analysis in human listeners"}],["$","meta","12",{"property":"og:type","content":"website"}],["$","meta","13",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","14",{"name":"twitter:title","content":"Understanding speech in the presence of other speech: Perceptual mechanisms for auditory scene analysis in human listeners"}],["$","meta","15",{"name":"twitter:description","content":"$18"}],["$","meta","16",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","link","17",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
c:null
