1:"$Sreact.fragment"
2:I[82104,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
3:I[10683,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"AuthProvider"]
4:I[63612,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"SearchProvider"]
5:I[68998,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
6:I[98904,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"default"]
7:I[15244,[],""]
8:I[43866,[],""]
9:I[14046,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","8173","static/chunks/8173-30737ce2fc776efb.js","9756","static/chunks/9756-90c6220c809c4148.js","3163","static/chunks/3163-d1a03f172499fcd8.js","7177","static/chunks/app/layout-802ca43371b3eb9d.js"],"ToastContainer"]
b:I[86213,[],"OutletBoundary"]
d:I[86213,[],"MetadataBoundary"]
f:I[86213,[],"ViewportBoundary"]
11:I[34835,[],""]
:HL["/search/_next/static/media/47cbc4e2adbc5db9-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/search/_next/static/css/2c4d913f25bfc6bf.css","style"]
:HL["/search/_next/static/css/4921cfd18b262f8c.css","style"]
0:{"P":null,"b":"crTstUN6M3QDVzxd_YHmv","p":"/search","c":["","items","vocal-learning-in-adulthood-investigating-the-mechanisms-of-vocal-imitation-using-mri-of-the-vocal-tract-and-brain-2015-2018"],"i":false,"f":[[["",{"children":["items",{"children":[["slug","vocal-learning-in-adulthood-investigating-the-mechanisms-of-vocal-imitation-using-mri-of-the-vocal-tract-and-brain-2015-2018","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/2c4d913f25bfc6bf.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":["$","meta",null,{"name":"emotion-insertion-point","content":""}]}],["$","body",null,{"className":"__className_62a302","children":["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"children":[["$","$L5",null,{"sx":{"display":"flex","flexDirection":{"xs":"column","md":"row"}},"children":[["$","$L6",null,{}],["$","$L5",null,{"component":"main","sx":{"flexGrow":1,"ml":{"xs":0,"md":"72px"},"mt":{"xs":"64px","md":0},"minHeight":{"xs":"calc(100vh - 64px)","md":"100vh"},"width":{"xs":"100%","md":"calc(100% - 72px)"}},"children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}],["$","$L9",null,{"position":"bottom-right"}]]}]}]}]}]]}]]}],{"children":["items",["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","vocal-learning-in-adulthood-investigating-the-mechanisms-of-vocal-imitation-using-mri-of-the-vocal-tract-and-brain-2015-2018","d"],["$","$1","c",{"children":[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","items","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$La",[["$","link","0",{"rel":"stylesheet","href":"/search/_next/static/css/4921cfd18b262f8c.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$Lb",null,{"children":"$Lc"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","R2gVHFOTWwjts2Dw1mnmr",{"children":[["$","$Ld",null,{"children":"$Le"}],["$","$Lf",null,{"children":"$L10"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
12:I[53704,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","2154","static/chunks/834cb1aa-fe75579b2a50baac.js","3524","static/chunks/2170a4aa-66be1631595ccab0.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","1057","static/chunks/1057-d97430463abd6821.js","2282","static/chunks/2282-26bc5318a4471ee9.js","9234","static/chunks/9234-fce85e807baa599f.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","5733","static/chunks/5733-d0ad15157d7394e5.js","8173","static/chunks/8173-30737ce2fc776efb.js","613","static/chunks/613-3467f3d6fe7e6e6a.js","9756","static/chunks/9756-90c6220c809c4148.js","8738","static/chunks/8738-58586275b0d791e8.js","2649","static/chunks/2649-8ae63f8e6332939b.js","1857","static/chunks/1857-99747bd4076c313b.js","2288","static/chunks/2288-ffb609d77f258e27.js","6387","static/chunks/app/items/%5Bslug%5D/page-566168fa6ba89e63.js"],""]
14:I[5749,["2992","static/chunks/bc9e92e6-ca3f8a01cbc7cc31.js","9895","static/chunks/f71d1b72-799ff7a6833dc50c.js","2154","static/chunks/834cb1aa-fe75579b2a50baac.js","3524","static/chunks/2170a4aa-66be1631595ccab0.js","6586","static/chunks/6586-1013c110456598c2.js","4889","static/chunks/4889-f0599128dd4090a0.js","1057","static/chunks/1057-d97430463abd6821.js","2282","static/chunks/2282-26bc5318a4471ee9.js","9234","static/chunks/9234-fce85e807baa599f.js","9141","static/chunks/9141-d17bf49085d8e296.js","2926","static/chunks/2926-f97573e569b0b5d8.js","5733","static/chunks/5733-d0ad15157d7394e5.js","8173","static/chunks/8173-30737ce2fc776efb.js","613","static/chunks/613-3467f3d6fe7e6e6a.js","9756","static/chunks/9756-90c6220c809c4148.js","8738","static/chunks/8738-58586275b0d791e8.js","2649","static/chunks/2649-8ae63f8e6332939b.js","1857","static/chunks/1857-99747bd4076c313b.js","2288","static/chunks/2288-ffb609d77f258e27.js","6387","static/chunks/app/items/%5Bslug%5D/page-566168fa6ba89e63.js"],"default"]
13:T1492,{"@context":"https://schema.org/","@type":"Dataset","name":"Vocal Learning in Adulthood: Investigating the mechanisms of vocal imitation using MRI of the vocal tract and brain 2015-2018","description":"This collection contains behavioural and brain activation data from 3 laboratory studies of speech imitation. Each of the three studies involved behavioural and imaging (MRI) test sessions in which participants were familiarised with novel auditory speech targets, and were asked to imitate them as closely as possible. Across the three studies, there were variations in the type of sounds imitated, and in the participant populations tested.\n The behavioural data in this collection contain measures of speech imitation performance, for different sound categories and at during different stages of the test session. These are expressed using selected acoustic properties of speech recordings, and based on the anatomical configurations of the vocal tract as measured with MRI. Neuroimaging data show the locations of brain activity and speech-related representations as statistical maps in normalised 3D brain space, related to the stages of speech imitation and the type of sound imitated. \nIn addition to the three experimental studies, a database of videos depicts the dynamics of the vocal tract and the sounds of speech during the production of English sentences and syllables from 55 speakers.\nWe are genetically programmed to acquire spoken language from our environment, and infants can master native pronunciation in multiple languages without explicit tuition. However, in adolescence and adulthood we have a limited capacity to achieve accurate pronunciation of unfamiliar languages, and even highly competent users of a language learned in adulthood might speak with a strong, non-native accent. The UK currently lies behind other EU nations in foreign language skills at school and in the workplace, therefore research into skill development has important educational and economic implications.\n\nPrevious research has used functional MRI to measure how the activity of functional systems in the brain changes as new speech sounds are learned. This work has described the integration of novel speech sounds into a talker's existing speech repertoire as it becomes more familiar. Within this, however, some talkers are more successful than others at attaining native-like pronunciation of new sounds, and this variability is correlated with the activation, size and structural composition of specific brain structures.\n\nTo date, the cognitive neuroscience of speech learning has assessed performance by measuring or judging the sounds of speech. However, there isn't a simple one-to-one relationship between how speech sounds in the air, and the underlying movements in the vocal tract. Therefore, an important missing piece of the puzzle is an understanding of how vocal articulations relate directly to brain activation during learning. Recent developments in MRI have shown that rapid 'real-time' anatomical scans can be used to create videos of the interior of the mouth and vocal tract, such that we can view how the lips, tongue and voice box are moved and configured to perform speech. We propose to combine real-time MRI with measures of brain structure and function to investigate the relationship between brain and behaviour during the learning of new speech sounds.\n\nOur project will focus on short-term learning of novel and unfamiliar vocal sounds by native speakers of English, where the participants will aim to imitate the sounds accurately and with native-like pronunciation. In an MRI scanner, listeners will repeatedly produce these novel sounds, as well as native sounds of English, while scans of the brain will measure neural activity. Interleaved with these scans, we will collect real-time images of the vocal articulators during imitation. Acoustic recordings will be made using an in-scanner microphone, and we will additionally collect high-resolution images of brain structure from each participant. With these data, we will investigate vocal learning in terms of i) the functional brain systems supporting learning, ii) the acoustic accuracy of vocal output and iii) the accuracy of the movements generating the sounds, as well as the relationship between these elements. Further, we can explore how individuals differ in their performance of vocal learning in terms of meeting these acoustic and motor targets, and how this relates to their brain structure and function. Across a series of experiments, we will also investigate how novel sounds are sequenced into new words, and assess the effects of expertise on vocal learning by comparing English speakers with vocal performance experts.\n\nOur proposed approach is truly novel, with potential to make groundbreaking developments in the cognitive neuroscience of vocal communication.","url":"https://harmonydata.ac.uk/search/items/vocal-learning-in-adulthood-investigating-the-mechanisms-of-vocal-imitation-using-mri-of-the-vocal-tract-and-brain-2015-2018","identifier":["http://dx.doi.org/10.5255/UKDA-SN-853317"],"keywords":["SPEECH","HUMAN BEHAVIOUR","HUMAN BIOLOGY","LANGUAGE SKILLS","SINGERS","ACOUSTICS","MRI"],"temporalCoverage":"2015-01-12/2018-06-28"}15:T1244,This collection contains behavioural and brain activation data from 3 laboratory studies of speech imitation. Each of the three studies involved behavioural and imaging (MRI) test sessions in which participants were familiarised with novel auditory speech targets, and were asked to imitate them as closely as possible. Across the three studies, there were variations in the type of sounds imitated, and in the participant populations tested.
 The behavioural data in this collection contain measures of speech imitation performance, for different sound categories and at during different stages of the test session. These are expressed using selected acoustic properties of speech recordings, and based on the anatomical configurations of the vocal tract as measured with MRI. Neuroimaging data show the locations of brain activity and speech-related representations as statistical maps in normalised 3D brain space, related to the stages of speech imitation and the type of sound imitated. 
In addition to the three experimental studies, a database of videos depicts the dynamics of the vocal tract and the sounds of speech during the production of English sentences and syllables from 55 speakers.
We are genetically programmed to acquire spoken language from our environment, and infants can master native pronunciation in multiple languages without explicit tuition. However, in adolescence and adulthood we have a limited capacity to achieve accurate pronunciation of unfamiliar languages, and even highly competent users of a language learned in adulthood might speak with a strong, non-native accent. The UK currently lies behind other EU nations in foreign language skills at school and in the workplace, therefore research into skill development has important educational and economic implications.

Previous research has used functional MRI to measure how the activity of functional systems in the brain changes as new speech sounds are learned. This work has described the integration of novel speech sounds into a talker's existing speech repertoire as it becomes more familiar. Within this, however, some talkers are more successful than others at attaining native-like pronunciation of new sounds, and this variability is correlated with the activation, size and structural composition of specific brain structures.

To date, the cognitive neuroscience of speech learning has assessed performance by measuring or judging the sounds of speech. However, there isn't a simple one-to-one relationship between how speech sounds in the air, and the underlying movements in the vocal tract. Therefore, an important missing piece of the puzzle is an understanding of how vocal articulations relate directly to brain activation during learning. Recent developments in MRI have shown that rapid 'real-time' anatomical scans can be used to create videos of the interior of the mouth and vocal tract, such that we can view how the lips, tongue and voice box are moved and configured to perform speech. We propose to combine real-time MRI with measures of brain structure and function to investigate the relationship between brain and behaviour during the learning of new speech sounds.

Our project will focus on short-term learning of novel and unfamiliar vocal sounds by native speakers of English, where the participants will aim to imitate the sounds accurately and with native-like pronunciation. In an MRI scanner, listeners will repeatedly produce these novel sounds, as well as native sounds of English, while scans of the brain will measure neural activity. Interleaved with these scans, we will collect real-time images of the vocal articulators during imitation. Acoustic recordings will be made using an in-scanner microphone, and we will additionally collect high-resolution images of brain structure from each participant. With these data, we will investigate vocal learning in terms of i) the functional brain systems supporting learning, ii) the acoustic accuracy of vocal output and iii) the accuracy of the movements generating the sounds, as well as the relationship between these elements. Further, we can explore how individuals differ in their performance of vocal learning in terms of meeting these acoustic and motor targets, and how this relates to their brain structure and function. Across a series of experiments, we will also investigate how novel sounds are sequenced into new words, and assess the effects of expertise on vocal learning by comparing English speakers with vocal performance experts.

Our proposed approach is truly novel, with potential to make groundbreaking developments in the cognitive neuroscience of vocal communication.a:[["$","$L12",null,{"strategy":"beforeInteractive","id":"structured-data","type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$13"}}],["$","$L14",null,{"dataset":{"title":"Vocal Learning in Adulthood: Investigating the mechanisms of vocal imitation using MRI of the vocal tract and brain 2015-2018","description":"$15","image":"$undefined","publisher":"$undefined","funders":"$undefined","geographicCoverage":"GB","temporalCoverage":"2015-01-12/2018-06-28","ageCoverage":"$undefined","studyDesign":[],"resourceType":"dataset","topics":["SPEECH","HUMAN BEHAVIOUR","HUMAN BIOLOGY","LANGUAGE SKILLS","SINGERS","ACOUSTICS","MRI"],"instruments":[],"dataCatalogs":[{"name":"UK Data Service","url":"https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=853317","logo":"$undefined"}],"matchedVariables":[],"allVariables":[],"additionalLinks":["https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=853317","https://reshare.ukdataservice.ac.uk/853317","http://dx.doi.org/10.5255/UKDA-SN-853317","http://dx.doi.org/10.5255/UKDA-SN-853317"],"child_datasets":[],"aiSummary":null}}]]
10:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
16:T1244,This collection contains behavioural and brain activation data from 3 laboratory studies of speech imitation. Each of the three studies involved behavioural and imaging (MRI) test sessions in which participants were familiarised with novel auditory speech targets, and were asked to imitate them as closely as possible. Across the three studies, there were variations in the type of sounds imitated, and in the participant populations tested.
 The behavioural data in this collection contain measures of speech imitation performance, for different sound categories and at during different stages of the test session. These are expressed using selected acoustic properties of speech recordings, and based on the anatomical configurations of the vocal tract as measured with MRI. Neuroimaging data show the locations of brain activity and speech-related representations as statistical maps in normalised 3D brain space, related to the stages of speech imitation and the type of sound imitated. 
In addition to the three experimental studies, a database of videos depicts the dynamics of the vocal tract and the sounds of speech during the production of English sentences and syllables from 55 speakers.
We are genetically programmed to acquire spoken language from our environment, and infants can master native pronunciation in multiple languages without explicit tuition. However, in adolescence and adulthood we have a limited capacity to achieve accurate pronunciation of unfamiliar languages, and even highly competent users of a language learned in adulthood might speak with a strong, non-native accent. The UK currently lies behind other EU nations in foreign language skills at school and in the workplace, therefore research into skill development has important educational and economic implications.

Previous research has used functional MRI to measure how the activity of functional systems in the brain changes as new speech sounds are learned. This work has described the integration of novel speech sounds into a talker's existing speech repertoire as it becomes more familiar. Within this, however, some talkers are more successful than others at attaining native-like pronunciation of new sounds, and this variability is correlated with the activation, size and structural composition of specific brain structures.

To date, the cognitive neuroscience of speech learning has assessed performance by measuring or judging the sounds of speech. However, there isn't a simple one-to-one relationship between how speech sounds in the air, and the underlying movements in the vocal tract. Therefore, an important missing piece of the puzzle is an understanding of how vocal articulations relate directly to brain activation during learning. Recent developments in MRI have shown that rapid 'real-time' anatomical scans can be used to create videos of the interior of the mouth and vocal tract, such that we can view how the lips, tongue and voice box are moved and configured to perform speech. We propose to combine real-time MRI with measures of brain structure and function to investigate the relationship between brain and behaviour during the learning of new speech sounds.

Our project will focus on short-term learning of novel and unfamiliar vocal sounds by native speakers of English, where the participants will aim to imitate the sounds accurately and with native-like pronunciation. In an MRI scanner, listeners will repeatedly produce these novel sounds, as well as native sounds of English, while scans of the brain will measure neural activity. Interleaved with these scans, we will collect real-time images of the vocal articulators during imitation. Acoustic recordings will be made using an in-scanner microphone, and we will additionally collect high-resolution images of brain structure from each participant. With these data, we will investigate vocal learning in terms of i) the functional brain systems supporting learning, ii) the acoustic accuracy of vocal output and iii) the accuracy of the movements generating the sounds, as well as the relationship between these elements. Further, we can explore how individuals differ in their performance of vocal learning in terms of meeting these acoustic and motor targets, and how this relates to their brain structure and function. Across a series of experiments, we will also investigate how novel sounds are sequenced into new words, and assess the effects of expertise on vocal learning by comparing English speakers with vocal performance experts.

Our proposed approach is truly novel, with potential to make groundbreaking developments in the cognitive neuroscience of vocal communication.17:T1244,This collection contains behavioural and brain activation data from 3 laboratory studies of speech imitation. Each of the three studies involved behavioural and imaging (MRI) test sessions in which participants were familiarised with novel auditory speech targets, and were asked to imitate them as closely as possible. Across the three studies, there were variations in the type of sounds imitated, and in the participant populations tested.
 The behavioural data in this collection contain measures of speech imitation performance, for different sound categories and at during different stages of the test session. These are expressed using selected acoustic properties of speech recordings, and based on the anatomical configurations of the vocal tract as measured with MRI. Neuroimaging data show the locations of brain activity and speech-related representations as statistical maps in normalised 3D brain space, related to the stages of speech imitation and the type of sound imitated. 
In addition to the three experimental studies, a database of videos depicts the dynamics of the vocal tract and the sounds of speech during the production of English sentences and syllables from 55 speakers.
We are genetically programmed to acquire spoken language from our environment, and infants can master native pronunciation in multiple languages without explicit tuition. However, in adolescence and adulthood we have a limited capacity to achieve accurate pronunciation of unfamiliar languages, and even highly competent users of a language learned in adulthood might speak with a strong, non-native accent. The UK currently lies behind other EU nations in foreign language skills at school and in the workplace, therefore research into skill development has important educational and economic implications.

Previous research has used functional MRI to measure how the activity of functional systems in the brain changes as new speech sounds are learned. This work has described the integration of novel speech sounds into a talker's existing speech repertoire as it becomes more familiar. Within this, however, some talkers are more successful than others at attaining native-like pronunciation of new sounds, and this variability is correlated with the activation, size and structural composition of specific brain structures.

To date, the cognitive neuroscience of speech learning has assessed performance by measuring or judging the sounds of speech. However, there isn't a simple one-to-one relationship between how speech sounds in the air, and the underlying movements in the vocal tract. Therefore, an important missing piece of the puzzle is an understanding of how vocal articulations relate directly to brain activation during learning. Recent developments in MRI have shown that rapid 'real-time' anatomical scans can be used to create videos of the interior of the mouth and vocal tract, such that we can view how the lips, tongue and voice box are moved and configured to perform speech. We propose to combine real-time MRI with measures of brain structure and function to investigate the relationship between brain and behaviour during the learning of new speech sounds.

Our project will focus on short-term learning of novel and unfamiliar vocal sounds by native speakers of English, where the participants will aim to imitate the sounds accurately and with native-like pronunciation. In an MRI scanner, listeners will repeatedly produce these novel sounds, as well as native sounds of English, while scans of the brain will measure neural activity. Interleaved with these scans, we will collect real-time images of the vocal articulators during imitation. Acoustic recordings will be made using an in-scanner microphone, and we will additionally collect high-resolution images of brain structure from each participant. With these data, we will investigate vocal learning in terms of i) the functional brain systems supporting learning, ii) the acoustic accuracy of vocal output and iii) the accuracy of the movements generating the sounds, as well as the relationship between these elements. Further, we can explore how individuals differ in their performance of vocal learning in terms of meeting these acoustic and motor targets, and how this relates to their brain structure and function. Across a series of experiments, we will also investigate how novel sounds are sequenced into new words, and assess the effects of expertise on vocal learning by comparing English speakers with vocal performance experts.

Our proposed approach is truly novel, with potential to make groundbreaking developments in the cognitive neuroscience of vocal communication.18:T1244,This collection contains behavioural and brain activation data from 3 laboratory studies of speech imitation. Each of the three studies involved behavioural and imaging (MRI) test sessions in which participants were familiarised with novel auditory speech targets, and were asked to imitate them as closely as possible. Across the three studies, there were variations in the type of sounds imitated, and in the participant populations tested.
 The behavioural data in this collection contain measures of speech imitation performance, for different sound categories and at during different stages of the test session. These are expressed using selected acoustic properties of speech recordings, and based on the anatomical configurations of the vocal tract as measured with MRI. Neuroimaging data show the locations of brain activity and speech-related representations as statistical maps in normalised 3D brain space, related to the stages of speech imitation and the type of sound imitated. 
In addition to the three experimental studies, a database of videos depicts the dynamics of the vocal tract and the sounds of speech during the production of English sentences and syllables from 55 speakers.
We are genetically programmed to acquire spoken language from our environment, and infants can master native pronunciation in multiple languages without explicit tuition. However, in adolescence and adulthood we have a limited capacity to achieve accurate pronunciation of unfamiliar languages, and even highly competent users of a language learned in adulthood might speak with a strong, non-native accent. The UK currently lies behind other EU nations in foreign language skills at school and in the workplace, therefore research into skill development has important educational and economic implications.

Previous research has used functional MRI to measure how the activity of functional systems in the brain changes as new speech sounds are learned. This work has described the integration of novel speech sounds into a talker's existing speech repertoire as it becomes more familiar. Within this, however, some talkers are more successful than others at attaining native-like pronunciation of new sounds, and this variability is correlated with the activation, size and structural composition of specific brain structures.

To date, the cognitive neuroscience of speech learning has assessed performance by measuring or judging the sounds of speech. However, there isn't a simple one-to-one relationship between how speech sounds in the air, and the underlying movements in the vocal tract. Therefore, an important missing piece of the puzzle is an understanding of how vocal articulations relate directly to brain activation during learning. Recent developments in MRI have shown that rapid 'real-time' anatomical scans can be used to create videos of the interior of the mouth and vocal tract, such that we can view how the lips, tongue and voice box are moved and configured to perform speech. We propose to combine real-time MRI with measures of brain structure and function to investigate the relationship between brain and behaviour during the learning of new speech sounds.

Our project will focus on short-term learning of novel and unfamiliar vocal sounds by native speakers of English, where the participants will aim to imitate the sounds accurately and with native-like pronunciation. In an MRI scanner, listeners will repeatedly produce these novel sounds, as well as native sounds of English, while scans of the brain will measure neural activity. Interleaved with these scans, we will collect real-time images of the vocal articulators during imitation. Acoustic recordings will be made using an in-scanner microphone, and we will additionally collect high-resolution images of brain structure from each participant. With these data, we will investigate vocal learning in terms of i) the functional brain systems supporting learning, ii) the acoustic accuracy of vocal output and iii) the accuracy of the movements generating the sounds, as well as the relationship between these elements. Further, we can explore how individuals differ in their performance of vocal learning in terms of meeting these acoustic and motor targets, and how this relates to their brain structure and function. Across a series of experiments, we will also investigate how novel sounds are sequenced into new words, and assess the effects of expertise on vocal learning by comparing English speakers with vocal performance experts.

Our proposed approach is truly novel, with potential to make groundbreaking developments in the cognitive neuroscience of vocal communication.e:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Vocal Learning in Adulthood: Investigating the mechanisms of vocal imitation using MRI of the vocal tract and brain 2015-2018"}],["$","meta","2",{"name":"description","content":"$16"}],["$","meta","3",{"property":"og:title","content":"Vocal Learning in Adulthood: Investigating the mechanisms of vocal imitation using MRI of the vocal tract and brain 2015-2018"}],["$","meta","4",{"property":"og:description","content":"$17"}],["$","meta","5",{"property":"og:url","content":"https://harmonydata.ac.uk/search/items/vocal-learning-in-adulthood-investigating-the-mechanisms-of-vocal-imitation-using-mri-of-the-vocal-tract-and-brain-2015-2018"}],["$","meta","6",{"property":"og:site_name","content":"Academic Resource Discovery"}],["$","meta","7",{"property":"og:locale","content":"en_US"}],["$","meta","8",{"property":"og:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","meta","9",{"property":"og:image:width","content":"1200"}],["$","meta","10",{"property":"og:image:height","content":"630"}],["$","meta","11",{"property":"og:image:alt","content":"Vocal Learning in Adulthood: Investigating the mechanisms of vocal imitation using MRI of the vocal tract and brain 2015-2018"}],["$","meta","12",{"property":"og:type","content":"website"}],["$","meta","13",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","14",{"name":"twitter:title","content":"Vocal Learning in Adulthood: Investigating the mechanisms of vocal imitation using MRI of the vocal tract and brain 2015-2018"}],["$","meta","15",{"name":"twitter:description","content":"$18"}],["$","meta","16",{"name":"twitter:image","content":"https://harmonydata.ac.uk/search/harmony.png"}],["$","link","17",{"rel":"icon","href":"/search/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
c:null
